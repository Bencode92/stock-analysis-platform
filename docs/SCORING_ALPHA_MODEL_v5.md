# ğŸ“Š Scoring & Alpha Model v5 â€” Architecture et SpÃ©cifications

> **Date**: 19 dÃ©cembre 2025  
> **Auteur**: Audit Expert (ChatGPT o1)  
> **Statut**: ApprouvÃ© pour implÃ©mentation P1  
> **PrÃ©requis**: Patches P0 complÃ©tÃ©s

---

## ğŸ¯ Principe Directeur

**Le scoring v5 produit un `scores[ticker]` propre (alpha), et l'allocation reste du ressort de l'optimiseur (SLSQP + contraintes + pÃ©nalitÃ©s).**

On **garde** l'objectif `score âˆ’ pÃ©nalitÃ© vol`, et on amÃ©liore :
1. **La qualitÃ© du score** (multi-factor, robuste, no-leakage)
2. **Les pÃ©nalitÃ©s/contraintes** manquantes (concentration, turnover, alternative_cap)
3. **Les garde-fous** (data coverage, filtres qualitÃ©)

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Ã‰tapes d'ImplÃ©mentation](#Ã©tapes-dimplÃ©mentation)
2. [Structure de Fichiers](#structure-de-fichiers)
3. [SpÃ©cifications par Ã‰tape](#spÃ©cifications-par-Ã©tape)
4. [Code Squelette](#code-squelette)
5. [IntÃ©gration Pipeline](#intÃ©gration-pipeline)
6. [DÃ©cisions Ã  Trancher](#dÃ©cisions-Ã -trancher)
7. [Tests Obligatoires](#tests-obligatoires)

---

## ğŸš€ Ã‰tapes d'ImplÃ©mentation

| # | Ã‰tape | Livrable | Effort | DÃ©pendances |
|---|-------|----------|--------|-------------|
| 1 | Data Contract | `instrument_master.json` + schÃ©ma features | 4h | - |
| 2 | Filtres QualitÃ© | `quality_filters.py` | 3h | Ã‰tape 1 |
| 3 | Feature Engineering | `features_prices.py` + `features_fundamentals.py` | 4h | Ã‰tape 1 |
| 4 | Normalisation Robuste | `transforms.py` | 2h | - |
| 5 | Score Composite v5 | `scoring_v5.py` | 4h | Ã‰tapes 3, 4 |
| 6 | Coverage Penalty | `coverage.py` | 1h | - |
| 7 | IntÃ©gration Optimiseur | Modification `optimizer.py` | 2h | P0 complÃ©tÃ© |
| 8 | Tests | `tests/test_scoring.py` | 4h | Toutes |

**Total estimÃ©**: ~24h de dÃ©veloppement

---

## ğŸ“ Structure de Fichiers

```
portfolio_engine/
â”œâ”€â”€ scoring/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration pondÃ©rations par profil
â”‚   â”œâ”€â”€ quality_filters.py     # Filtres prÃ©-scoring avec traÃ§abilitÃ©
â”‚   â”œâ”€â”€ features_prices.py     # Features calculÃ©es sur prix (momentum, vol, DD)
â”‚   â”œâ”€â”€ features_fundamentals.py  # Features fondamentales (ROIC, FCF, PEG)
â”‚   â”œâ”€â”€ transforms.py          # Winsorize + z-score + to_0_100
â”‚   â”œâ”€â”€ coverage.py            # Calcul et pÃ©nalitÃ© coverage
â”‚   â”œâ”€â”€ scoring_v5.py          # Score composite final
â”‚   â”œâ”€â”€ schema.py              # SchÃ©ma de donnÃ©es attendu
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_transforms.py
â”‚       â”œâ”€â”€ test_filters.py
â”‚       â””â”€â”€ test_scoring.py
â”œâ”€â”€ optimizer.py               # Utilise scores_z de scoring_v5
â””â”€â”€ constraint_oracle.py       # Recalcul indÃ©pendant
```

---

## ğŸ“ SpÃ©cifications par Ã‰tape

### Ã‰tape 1 â€” Data Contract (BLOQUANT)

**But**: ArrÃªter les "donnÃ©es manquantes = 50 neutre"

**Livrable**: `instrument_master.json`

```json
{
  "schema_version": "1.0",
  "assets": {
    "AAPL": {
      "ticker": "AAPL",
      "isin": "US0378331005",
      "asset_class": "stock",
      "currency": "USD",
      "country": "US",
      "region": "North America",
      "sector": "Technology",
      "industry": "Consumer Electronics",
      "risk_bucket": "equity_like",
      "market_cap_usd": 3000000000000,
      "avg_daily_volume": 50000000,
      "data_quality": {
        "price_available": true,
        "fundamentals_available": true,
        "last_fundamental_date": "2024-10-31",
        "coverage_ratio": 0.95
      }
    }
  }
}
```

**Checklist**:
- [ ] `ticker`, `asset_class` (stock/etf/bond_etf/crypto)
- [ ] `currency`, `country/region`, `sector`
- [ ] `risk_bucket` (equity_like/bond_like/alternative/crypto)
- [ ] Timestamps "as-of" (date prix, date fondamentaux)
- [ ] `coverage_ratio` (0â€“1) calculÃ© et loggÃ©

**âš ï¸ HypothÃ¨se fragile #1**: Tu as rÃ©ellement les dates de publication des fondamentaux. Sinon, risque de leakage.

---

### Ã‰tape 2 â€” Filtres QualitÃ© (prÃ©-scoring)

**But**: Filtrer avec traÃ§abilitÃ© (pourquoi exclu)

**Livrable**: `quality_filters.py`

```python
@dataclass
class FilterResult:
    eligible: List[str]           # Tickers Ã©ligibles
    rejected: Dict[str, str]      # {ticker: reason}
    warnings: Dict[str, str]      # {ticker: warning}

def apply_quality_filters(
    universe: pd.DataFrame,
    config: Dict[str, Any]
) -> FilterResult:
    """
    Filtres par asset class:
    - stocks: market_cap_min, volume_min, data coverage
    - ETFs: AUM_min, leverage==1, TER disponible
    - bond_ETFs: duration/credit disponibles
    - crypto: tier1, historique min, vol < seuil
    """
```

**RÃ¨gles par Asset Class**:

| Asset Class | Filtre | Valeur DÃ©faut | Configurable |
|-------------|--------|---------------|--------------|
| **Stocks** | market_cap_min | $100M | âœ… |
| **Stocks** | avg_volume_min | $1M/jour | âœ… |
| **Stocks** | vol_data_required | true | âœ… |
| **ETFs** | AUM_min | $50M | âœ… |
| **ETFs** | leverage_max | 1x | âœ… |
| **ETFs** | TER_required | true | âœ… |
| **Bond ETFs** | duration_required | true | âœ… |
| **Crypto** | tier | 1 (BTC/ETH/SOL) | âœ… |
| **Crypto** | vol_max | 150% | âœ… |

**Point critique**: Filtres **config-driven**, pas hardcodÃ©s.

---

### Ã‰tape 3 â€” Feature Engineering "as-of"

**But**: Calculer momentum/vol/DD Ã  partir des sÃ©ries de prix jusqu'Ã  date t, pas "aujourd'hui"

**Livrables**: `features_prices.py` + `features_fundamentals.py`

#### features_prices.py

```python
def compute_price_features(
    prices: pd.DataFrame,      # index=date, columns=tickers
    as_of_date: pd.Timestamp,
    lookbacks: Dict[str, int]  # {"1m": 21, "3m": 63, "12m": 252}
) -> pd.DataFrame:
    """
    Returns DataFrame avec colonnes:
    - mom_1m, mom_3m, mom_12m (returns cumulÃ©s)
    - vol_1y, vol_3y (annualisÃ©s)
    - max_dd_1y, max_dd_3y
    - sharpe_1y (si risk-free disponible)
    """
```

**RÃ¨gles anti-leakage**:
- Utiliser uniquement `prices[:as_of_date]`
- Pas de forward-fill au-delÃ  de 3 jours
- Logger les dates effectives utilisÃ©es

#### features_fundamentals.py

```python
def compute_fundamental_features(
    fundamentals: pd.DataFrame,  # Avec colonne 'report_date'
    as_of_date: pd.Timestamp
) -> pd.DataFrame:
    """
    Returns DataFrame avec colonnes:
    - roic, fcf_yield, peg, pe_ratio
    - revenue_growth_3y, earnings_growth_3y
    - debt_to_equity, interest_coverage
    
    RÃˆGLE: Utiliser le dernier report CONNU Ã  as_of_date
    """
```

**âš ï¸ HypothÃ¨se fragile #2**: Tes champs `perf_1y`, `YTD`, `max_drawdown_3y` sont dÃ©jÃ  as-of. Si non â†’ paper alpha.

---

### Ã‰tape 4 â€” Normalisation Robuste

**But**: Ã‰viter les mappings "PEG=3 â†’ 0" qui cassent selon secteur/rÃ©gime

**Livrable**: `transforms.py`

```python
def winsorize(s: pd.Series, lo: float = 0.01, hi: float = 0.99) -> pd.Series:
    """Clip aux percentiles lo/hi pour robustesse aux outliers"""
    if s.dropna().empty:
        return s
    ql, qh = s.quantile(lo), s.quantile(hi)
    return s.clip(ql, qh)

def zscore(s: pd.Series) -> pd.Series:
    """Z-score cross-section (mean=0, std=1)"""
    s = s.astype(float)
    mu, sig = s.mean(), s.std(ddof=0)
    return (s - mu) / (sig + 1e-12)

def to_0_100(z: pd.Series) -> pd.Series:
    """SigmoÃ¯de douce vers Ã©chelle 0-100 (audit-friendly)"""
    return (100.0 / (1.0 + np.exp(-z))).clip(0, 100)

def normalize_feature(
    s: pd.Series,
    method: str = "zscore",  # "zscore", "percentile", "minmax"
    winsorize_pct: Tuple[float, float] = (0.01, 0.99)
) -> pd.Series:
    """Pipeline complet: winsorize â†’ normalize"""
    s_win = winsorize(s, *winsorize_pct)
    
    if method == "zscore":
        return zscore(s_win)
    elif method == "percentile":
        return s_win.rank(pct=True)
    elif method == "minmax":
        return (s_win - s_win.min()) / (s_win.max() - s_win.min() + 1e-12)
    else:
        raise ValueError(f"Unknown method: {method}")
```

---

### Ã‰tape 5 â€” Score Composite v5

**But**: PondÃ©rer par profil sans mÃ©langer mÃ©triques non comparables

**Livrable**: `scoring_v5.py`

#### Configuration par Profil

```yaml
# config/scoring_config.yaml
profiles:
  Agressif:
    stocks:
      quality: 0.25
      value: 0.15
      momentum: 0.45
      risk: 0.15
    etfs:
      cost: 0.30
      liquidity: 0.25
      momentum: 0.30
      tracking: 0.15
    bonds:
      yield: 0.35
      duration: 0.25
      credit: 0.25
      cost: 0.15
  
  ModÃ©rÃ©:
    stocks:
      quality: 0.35
      value: 0.25
      momentum: 0.25
      risk: 0.15
    # ...
  
  Stable:
    stocks:
      quality: 0.40
      value: 0.30
      momentum: 0.10
      risk: 0.20
    # ...
```

#### Score par Asset Class

**Stocks**:
- Quality: ROIC, FCF Yield
- Value: PEG (inversÃ©), P/E (inversÃ©)
- Momentum: mom_3m, mom_12m
- Risk: vol_1y (inversÃ©), max_dd_1y (inversÃ©)

**ETFs Equity-like**:
- Cost: TER (inversÃ©)
- Liquidity: AUM, volume
- Momentum: mom_3m
- Tracking: tracking_error (inversÃ©)

**Bond ETFs**:
- Yield: yield_to_maturity
- Duration: duration (ajustÃ© au profil)
- Credit: credit_quality score
- Cost: TER (inversÃ©)

**Crypto** (si activÃ©):
- Momentum: mom_1m, mom_3m
- Risk: vol_1m (inversÃ©)
- Quality: tier, market_cap

**âš ï¸ HypothÃ¨se fragile #3**: Tu veux intÃ©grer "risk" dans le score alors que tu pÃ©nalises dÃ©jÃ  la vol en optimisation â†’ risque de double comptage. **Recommandation**: Risk lÃ©ger (0.15) ou zÃ©ro dans le score.

---

### Ã‰tape 6 â€” Coverage Penalty

**But**: Un actif incomplet doit Ãªtre pÃ©nalisÃ© mÃ©caniquement

**Livrable**: `coverage.py`

```python
def compute_coverage(feature_df: pd.DataFrame) -> pd.Series:
    """
    Ratio de features non-nulles par ticker
    Returns: Series index=ticker, values in [0, 1]
    """
    return (1.0 - feature_df.isna().mean(axis=1)).clip(0, 1)

def apply_coverage_penalty(
    score_z: pd.Series,
    coverage: pd.Series,
    min_multiplier: float = 0.5
) -> pd.Series:
    """
    score_final = score * (min_mult + (1-min_mult) * coverage)
    
    Exemples:
    - coverage=0 â†’ score Ã— 0.5
    - coverage=0.5 â†’ score Ã— 0.75
    - coverage=1 â†’ score Ã— 1.0
    """
    multiplier = min_multiplier + (1 - min_multiplier) * coverage
    return score_z * multiplier
```

**RÃ¨gle audit-friendly**: PÃ©nalitÃ© explicite et configurable, pas de "50 neutre" implicite.

---

### Ã‰tape 7 â€” IntÃ©gration Optimiseur

**But**: L'optimiseur prend `scores[ticker]` et optimise sous contraintes

**Ã€ NE PAS FAIRE**: CrÃ©er un `compute_final_weights()` heuristique qui bypass SLSQP.

**Ã€ FAIRE cÃ´tÃ© optimiseur** (dÃ©jÃ  dans patches P0/P1):
- PÃ©nalitÃ© HHI dans l'objectif
- PÃ©nalitÃ© turnover active
- `alternative_cap` comme vraie contrainte SLSQP

```python
# Dans optimizer.py
def optimize_portfolio_v2(
    scores_z: pd.Series,        # Depuis scoring_v5
    cov: np.ndarray,
    profile: ProfileConstraints,
    asset_metadata: List[Dict],
    w_prev: Optional[np.ndarray] = None
) -> OptimizationResult:
    """
    Utilise scores_z (PAS scores_0_100) dans l'objectif
    """
    scores = scores_z.values
    
    def objective(w):
        return objective_v2(
            w, scores, cov, 
            vol_target=profile.vol_target/100,
            w_prev=w_prev
        )
    
    # ... SLSQP avec contraintes v2
```

---

## ğŸ’» Code Squelette

### scoring_v5.py (complet)

```python
"""
Portfolio Engine - Scoring v5
Multi-factor scoring robuste et auditable
"""

from dataclasses import dataclass
from typing import Dict, Any, Tuple, List, Optional
import numpy as np
import pandas as pd


@dataclass(frozen=True)
class ScoreOutput:
    """RÃ©sultat du scoring v5"""
    scores_z: pd.Series                     # Z-scores (pour optimiseur)
    scores_0_100: pd.Series                 # Scores UI (0-100)
    breakdown: Dict[str, Dict[str, float]]  # ticker -> factor -> contribution
    coverage: pd.Series                     # Coverage ratio par ticker
    rejected: Dict[str, str]                # ticker -> rejection reason


def winsorize(s: pd.Series, lo: float = 0.01, hi: float = 0.99) -> pd.Series:
    """Clip aux percentiles pour robustesse outliers"""
    if s.dropna().empty:
        return s
    ql, qh = s.quantile(lo), s.quantile(hi)
    return s.clip(ql, qh)


def zscore(s: pd.Series) -> pd.Series:
    """Z-score cross-section"""
    s = s.astype(float)
    mu, sig = s.mean(), s.std(ddof=0)
    return (s - mu) / (sig + 1e-12)


def to_0_100(z: pd.Series) -> pd.Series:
    """SigmoÃ¯de douce vers 0-100"""
    return (100.0 / (1.0 + np.exp(-z.clip(-10, 10)))).clip(0, 100)


def compute_coverage(feature_df: pd.DataFrame) -> pd.Series:
    """Ratio de features non-nulles"""
    return (1.0 - feature_df.isna().mean(axis=1)).clip(0, 1)


def apply_coverage_penalty(
    score_z: pd.Series, 
    coverage: pd.Series,
    min_mult: float = 0.5
) -> pd.Series:
    """PÃ©nalise les actifs avec donnÃ©es manquantes"""
    mult = min_mult + (1 - min_mult) * coverage
    return score_z * mult


def normalize_and_weight(
    features: pd.DataFrame,
    feature_weights: Dict[str, Tuple[str, float]]  # {factor: (column, weight)}
) -> Tuple[pd.Series, Dict[str, pd.Series]]:
    """
    Normalise chaque feature et calcule le score pondÃ©rÃ©
    
    Args:
        features: DataFrame avec colonnes de features
        feature_weights: {factor_name: (column_name, weight)}
                        weight > 0 = higher is better
                        weight < 0 = lower is better
    
    Returns:
        (score_z, contributions_by_factor)
    """
    contributions = {}
    total_weight = sum(abs(w) for _, w in feature_weights.values())
    
    score = pd.Series(0.0, index=features.index)
    
    for factor, (col, weight) in feature_weights.items():
        if col not in features.columns:
            contributions[factor] = pd.Series(0.0, index=features.index)
            continue
        
        # Normalize
        z = zscore(winsorize(features[col]))
        
        # Direction: positive weight = higher is better
        # Si weight nÃ©gatif, on inverse (lower is better)
        if weight < 0:
            z = -z
            weight = abs(weight)
        
        # Contribution normalisÃ©e
        contrib = z * (weight / total_weight)
        contributions[factor] = contrib
        score += contrib
    
    return score, contributions


def score_stocks(
    features: pd.DataFrame,
    weights: Dict[str, float]
) -> Tuple[pd.Series, Dict[str, Dict[str, float]]]:
    """
    Score pour actions
    
    Features attendues: roic, fcf_yield, peg, pe_ratio, 
                       mom_3m, mom_12m, vol_1y, max_dd_1y
    """
    feature_weights = {
        # Quality (higher is better)
        "quality_roic": ("roic", weights.get("quality", 0.3) * 0.6),
        "quality_fcf": ("fcf_yield", weights.get("quality", 0.3) * 0.4),
        # Value (lower PEG/PE is better â†’ negative weight)
        "value_peg": ("peg", -weights.get("value", 0.2) * 0.6),
        "value_pe": ("pe_ratio", -weights.get("value", 0.2) * 0.4),
        # Momentum (higher is better)
        "mom_3m": ("mom_3m", weights.get("momentum", 0.3) * 0.5),
        "mom_12m": ("mom_12m", weights.get("momentum", 0.3) * 0.5),
        # Risk (lower vol/DD is better â†’ negative weight)
        "risk_vol": ("vol_1y", -weights.get("risk", 0.2) * 0.5),
        "risk_dd": ("max_dd_1y", -weights.get("risk", 0.2) * 0.5),
    }
    
    score_z, contribs = normalize_and_weight(features, feature_weights)
    
    # Build breakdown dict
    breakdown = {}
    for ticker in features.index:
        breakdown[ticker] = {
            factor: float(contrib.loc[ticker]) 
            for factor, contrib in contribs.items()
        }
    
    return score_z, breakdown


def score_etfs(
    features: pd.DataFrame,
    weights: Dict[str, float]
) -> Tuple[pd.Series, Dict[str, Dict[str, float]]]:
    """
    Score pour ETFs equity-like
    
    Features attendues: ter, aum, volume, mom_3m, tracking_error
    """
    feature_weights = {
        # Cost (lower TER is better)
        "cost_ter": ("ter", -weights.get("cost", 0.3)),
        # Liquidity (higher is better)
        "liq_aum": ("aum", weights.get("liquidity", 0.25) * 0.6),
        "liq_volume": ("volume", weights.get("liquidity", 0.25) * 0.4),
        # Momentum
        "mom_3m": ("mom_3m", weights.get("momentum", 0.3)),
        # Tracking (lower is better)
        "tracking": ("tracking_error", -weights.get("tracking", 0.15)),
    }
    
    score_z, contribs = normalize_and_weight(features, feature_weights)
    
    breakdown = {}
    for ticker in features.index:
        breakdown[ticker] = {
            factor: float(contrib.loc[ticker])
            for factor, contrib in contribs.items()
        }
    
    return score_z, breakdown


def score_bonds(
    features: pd.DataFrame,
    weights: Dict[str, float],
    profile_duration_target: Optional[float] = None
) -> Tuple[pd.Series, Dict[str, Dict[str, float]]]:
    """
    Score pour Bond ETFs
    
    Features attendues: ytm, duration, credit_score, ter
    """
    feature_weights = {
        # Yield (higher is better)
        "yield": ("ytm", weights.get("yield", 0.35)),
        # Duration (depends on profile - simplified: lower for Stable)
        "duration": ("duration", -weights.get("duration", 0.25) if profile_duration_target else 0),
        # Credit (higher score is better)
        "credit": ("credit_score", weights.get("credit", 0.25)),
        # Cost (lower is better)
        "cost": ("ter", -weights.get("cost", 0.15)),
    }
    
    score_z, contribs = normalize_and_weight(features, feature_weights)
    
    breakdown = {}
    for ticker in features.index:
        breakdown[ticker] = {
            factor: float(contrib.loc[ticker])
            for factor, contrib in contribs.items()
        }
    
    return score_z, breakdown


def compute_scores_v5(
    universe: pd.DataFrame,     # index=ticker, cols: asset_class, sector, etc.
    features: pd.DataFrame,     # index=ticker, cols: engineered features
    profile: str,
    config: Dict[str, Any],
) -> ScoreOutput:
    """
    Point d'entrÃ©e principal du scoring v5
    
    Args:
        universe: MÃ©tadonnÃ©es des actifs
        features: Features calculÃ©es (as-of date)
        profile: "Agressif", "ModÃ©rÃ©", "Stable"
        config: Configuration avec pondÃ©rations par profil
    
    Returns:
        ScoreOutput avec scores, breakdown, coverage, rejected
    """
    rejected: Dict[str, str] = {}
    eligible = universe.index.tolist()
    
    # 1) Apply quality filters (stub - implement in quality_filters.py)
    # rejected[ticker] = "reason"
    # eligible = [t for t in eligible if t not in rejected]
    
    u = universe.loc[eligible]
    f = features.loc[eligible].copy()
    
    # 2) Compute coverage
    coverage = compute_coverage(f)
    
    # 3) Initialize scores
    scores_z = pd.Series(0.0, index=eligible, dtype=float)
    breakdown: Dict[str, Dict[str, float]] = {t: {} for t in eligible}
    
    profile_config = config.get("profiles", {}).get(profile, {})
    
    # 4) Score by asset class
    for asset_class in ["stock", "etf", "bond_etf", "crypto"]:
        mask = u["asset_class"] == asset_class
        tickers = u[mask].index.tolist()
        
        if not tickers:
            continue
        
        f_subset = f.loc[tickers]
        weights = profile_config.get(f"{asset_class}s", {})
        
        if asset_class == "stock":
            s_z, bd = score_stocks(f_subset, weights)
        elif asset_class == "etf":
            s_z, bd = score_etfs(f_subset, weights)
        elif asset_class == "bond_etf":
            s_z, bd = score_bonds(f_subset, weights)
        else:
            # Crypto or other - simplified
            s_z = pd.Series(0.0, index=tickers)
            bd = {t: {} for t in tickers}
        
        scores_z.loc[tickers] = s_z
        for t in tickers:
            breakdown[t].update(bd.get(t, {}))
    
    # 5) Apply coverage penalty
    scores_z_adj = apply_coverage_penalty(scores_z, coverage)
    
    # 6) Convert to 0-100 for UI
    scores_0_100 = to_0_100(scores_z_adj)
    
    return ScoreOutput(
        scores_z=scores_z_adj,
        scores_0_100=scores_0_100,
        breakdown=breakdown,
        coverage=coverage,
        rejected=rejected,
    )
```

---

## ğŸ”„ IntÃ©gration Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE COMPLET v7                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. DATA LOADING                                                â”‚
â”‚     â””â”€â”€ instrument_master.json                                  â”‚
â”‚     â””â”€â”€ prices (as-of date t)                                   â”‚
â”‚     â””â”€â”€ fundamentals (lagged)                                   â”‚
â”‚                                                                 â”‚
â”‚  2. FEATURE ENGINEERING                                         â”‚
â”‚     â””â”€â”€ features_prices.py â†’ mom, vol, dd                       â”‚
â”‚     â””â”€â”€ features_fundamentals.py â†’ roic, fcf, peg               â”‚
â”‚                                                                 â”‚
â”‚  3. QUALITY FILTERS                                             â”‚
â”‚     â””â”€â”€ quality_filters.py â†’ eligible, rejected                 â”‚
â”‚                                                                 â”‚
â”‚  4. SCORING v5                                                  â”‚
â”‚     â””â”€â”€ scoring_v5.py â†’ scores_z, scores_0_100, breakdown       â”‚
â”‚                                                                 â”‚
â”‚  5. OPTIMIZATION                                                â”‚
â”‚     â””â”€â”€ optimizer.py (objective_v2 avec scores_z)               â”‚
â”‚     â””â”€â”€ constraints_v2 (bonds, crypto, vol, alternatives)       â”‚
â”‚                                                                 â”‚
â”‚  6. CONSTRAINT ORACLE                                           â”‚
â”‚     â””â”€â”€ constraint_oracle.py â†’ recalcul indÃ©pendant             â”‚
â”‚                                                                 â”‚
â”‚  7. GATING                                                      â”‚
â”‚     â””â”€â”€ apply_gating_policy() â†’ PASS/WARN/REJECT                â”‚
â”‚                                                                 â”‚
â”‚  8. EXPORT                                                      â”‚
â”‚     â””â”€â”€ portfolios.json (avec hyperparams, breakdown, coverage) â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš–ï¸ DÃ©cisions Ã  Trancher

### 1. Risk dans le score: OUI ou NON?

| Option | Avantage | InconvÃ©nient |
|--------|----------|--------------|
| **OUI (lÃ©ger 0.15)** | Favorise les actifs stables | Double comptage avec pÃ©nalitÃ© vol |
| **NON (0.0)** | SÃ©paration alpha/risk | Moins de contrÃ´le sur le risque individuel |

**Recommandation**: **LÃ©ger (0.15)** tant que pas de pÃ©nalitÃ© concentration/turnover active.

### 2. ETFs expositions sector/country: source fiable?

| Situation | Action |
|-----------|--------|
| **Source fiable** | PÃ©naliser concentration cachÃ©e |
| **Source absente/incomplÃ¨te** | Ne pas pÃ©naliser (ou WARN seulement) |

**Recommandation**: Commencer sans, ajouter plus tard avec source vÃ©rifiÃ©e.

---

## ğŸ§ª Tests Obligatoires

### Unit Tests

```python
# tests/test_scoring.py

def test_winsorize_outliers():
    """Outliers sont clippÃ©s"""
    s = pd.Series([1, 2, 3, 100])  # 100 est outlier
    result = winsorize(s, 0.01, 0.99)
    assert result.max() < 100

def test_zscore_mean_zero():
    """Z-score a mean=0, std=1"""
    s = pd.Series([1, 2, 3, 4, 5])
    z = zscore(s)
    assert abs(z.mean()) < 1e-10
    assert abs(z.std() - 1) < 0.1

def test_coverage_penalty():
    """Coverage 0.5 â†’ multiplier 0.75"""
    score = pd.Series([1.0])
    coverage = pd.Series([0.5])
    result = apply_coverage_penalty(score, coverage, min_mult=0.5)
    assert abs(result.iloc[0] - 0.75) < 1e-10

def test_score_monotonicity_ter():
    """TER plus bas â†’ score ETF plus haut"""
    features = pd.DataFrame({
        "ter": [0.10, 0.50, 0.20],
        "aum": [1e9, 1e9, 1e9],
        "volume": [1e6, 1e6, 1e6],
        "mom_3m": [0.05, 0.05, 0.05],
    }, index=["ETF_A", "ETF_B", "ETF_C"])
    
    weights = {"cost": 0.5, "liquidity": 0.25, "momentum": 0.25}
    scores, _ = score_etfs(features, weights)
    
    # ETF_A (TER 0.10) > ETF_C (TER 0.20) > ETF_B (TER 0.50)
    assert scores["ETF_A"] > scores["ETF_C"]
    assert scores["ETF_C"] > scores["ETF_B"]
```

### Integration Tests

```python
def test_full_pipeline_deterministic():
    """Pipeline complet sur univers figÃ© â†’ rÃ©sultats dÃ©terministes"""
    # Charger donnÃ©es de test
    # ExÃ©cuter scoring_v5
    # VÃ©rifier rÃ©sultats identiques Ã  chaque run

def test_anti_leakage():
    """Rejouer t-30 avec donnÃ©es tronquÃ©es â†’ mÃªme rÃ©sultat"""
    # Calculer features Ã  t
    # Calculer features Ã  t-30 avec donnÃ©es[:t-30]
    # Scores doivent Ãªtre cohÃ©rents (pas de futur utilisÃ©)
```

---

## ğŸ“š RÃ©fÃ©rences

- [Action Plan v7](./PORTFOLIO_ENGINE_ACTION_PLAN_v7.md)
- [Technical Specs v7](./PORTFOLIO_ENGINE_TECH_SPECS_v7.md)
- [Constraints Documentation](./constraints.md)
