# !/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des actualit√©s et √©v√©nements depuis Financial Modeling Prep
- General News API: Pour l'actualit√© √©conomique g√©n√©rale
- Stock News API: Pour les actions et ETF
- Crypto News API: Pour les cryptomonnaies
- Press Releases API: Pour les communiqu√©s de presse des entreprises
- FMP Articles API: Pour les articles r√©dig√©s par FMP
- IPOs Calendar: Pour les introductions en bourse √† venir
- Mergers & Acquisitions: Pour les op√©rations de fusion/acquisition
"""

import os
import json
import requests
import logging
from datetime import datetime, timedelta
import re
from collections import Counter

# Configuration du logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Chemins des fichiers
NEWS_JSON_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "news.json")
THEMES_JSON_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "themes.json")

# Configuration
CONFIG = {
    "api_key": os.environ.get("FMP_API_KEY", ""),
    "endpoints": {
        "general_news": "https://financialmodelingprep.com/stable/news/general-latest",
        "fmp_articles": "https://financialmodelingprep.com/stable/fmp-articles",
        "stock_news": "https://financialmodelingprep.com/stable/news/stock",
        "crypto_news": "https://financialmodelingprep.com/stable/news/crypto", 
        "press_releases": "https://financialmodelingprep.com/stable/news/press-releases",
        "earnings_calendar": "https://financialmodelingprep.com/api/v3/earning_calendar",
        "economic_calendar": "https://financialmodelingprep.com/api/v3/economic_calendar",
        "ipos_calendar": "https://financialmodelingprep.com/stable/ipos-calendar",
        "mergers_acquisitions": "https://financialmodelingprep.com/stable/mergers-acquisitions-latest"
    },
    "news_limits": {
        "general_news": 20,
        "fmp_articles": 1,
        "stock_news": 50,
        "crypto_news": 20,
        "press_releases": 1
    },
    "output_limits": {
        "us": 30,
        "france": 20,
        "uk": 15,
        "germany": 15,
        "china": 15,
        "japan": 10,
        "emerging_markets": 15,
        "global": 20
    },
    "max_total_articles": 150,  # Nombre maximum total d'articles √† conserver
    "days_ahead": 7,
    "days_back": 30
}

# Mots-cl√©s pour le score des actualit√©s
NEWS_KEYWORDS = {
    "high_impact": [
        "crash", "collapse", "crise", "recession", "fail", "bankruptcy", "r√©cession", "banque centrale", 
        "inflation", "hike", "drop", "plunge", "default", "fitch downgrade", "downgrade", "hausse des taux", 
        "bond yield", "yield curve", "sell-off", "bear market", "effondrement", "chute", "krach",
        "d√©gringolade", "catastrophe", "urgence", "alerte", "d√©faut", "risque", "choc", "contagion",
        "panique", "d√©faillance", "correction", "faillite", "taux directeur"
    ],
    "medium_impact": [
        "growth", "expansion", "job report", "fed decision", "quarterly earnings", "acquisition", 
        "ipo", "merger", "partnership", "profit warning", "bond issuance", "croissance", "emploi", 
        "rapport", "BCE", "FED", "r√©sultats trimestriels", "fusion", "acquisition", "partenariat",
        "b√©n√©fices", "√©mission obligataire", "√©mission d'obligations", "perspectives", "avertissement",
        "rachat", "introduction en bourse", "nouveau PDG", "restructuration"
    ],
    "low_impact": [
        "recommendation", "stock buyback", "dividend", "announcement", "management change", "forecast",
        "recommandation", "rachat d'actions", "dividende", "annonce", "changement de direction", "pr√©vision",
        "nomination", "produit", "service", "strat√©gie", "march√©", "plan", "mise √† jour", "tendance"
    ]
}

# Structure des th√®mes dominants
THEMES_DOMINANTS = {
    "macroeconomie": {
        "inflation": ["inflation", "prix", "CPI", "taux d'int√©r√™t", "interest rate", "yield"],
        "recession": ["recession", "slowdown", "GDP", "PIB", "croissance", "crise"],
        "politique_monetaire": ["fed", "bce", "banque centrale", "tapering", "quantitative easing"],
        "geopolitique": ["conflit", "guerre", "tensions", "ukraine", "israel", "chine", "taiwan"],
        "transition_energetique": ["climat", "esg", "biodiversit√©", "net zero", "transition", "durable"]
    },
    "secteurs": {
        "technologie": ["ai", "cloud", "cyber", "tech", "semiconducteur", "digital", "data"],
        "√©nergie": ["p√©trole", "gas", "uranium", "√©nergie", "baril", "oil", "renouvelable"],
        "d√©fense": ["d√©fense", "militaire", "armes", "nato", "r√©armement"],
        "finance": ["banques", "assurances", "taux", "obligations", "treasury"],
        "immobilier": ["real estate", "immobilier", "epra", "infrastructure"],
        "consommation": ["retail", "consommation", "luxe", "achat", "revenu disponible"],
        "sant√©": ["sant√©", "biotech", "pharma", "vaccin", "fda", "clinical trial", "m√©dicament"],
        "industrie": ["industrie", "manufacturing", "usine", "production", "automation", "supply chain"],
        "transport": ["logistique", "transport", "shipping", "camion", "port", "airline"],
        "agriculture": ["wheat", "corn", "cacao", "agriculture", "engrais", "fertilizer", "commodities"]
    },
    "regions": {
        "europe": ["europe", "france", "bce", "allemagne", "italie", "zone euro", "ue", "union europ√©enne"],
        "usa": ["usa", "fed", "s&p", "nasdaq", "dow jones", "√©tats-unis"],
        "asie": ["chine", "japon", "cor√©e", "inde", "asie", "emerging asia"],
        "latam": ["br√©sil", "mexique", "latam", "am√©rique latine"],
        "canada": ["canada", "ottawa", "toronto", "quebec"],
        "australie": ["australie", "sydney", "aussie", "asx"],
        "afrique": ["nigeria", "afrique", "south africa", "johannesburg", "kenya", "lagos"],
        "blocs": ["asean", "ocde", "brics", "opep", "nato", "g7", "g20"],
        "global": ["monde", "acwi", "international", "global", "tous march√©s"]
    }
}

# Sources importantes par cat√©gorie (pour le calcul du score)
IMPORTANT_SOURCES = {
    "general_news": [
        "Bloomberg", "Reuters", "Financial Times", "Wall Street Journal", "CNBC", 
        "BBC", "New York Times", "The Economist", "Les Echos", "Le Monde", "La Tribune"
    ],
    "stock_news": [
        "Bloomberg", "Reuters", "CNBC", "MarketWatch", "Seeking Alpha", "Barron's", 
        "Investor's Business Daily", "Motley Fool", "Morningstar", "Yahoo Finance"
    ],
    "crypto_news": [
        "CoinDesk", "Cointelegraph", "The Block", "Decrypt", "Bitcoin Magazine", 
        "CryptoSlate", "Bitcoinist", "CoinMarketCap", "Crypto Briefing"
    ],
    "press_releases": [
        "PR Newswire", "Business Wire", "Globe Newswire", "MarketWatch", "Yahoo Finance",
        "Company Website", "SEC Filing", "Investor Relations"
    ]
}

# Mots-cl√©s importants par cat√©gorie (pour le calcul du score)
HIGH_IMPORTANCE_KEYWORDS = {
    "general_news": [
        "recession", "inflation", "fed", "central bank", "interest rate", "gdp", 
        "unemployment", "market crash", "crisis", "economic growth", "federal reserve",
        "treasury", "ecb", "bce", "default", "geopolitical", "war", "conflict"
    ],
    "stock_news": [
        "earnings", "beat", "miss", "guidance", "outlook", "upgrade", "downgrade", 
        "acquisition", "merger", "ipo", "buyback", "dividend", "profit", "loss",
        "revenue", "forecast", "ceo", "executive", "lawsuit", "regulation"
    ],
    "crypto_news": [
        "bitcoin", "ethereum", "blockchain", "altcoin", "defi", "nft", "regulation", 
        "adoption", "halving", "mining", "exchange", "wallet", "staking", "sec", 
        "token", "smart contract", "dao", "hack", "security", "volatile"
    ],
    "press_releases": [
        "announce", "launch", "partnership", "collaboration", "expansion", 
        "appointment", "award", "contract", "patent", "breakthrough", "milestone", 
        "revenue", "financial results", "quarterly", "annual report"
    ]
}

# Mots-cl√©s d'importance moyenne par cat√©gorie
MEDIUM_IMPORTANCE_KEYWORDS = {
    "general_news": [
        "policy", "regulation", "trade", "budget", "deficit", "surplus", "consumer", 
        "confidence", "retail", "manufacturing", "services", "housing", "real estate"
    ],
    "stock_news": [
        "stock", "shares", "investor", "market", "trading", "performance", "index", 
        "sector", "industry", "competition", "strategy", "launch", "product", "service"
    ],
    "crypto_news": [
        "crypto", "digital asset", "coin", "market cap", "investment", "analyst", 
        "prediction", "whale", "memecoin", "correction", "rally", "bullish", "bearish"
    ],
    "press_releases": [
        "report", "update", "invest", "development", "growth", "statement", 
        "comment", "response", "release", "event", "conference", "meeting"
    ]
}

def read_existing_news():
    """Lit le fichier JSON existant comme fallback"""
    try:
        with open(NEWS_JSON_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return None

def fetch_api_data(endpoint, params=None):
    """Fonction g√©n√©rique pour r√©cup√©rer des donn√©es depuis l'API FMP"""
    if not CONFIG["api_key"]:
        logger.error("Cl√© API FMP non d√©finie. Veuillez d√©finir FMP_API_KEY dans les variables d'environnement.")
        return []
        
    if params is None:
        params = {}
    
    params["apikey"] = CONFIG["api_key"]
    
    try:
        logger.info(f"R√©cup√©ration des donn√©es depuis {endpoint} avec params {params}")
        response = requests.get(endpoint, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        logger.info(f"‚úÖ {len(data)} √©l√©ments r√©cup√©r√©s depuis {endpoint}")
        return data
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration depuis {endpoint}: {str(e)}")
        return []

def fetch_articles_by_period(endpoint, start_date, end_date, source_type=None, days_interval=7, max_pages=5):
    """
    R√©cup√®re des articles sur une p√©riode donn√©e en d√©coupant la p√©riode en intervalles
    et en utilisant la pagination pour obtenir un maximum d'articles
    """
    logger.info(f"D√©marrage de l'extraction d'articles de {start_date} √† {end_date} par tranches de {days_interval} jours")
    
    # Utiliser la limite sp√©cifique √† la source ou 50 par d√©faut
    per_page = CONFIG["news_limits"].get(source_type, 50) if source_type else 50
    
    from_date = datetime.strptime(start_date, "%Y-%m-%d")
    to_date = datetime.strptime(end_date, "%Y-%m-%d")
    all_articles = []
    
    # Parcourir la p√©riode par intervalles
    current_from = from_date
    while current_from < to_date:
        current_to = min(current_from + timedelta(days=days_interval), to_date)
        
        logger.info(f"Traitement de la p√©riode {current_from.strftime('%Y-%m-%d')} ‚Üí {current_to.strftime('%Y-%m-%d')}")
        
        # Parcourir les pages pour chaque intervalle
        for page in range(max_pages):
            params = {
                "from": current_from.strftime("%Y-%m-%d"),
                "to": current_to.strftime("%Y-%m-%d"),
                "page": page,
                "limit": per_page
            }
            
            articles = fetch_api_data(endpoint, params)
            
            if not articles:
                break  # Plus d'articles pour cette p√©riode
                
            logger.info(f"  Page {page+1}: {len(articles)} articles r√©cup√©r√©s")
            all_articles.extend(articles)
            
            # Si on a r√©cup√©r√© moins d'articles que la limite, on a atteint la fin
            if len(articles) < per_page:
                break
                
        # Passer √† l'intervalle suivant
        current_from = current_to
    
    logger.info(f"Total d'articles r√©cup√©r√©s sur la p√©riode: {len(all_articles)}")
    return all_articles

def get_general_news():
    """R√©cup√®re les actualit√©s √©conomiques g√©n√©rales"""
    # R√©cup√©rer les actualit√©s des 30 derniers jours
    today = datetime.today()
    start_date = (today - timedelta(days=CONFIG["days_back"])).strftime("%Y-%m-%d")
    end_date = today.strftime("%Y-%m-%d")
    
    return fetch_articles_by_period(CONFIG["endpoints"]["general_news"], start_date, end_date, "general_news")

def get_fmp_articles():
    """R√©cup√®re les articles r√©dig√©s par FMP"""
    # R√©cup√©rer les articles des 30 derniers jours
    today = datetime.today()
    start_date = (today - timedelta(days=CONFIG["days_back"])).strftime("%Y-%m-%d")
    end_date = today.strftime("%Y-%m-%d")
    
    return fetch_articles_by_period(CONFIG["endpoints"]["fmp_articles"], start_date, end_date, "fmp_articles")

def get_stock_news():
    """R√©cup√®re les actualit√©s des actions"""
    # R√©cup√©rer les actualit√©s des 30 derniers jours
    today = datetime.today()
    start_date = (today - timedelta(days=CONFIG["days_back"])).strftime("%Y-%m-%d")
    end_date = today.strftime("%Y-%m-%d")
    
    return fetch_articles_by_period(CONFIG["endpoints"]["stock_news"], start_date, end_date, "stock_news")

def get_crypto_news():
    """R√©cup√®re les actualit√©s des cryptomonnaies"""
    # R√©cup√©rer les actualit√©s des 30 derniers jours
    today = datetime.today()
    start_date = (today - timedelta(days=CONFIG["days_back"])).strftime("%Y-%m-%d")
    end_date = today.strftime("%Y-%m-%d")
    
    return fetch_articles_by_period(CONFIG["endpoints"]["crypto_news"], start_date, end_date, "crypto_news")

def get_press_releases():
    """R√©cup√®re les communiqu√©s de presse"""
    # R√©cup√©rer les communiqu√©s de presse des 30 derniers jours
    today = datetime.today()
    start_date = (today - timedelta(days=CONFIG["days_back"])).strftime("%Y-%m-%d")
    end_date = today.strftime("%Y-%m-%d")
    
    return fetch_articles_by_period(CONFIG["endpoints"]["press_releases"], start_date, end_date, "press_releases")

def get_earnings_calendar():
    """R√©cup√®re le calendrier des r√©sultats d'entreprises"""
    today = datetime.now().strftime("%Y-%m-%d")
    future = (datetime.now() + timedelta(days=CONFIG["days_ahead"])).strftime("%Y-%m-%d")
    
    params = {
        "from": today,
        "to": future
    }
    return fetch_api_data(CONFIG["endpoints"]["earnings_calendar"], params)

def get_economic_calendar():
    """R√©cup√®re le calendrier √©conomique"""
    today = datetime.now().strftime("%Y-%m-%d")
    future = (datetime.now() + timedelta(days=CONFIG["days_ahead"])).strftime("%Y-%m-%d")
    
    params = {
        "from": today,
        "to": future
    }
    return fetch_api_data(CONFIG["endpoints"]["economic_calendar"], params)

def get_ipos_calendar():
    """R√©cup√®re les introductions en bourse √† venir"""
    today = datetime.now().strftime("%Y-%m-%d")
    future = (datetime.now() + timedelta(days=CONFIG["days_ahead"])).strftime("%Y-%m-%d")

    params = {
        "from": today,
        "to": future
    }

    return fetch_api_data(CONFIG["endpoints"]["ipos_calendar"], params)

def get_mergers_acquisitions(limit=100):
    """R√©cup√®re les derni√®res op√©rations de fusion/acquisition"""
    params = {
        "page": 0,
        "limit": limit
    }
    return fetch_api_data(CONFIG["endpoints"]["mergers_acquisitions"], params)

def extract_themes(article):
    """Identifie les th√®mes dominants √† partir du contenu de l'article"""
    text = (article.get("text", "") + " " + article.get("title", "")).lower()
    themes_detected = {"macroeconomie": [], "secteurs": [], "regions": []}
    
    for axe, groupes in THEMES_DOMINANTS.items():
        for theme, keywords in groupes.items():
            if any(kw in text for kw in keywords):
                themes_detected[axe].append(theme)

    return themes_detected

def compute_sentiment_distribution(articles):
    """Calcule la distribution des sentiments pour un ensemble d'articles"""
    sentiment_counts = Counter(article["impact"] for article in articles if "impact" in article)
    total = sum(sentiment_counts.values())

    if total == 0:
        return {"positive": 0.0, "neutral": 0.0, "negative": 0.0}

    return {
        "positive": round(sentiment_counts["positive"] / total * 100, 1),
        "neutral": round(sentiment_counts["neutral"] / total * 100, 1),
        "negative": round(sentiment_counts["negative"] / total * 100, 1)
    }

def determine_category(article, source=None):
    """
    D√©termine la cat√©gorie de l'actualit√©:
    - economie: actualit√©s macro-√©conomiques
    - marches: actualit√©s sur les indices, ETF, etc.
    - entreprises: actualit√©s sp√©cifiques aux entreprises
    - crypto: actualit√©s cryptomonnaies
    - tech: actualit√©s technologiques
    """
    # V√©rification du symbole pour la crypto
    if article.get("symbol") and any(ticker in str(article.get("symbol")) for ticker in ["BTC", "ETH", "CRYPTO", "COIN"]):
        return "crypto"
        
    # Analyse du texte pour d√©terminer la cat√©gorie
    text = (article.get("text", "") + " " + article.get("title", "")).lower()
    
    # Cat√©gorie crypto (priorit√© 1)
    crypto_keywords = [
        "crypto", "bitcoin", "ethereum", "blockchain", "token", "defi", "nft", 
        "altcoin", "binance", "coinbase", "mining", "miner", "wallet", "staking",
        "web3", "cryptocurrency"
    ]
    
    if any(word in text for word in crypto_keywords):
        return "crypto"
    
    # Cat√©gorie tech (priorit√© 2)
    tech_keywords = [
        "ai", "artificial intelligence", "machine learning", "data science", 
        "software", "hardware", "tech", "technology", "startup", "app", 
        "mobile", "cloud", "computing", "digital", "internet", "online", "web"
    ]
    
    if any(word in text for word in tech_keywords):
        return "tech"
    
    # Cat√©gorie √©conomie (priorit√© 3)
    economie_keywords = [
        "economy", "√©conomie", "inflation", "gdp", "pib", "fed", "central bank",
        "banque centrale", "interest rate", "taux d'int√©r√™t", "economic", "unemployment",
        "consumer", "spending", "policy", "fiscal", "monetary", "recession"
    ]
    
    if any(word in text for word in economie_keywords):
        return "economie"
    
    # Cat√©gorie march√©s (priorit√© 4)
    marches_keywords = [
        "etf", "fund", "fonds", "index", "indice", "s&p", "dow", "cac", "nasdaq", 
        "bond", "treasury", "yield", "mati√®res premi√®res", "commodities", "oil", 
        "gold", "p√©trole", "or", "market", "stock market", "bull market", 
        "bear market", "rally", "correction", "volatility", "vix"
    ]
    
    if any(word in text for word in marches_keywords):
        return "marches"
    
    # Par d√©faut: entreprises
    return "entreprises"

def determine_country(article):
    """
    D√©termine le pays/r√©gion de l'actualit√© en utilisant une analyse plus d√©taill√©e
    pour d√©tecter davantage de pays que juste france/us
    """
    # V√©rifier le symbole pour information initiale
    symbol = article.get("symbol", "")
    if symbol:
        if any(suffix in str(symbol) for suffix in [".PA", ".PAR"]):
            return "france"
        elif any(suffix in str(symbol) for suffix in [".L", ".LON"]):
            return "uk"
        elif any(suffix in str(symbol) for suffix in [".DE", ".FRA", ".XE"]):
            return "germany"
        elif any(suffix in str(symbol) for suffix in [".SS", ".SZ", ".HK"]):
            return "china"
        elif any(suffix in str(symbol) for suffix in [".T", ".JP"]):
            return "japan"
    
    # Analyse du texte pour une d√©tection plus pr√©cise
    text = (article.get("text", "") + " " + article.get("title", "")).lower()
    
    # Keywords pour diff√©rents pays/r√©gions
    country_keywords = {
        "france": [
            "france", "fran√ßais", "paris", "cac", "bourse de paris", "euronext", "amf", 
            "autorit√© des march√©s", "bercy", "matignon", "elys√©e", "bpifrance", "fran√ßaise", "hexagone"
        ],
        "uk": [
            "uk", "united kingdom", "britain", "british", "london", "ftse", "bank of england", 
            "pound sterling", "gbp", "boe", "royal", "london stock exchange", "britain"
        ],
        "germany": [
            "germany", "allemagne", "berlin", "frankfurt", "dax", "deutsche", "euro", "ecb", 
            "bundesbank", "merkel", "scholz", "german", "allemand"
        ],
        "china": [
            "china", "chinese", "beijing", "shanghai", "hong kong", "shenzhen", "yuan", "renminbi", 
            "pboc", "ccp", "xi jinping", "chinois", "chine"
        ],
        "japan": [
            "japan", "japanese", "tokyo", "nikkei", "yen", "bank of japan", "boj", "abenomics", 
            "japon", "japonais", "kishida", "abe", "jpx"
        ],
        "emerging_markets": [
            "emerging markets", "emerging economies", "brics", "brazil", "russia", "india", 
            "south africa", "indonesia", "turkey", "mexico", "thailand", "vietnam", 
            "manila", "mumbai", "bovespa", "sensex", "micex"
        ],
        "global": [
            "global", "world", "international", "worldwide", "global economy", "global markets",
            "mondial", "monde", "international", "all markets", "across markets"
        ]
    }
    
    # V√©rifier chaque pays/r√©gion par ordre de priorit√©
    for country, keywords in country_keywords.items():
        if any(keyword in text for keyword in keywords):
            return country
    
    # Par d√©faut: "us" (march√© le plus important globalement)
    return "us"

def determine_impact(article):
    """D√©termine l'impact de l'actualit√© (positive/negative/neutral)"""
    # Si le sentiment est fourni par l'API
    sentiment = article.get("sentiment")
    if sentiment:
        try:
            sentiment_value = float(sentiment)
            if sentiment_value > 0.2:
                return "positive"
            elif sentiment_value < -0.2:
                return "negative"
            else:
                return "neutral"
        except:
            pass
    
    # Analyse basique du texte si pas de sentiment fourni
    text = (article.get("text", "") + " " + article.get("title", "")).lower()
    
    positive_words = [
        "surge", "soar", "gain", "rise", "jump", "boost", "recovery", "profit", 
        "beat", "success", "bullish", "upward", "rally", "outperform", "growth",
        "positive", "optimistic", "momentum", "exceed", "improvement", "confidence",
        "strong", "strength", "uptick", "upgrade", "hausse", "progresse", "augmente"
    ]
    
    negative_words = [
        "drop", "fall", "decline", "loss", "plunge", "tumble", "crisis", "risk", 
        "warning", "concern", "bearish", "downward", "slump", "underperform", "recession",
        "negative", "pessimistic", "weakness", "miss", "downgrade", "cut", "reduction",
        "pressure", "struggle", "slowdown", "baisse", "chute", "recule", "diminue" 
    ]
    
    positive_count = sum(1 for word in positive_words if word in text)
    negative_count = sum(1 for word in negative_words if word in text)
    
    if positive_count > negative_count:
        return "positive"
    elif negative_count > positive_count:
        return "negative"
    else:
        return "neutral"

def format_date(date_str):
    """Formate une date au format YYYY-MM-DD en DD/MM/YYYY"""
    try:
        date_parts = date_str.split(" ")[0].split("-")
        if len(date_parts) == 3:
            return f"{date_parts[2]}/{date_parts[1]}/{date_parts[0]}"
        return date_str.replace("-", "/")
    except:
        # Fallback en cas d'erreur
        return date_str.replace("-", "/")

def format_time(date_str):
    """Extrait l'heure au format HH:MM √† partir d'une date compl√®te"""
    try:
        time_parts = date_str.split(" ")[1].split(":")
        if len(time_parts) >= 2:
            return f"{time_parts[0]}:{time_parts[1]}"
        return "00:00"
    except:
        # Fallback en cas d'erreur
        return "00:00"

def normalize_article(article, source=None):
    """Normalise les diff√©rents formats d'articles FMP en un format standard"""
    # G√©rer les diff√©rents formats selon l'endpoint
    
    # D√©terminer les champs cl√©s
    title = article.get("title", "")
    
    # Pour FMP Articles API
    if "date" in article and "content" in article and "tickers" in article:
        text = article.get("content", "")
        date = article.get("date", "")
        symbol = article.get("tickers", "")
        site = article.get("site", "Financial Modeling Prep")
        url = article.get("link", "")
    # Pour les autres endpoints
    else:
        text = article.get("text", "")
        date = article.get("publishedDate", "")
        symbol = article.get("symbol", "")
        site = article.get("site", article.get("publisher", ""))
        url = article.get("url", "")
    
    # Retourner un article normalis√©
    return {
        "title": title,
        "text": text,
        "publishedDate": date,
        "symbol": symbol,
        "site": site,
        "url": url,
        "source_type": source  # Ajout du type de source pour la classification
    }

def remove_duplicates(news_list):
    """Supprime les articles en double bas√©s sur le titre"""
    seen_titles = set()
    unique_news = []
    
    for item in news_list:
        title = item["title"].lower()
        if title not in seen_titles:
            seen_titles.add(title)
            unique_news.append(item)
    
    return unique_news

def compute_importance_score(article, category):
    """
    Calcule un score d'importance pour un article en fonction de sa cat√©gorie et de son contenu.
    
    Args:
        article (dict): L'article contenant title, content, source, etc.
        category (str): La cat√©gorie de l'article (general_news, stock_news, crypto_news, press_releases)
    
    Returns:
        float: Score d'importance entre 0 et 100
    """
    score = 0
    
    # Combinaison du titre et du texte pour analyse
    content = f"{article.get('title', '')} {article.get('content', '')}".lower()
    
    # 1. Score bas√© sur les mots-cl√©s de haute importance (max 40 points)
    high_keywords = HIGH_IMPORTANCE_KEYWORDS.get(category, [])
    matched_high_keywords = set()
    for keyword in high_keywords:
        if keyword in content:
            matched_high_keywords.add(keyword)
    
    high_keyword_score = min(40, len(matched_high_keywords) * 5)
    
    # 2. Score bas√© sur les mots-cl√©s d'importance moyenne (max 20 points)
    medium_keywords = MEDIUM_IMPORTANCE_KEYWORDS.get(category, [])
    matched_medium_keywords = set()
    for keyword in medium_keywords:
        if keyword in content:
            matched_medium_keywords.add(keyword)
    
    medium_keyword_score = min(20, len(matched_medium_keywords) * 2.5)
    
    # 3. Score bas√© sur la source (max 20 points)
    source_score = 0
    article_source = article.get("source", "").lower()
    for important_source in IMPORTANT_SOURCES.get(category, []):
        if important_source.lower() in article_source:
            source_score = 20
            break
    
    # 4. Score bas√© sur la longueur du titre et du contenu (max 10 points)
    title_length = len(article.get("title", ""))
    text_length = len(article.get("content", ""))
    
    title_score = min(5, title_length / 20)  # 5 points max pour un titre de 100 caract√®res ou plus
    text_score = min(5, text_length / 500)   # 5 points max pour un texte de 2500 caract√®res ou plus
    
    # 5. Score bas√© sur l'impact (max 10 points)
    impact_score = 0
    impact = article.get("impact", "neutral")
    if impact == "negative":
        impact_score = 10  # Les actualit√©s n√©gatives sont souvent plus impactantes
    elif impact == "positive":
        impact_score = 8
    else:
        impact_score = 5
    
    # Calcul du score total
    total_score = high_keyword_score + medium_keyword_score + source_score + title_score + text_score + impact_score
    
    # Normalisation entre 0 et 100
    normalized_score = min(100, total_score)
    
    return normalized_score

def calculate_output_limits(articles_by_country, max_total=150):
    """
    Calcule les limites de sortie pour chaque pays/r√©gion en fonction des articles disponibles
    et de leur importance.
    
    Args:
        articles_by_country (dict): Dictionnaire des articles par pays
        max_total (int): Nombre maximum total d'articles √† conserver
    
    Returns:
        dict: Limites pour chaque pays/r√©gion
    """
    # Configuration de base des limites par pays/r√©gion
    base_limits = CONFIG["output_limits"]
    
    # Compter les articles par pays
    country_counts = {country: len(articles) for country, articles in articles_by_country.items()}
    
    # Ajuster les limites en fonction des articles disponibles
    adjusted_limits = {}
    remaining_quota = max_total
    
    # Premi√®re passe : attribuer un minimum pour chaque pays qui a des articles
    for country, count in country_counts.items():
        # Si pays non d√©fini dans base_limits, le consid√©rer comme global
        if country not in base_limits:
            if "global" not in country_counts:
                country_counts["global"] = 0
            country_counts["global"] += count
            continue
            
        min_limit = min(count, max(5, base_limits.get(country, 10) // 2))
        adjusted_limits[country] = min_limit
        remaining_quota -= min_limit
    
    # Assurer que global est pris en compte m√™me s'il n'a pas d'articles
    if "global" not in adjusted_limits and "global" in base_limits:
        adjusted_limits["global"] = 0
    
    # Deuxi√®me passe : distribuer le quota restant proportionnellement
    if remaining_quota > 0:
        # Calculer le total des limites de base pour les pays avec des articles
        total_base = sum(base_limits.get(country, 10) for country in adjusted_limits.keys())
        
        # Distribuer proportionnellement
        for country in list(adjusted_limits.keys()):  # Utiliser une copie des cl√©s
            if total_base > 0:
                country_ratio = base_limits.get(country, 10) / total_base
                additional = int(remaining_quota * country_ratio)
                adjusted_limits[country] += additional
                remaining_quota -= additional
        
        # Attribuer tout quota restant √† global ou au premier pays si global n'existe pas
        if "global" in adjusted_limits:
            adjusted_limits["global"] += remaining_quota
        elif adjusted_limits:
            first_country = next(iter(adjusted_limits))
            adjusted_limits[first_country] += remaining_quota
    
    return adjusted_limits

def determine_event_impact(event):
    """D√©termine le niveau d'impact d'un √©v√©nement √©conomique"""
    # √âv√©nements √† fort impact
    high_impact_events = [
        "Interest Rate Decision", "Fed Interest Rate", "ECB Interest Rate", 
        "Inflation Rate", "GDP Growth", "GDP Release", "Employment Change",
        "Unemployment Rate", "Non-Farm Payrolls", "CPI", "Retail Sales",
        "FOMC", "FED", "BCE", "ECB", "Fed Chair", "Treasury", "Central Bank"
    ]
    
    # √âv√©nements √† impact moyen
    medium_impact_events = [
        "PMI", "Consumer Confidence", "Trade Balance", "Industrial Production",
        "Manufacturing Production", "Housing Starts", "Building Permits",
        "Durable Goods Orders", "Factory Orders", "Earnings Report", "Balance Sheet"
    ]
    
    # V√©rifier le nom de l'√©v√©nement
    event_name = event.get("event", "").lower()
    
    if any(keyword.lower() in event_name for keyword in high_impact_events):
        return "high"
    
    if any(keyword.lower() in event_name for keyword in medium_impact_events):
        return "medium"
    
    # V√©rifier si l'√©v√©nement est d√©j√† class√© par FMP
    if event.get("impact") == "High":
        return "high"
    elif event.get("impact") == "Medium":
        return "medium"
    
    # Par d√©faut, impact faible
    return "low"

def calculate_event_score(event):
    """Calcule un score pour hi√©rarchiser l'importance des √©v√©nements √©conomiques"""
    score = 0
    
    # Impact de l'√©v√©nement
    impact = determine_event_impact(event)
    if impact == "high":
        score += 10
    elif impact == "medium":
        score += 5
    else:
        score += 1
    
    # Bonus pour les √âtats-Unis (march√© influent)
    if event.get("country") == "US" or event.get("country") == "United States":
        score += 3
    
    # Bonus pour les √©v√©nements avec √©cart important vs pr√©visions
    if event.get("actual") and event.get("forecast"):
        try:
            actual = float(event.get("actual").replace("%", ""))
            forecast = float(event.get("forecast").replace("%", ""))
            diff = abs(actual - forecast)
            
            if diff > 5:
                score += 5  # √âcart tr√®s important
            elif diff > 2:
                score += 3  # √âcart significatif
            elif diff > 0.5:
                score += 1  # √âcart notable
        except (ValueError, AttributeError):
            # Si on ne peut pas convertir en float, on ignore
            pass
    
    # Ajustement par type d'√©v√©nement pour les r√©sultats d'entreprises
    if event.get("type") == "earnings":
        # Essayer d'extraire le symbole de l'action des r√©sultats
        title = event.get("title", "")
        
        # Bonus pour les entreprises importantes
        major_companies = ["AAPL", "MSFT", "GOOGL", "AMZN", "META", "NVDA", "TSLA", "JPM", "V", "PYPL", "DIS"]
        if any(company in title for company in major_companies):
            score += 3
    
    return score

def extract_top_themes(news_data, days=30, max_examples=3):
    """Analyse les th√®mes dominants sur une p√©riode donn√©e (ex: 30 jours) avec analyse d√©taill√©e des mots-cl√©s"""
    cutoff_date = datetime.now() - timedelta(days=days)
    
    # Compteur simple pour les 5 th√®mes les plus fr√©quents
    themes_counter = {
        "macroeconomie": Counter(),
        "secteurs": Counter(),
        "regions": Counter()
    }
    
    # Structure avanc√©e pour stocker les d√©tails de chaque th√®me
    themes_details = {
        "macroeconomie": {},
        "secteurs": {},
        "regions": {}
    }
    
    # Collection d'articles par th√®me pour calculer la distribution des sentiments
    theme_articles = {
        "macroeconomie": {},
        "secteurs": {},
        "regions": {}
    }
    
    total_articles = 0
    processed_articles = 0
    
    for country_articles in news_data.values():
        if not isinstance(country_articles, list):
            continue
        
        total_articles += len(country_articles)
        
        for article in country_articles:
            # Utiliser rawDate si disponible, sinon fallback sur date format√©e
            try:
                if "rawDate" in article:
                    # Format YYYY-MM-DD HH:MM:SS
                    article_date = datetime.strptime(article["rawDate"].split(" ")[0], "%Y-%m-%d")
                else:
                    # Format DD/MM/YYYY (pour compatibilit√© avec anciennes donn√©es)
                    article_date = datetime.strptime(article["date"], "%d/%m/%Y")
                
                if article_date < cutoff_date:
                    continue
                
                processed_articles += 1
                
                themes = article.get("themes", {})
                for axe, subthemes in themes.items():
                    for theme in subthemes:
                        # Collecte pour le calcul des sentiments plus tard
                        if theme not in theme_articles[axe]:
                            theme_articles[axe][theme] = []
                        theme_articles[axe][theme].append(article)
                        
                        # Mettre √† jour le compteur simple
                        themes_counter[axe][theme] += 1
                        
                        # Initialiser la structure d√©taill√©e si elle n'existe pas encore
                        if theme not in themes_details[axe]:
                            themes_details[axe][theme] = {
                                "count": 0,
                                "examples": [],
                                "keywords": {}
                            }
                        
                        # Incr√©menter le compteur dans la structure d√©taill√©e
                        themes_details[axe][theme]["count"] += 1
                        
                        # Ajouter l'exemple (limit√© √† max_examples)
                        title = article.get("title", "")
                        if (len(themes_details[axe][theme]["examples"]) < max_examples and 
                            title not in themes_details[axe][theme]["examples"]):
                            themes_details[axe][theme]["examples"].append(title)
                        
                        # Analyser les mots-cl√©s sp√©cifiques
                        text = (article.get("content", "") or article.get("text", "") + " " + title).lower()
                        
                        # R√©cup√©rer la liste des mots-cl√©s pour ce th√®me
                        if axe in THEMES_DOMINANTS and theme in THEMES_DOMINANTS[axe]:
                            keywords = THEMES_DOMINANTS[axe][theme]
                            for keyword in keywords:
                                if keyword.lower() in text:
                                    if keyword not in themes_details[axe][theme]["keywords"]:
                                        themes_details[axe][theme]["keywords"][keyword] = {
                                            "count": 0,
                                            "examples": []
                                        }
                                    # Incr√©menter le compteur du mot-cl√©
                                    themes_details[axe][theme]["keywords"][keyword]["count"] += 1
                                    # Ajouter l'exemple pour ce mot-cl√© sp√©cifique
                                    if (len(themes_details[axe][theme]["keywords"][keyword]["examples"]) < max_examples and
                                        title not in themes_details[axe][theme]["keywords"][keyword]["examples"]):
                                        themes_details[axe][theme]["keywords"][keyword]["examples"].append(title)
                
            except Exception as e:
                logger.warning(f"Article ignor√© pour date invalide: {article.get('title')} | Erreur: {str(e)}")
                continue
    
    logger.info(f"Analyse des th√®mes: {processed_articles}/{total_articles} articles utilis√©s pour la p√©riode de {days} jours")
    
    # Ajouter les stats de sentiment dans les d√©tails
    for axe, theme_dict in theme_articles.items():
        for theme, articles in theme_dict.items():
            sentiment_stats = compute_sentiment_distribution(articles)
            if theme in themes_details[axe]:
                themes_details[axe][theme]["sentiment_distribution"] = sentiment_stats
    
    # Obtenir les 5 th√®mes principaux pour chaque axe avec leurs d√©tails
    top_themes_with_details = {}
    for axe in themes_counter:
        top_themes = themes_counter[axe].most_common(5)
        top_themes_with_details[axe] = {}
        for theme, count in top_themes:
            top_themes_with_details[axe][theme] = themes_details[axe].get(theme, {"count": count, "examples": []})
    
    return top_themes_with_details

def build_theme_summary(theme_name, theme_data):
    """G√©n√®re automatiquement un r√©sum√© texte simple pour un th√®me"""
    count = theme_data.get("count", 0)
    examples = theme_data.get("examples", [])
    keywords = theme_data.get("keywords", {})
    sentiment_distribution = theme_data.get("sentiment_distribution", {})

    keywords_list = sorted(keywords.items(), key=lambda x: x[1]["count"], reverse=True)
    keywords_str = ", ".join([f"{kw} ({info['count']})" for kw, info in keywords_list[:5]])

    if not examples:
        return f"Le th√®me '{theme_name}' est apparu dans {count} articles r√©cemment."

    sentiment_info = ""
    if sentiment_distribution:
        pos = sentiment_distribution.get("positive", 0)
        neg = sentiment_distribution.get("negative", 0)
        if pos > neg + 20:
            sentiment_info = f" Le sentiment est majoritairement positif ({pos}% vs {neg}% n√©gatif)."
        elif neg > pos + 20:
            sentiment_info = f" Le sentiment est majoritairement n√©gatif ({neg}% vs {pos}% positif)."
        else:
            sentiment_info = f" Le sentiment est mitig√© ({pos}% positif, {neg}% n√©gatif)."

    return (
        f"üì∞ Le th√®me **{theme_name}** a √©t√© d√©tect√© dans **{count} articles** "
        f"au cours de la p√©riode, principalement √† travers des sujets comme : {keywords_str}."
        f"{sentiment_info} "
        f"Exemples d'articles : ¬´ {examples[0]} ¬ª"
        + (f", ¬´ {examples[1]} ¬ª" if len(examples) > 1 else "")
        + (f", ¬´ {examples[2]} ¬ª" if len(examples) > 2 else "") + "."
    )

def process_news_data(news_sources):
    """Traite et formate les actualit√©s FMP pour correspondre au format TradePulse"""
    # Initialiser la structure pour tous les pays/r√©gions possibles
    formatted_data = {
        "lastUpdated": datetime.now().isoformat()
    }
    
    for country in CONFIG["output_limits"].keys():
        formatted_data[country] = []
    
    # Liste de tous les articles avant la s√©paration par pays
    all_articles = []
    
    # Traiter chaque source d'actualit√©s
    for source_type, articles in news_sources.items():
        for article in articles:
            # Normaliser l'article
            normalized = normalize_article(article, source_type)
            
            # V√©rifier si le titre est suffisamment long pour √™tre pertinent
            if len(normalized["title"]) < 10:
                continue
                
            # V√©rifier si le contenu est suffisamment d√©taill√©
            if len(normalized["text"]) < 50:
                continue
            
            # D√©terminer la cat√©gorie et le pays
            category = determine_category(normalized, source_type)
            country = determine_country(normalized)
            impact = determine_impact(normalized)
            
            # Donn√©es essentielles
            news_item = {
                "title": normalized["title"],
                "content": normalized["text"],
                "source": normalized["site"],
                "rawDate": normalized["publishedDate"],  # Conserver la date brute pour le filtrage
                "date": format_date(normalized["publishedDate"]),
                "time": format_time(normalized["publishedDate"]),
                "category": category,
                "impact": impact,
                "country": country,
                "url": normalized.get("url", ""),
                "themes": extract_themes(normalized),
                "source_type": source_type
            }
            
            # Calculer le score d'importance
            news_item["importance_score"] = compute_importance_score(news_item, source_type)
            
            # Ajouter √† la liste globale
            all_articles.append(news_item)
    
    # Supprimer les doublons
    all_articles = remove_duplicates(all_articles)
    
    # Trier par score d'importance
    all_articles.sort(key=lambda x: x["importance_score"], reverse=True)
    
    # R√©partir par pays
    articles_by_country = {}
    for article in all_articles:
        country = article["country"]
        if country not in articles_by_country:
            articles_by_country[country] = []
        articles_by_country[country].append(article)
    
    # Calculer les limites appropri√©es pour chaque pays
    adjusted_limits = calculate_output_limits(articles_by_country, CONFIG["max_total_articles"])
    
    # Appliquer les limites par pays
    for country, articles in articles_by_country.items():
        limit = adjusted_limits.get(country, 10)
        # Si le pays existe dans formatted_data
        if country in formatted_data:
            formatted_data[country] = articles[:limit]
        else:
            # Sinon, ajouter au global
            if "global" not in formatted_data:
                formatted_data["global"] = []
            formatted_data["global"].extend(articles[:limit])
            logger.info(f"Pays {country} non g√©r√©, {len(articles[:limit])} articles ajout√©s √† 'global'")
    
    # Statistiques sur les donn√©es
    total_articles = sum(len(articles) for articles in formatted_data.values() if isinstance(articles, list))
    logger.info(f"Total des articles trait√©s et format√©s: {total_articles}")
    
    return formatted_data

def process_events_data(earnings, economic):
    """Traite et formate les donn√©es d'√©v√©nements"""
    events = []
    
    # Traiter le calendrier √©conomique
    for eco_event in economic:
        # Ajouter l'impact et le score
        impact = determine_event_impact(eco_event)
        score = calculate_event_score(eco_event)
        
        event = {
            "title": eco_event.get("event", ""),
            "date": format_date(eco_event.get("date", "")),
            "time": eco_event.get("time", "09:00"),
            "type": "economic",
            "importance": impact,
            "score": score
        }
        events.append(event)
    
    # Traiter le calendrier des r√©sultats
    for earning in earnings:
        # Ne garder que les r√©sultats avec des pr√©visions
        if earning.get("epsEstimated"):
            # Cr√©er un faux √©v√©nement pour le calcul du score
            temp_event = {
                "event": f"Earnings {earning.get('symbol')}",
                "type": "earnings",
                "title": f"R√©sultats {earning.get('symbol')} - Pr√©vision: {earning.get('epsEstimated')}$ par action"
            }
            
            impact = "medium"  # Par d√©faut pour les r√©sultats
            score = calculate_event_score(temp_event)
            
            event = {
                "title": f"R√©sultats {earning.get('symbol')} - Pr√©vision: {earning.get('epsEstimated')}$ par action",
                "date": format_date(earning.get("date", "")),
                "time": "16:30",  # Heure typique pour les annonces de r√©sultats
                "type": "earnings",
                "importance": impact,
                "score": score
            }
            events.append(event)
    
    # Trier les √©v√©nements par score puis par date
    events.sort(key=lambda x: (x["score"], x["date"]), reverse=True)
    
    # Limiter √† 15 √©v√©nements maximum
    return events[:15]

def process_ipos_data(ipos):
    """Formate les donn√©es d'IPO pour affichage"""
    formatted_ipos = []
    for ipo in ipos:
        try:
            formatted_ipos.append({
                "title": f"IPO: {ipo.get('company')} ({ipo.get('symbol')})",
                "date": format_date(ipo.get("date")),
                "time": "09:00",
                "type": "ipo",
                "importance": "medium",
                "score": 5,
                "exchange": ipo.get("exchange", ""),
                "priceRange": ipo.get("priceRange", ""),
                "marketCap": ipo.get("marketCap", ""),
                "status": ipo.get("actions", "Expected")
            })
        except Exception as e:
            logger.warning(f"Erreur lors du traitement d'une IPO: {str(e)}")
    return formatted_ipos

def process_ma_data(ma_list):
    """Formate les donn√©es de fusions/acquisitions"""
    formatted_ma = []
    for ma in ma_list:
        try:
            formatted_ma.append({
                "title": f"M&A: {ma.get('companyName')} acquiert {ma.get('targetedCompanyName')}",
                "date": format_date(ma.get("transactionDate")),
                "time": "10:00",
                "type": "m&a",
                "importance": "medium",
                "score": 6,
                "source": ma.get("link", ""),
                "symbol": ma.get("symbol", ""),
                "targetedSymbol": ma.get("targetedSymbol", "")
            })
        except Exception as e:
            logger.warning(f"Erreur lors du traitement M&A: {str(e)}")
    return formatted_ma

def update_news_json_file(news_data, events):
    """Met √† jour le fichier news.json avec les donn√©es format√©es"""
    try:
        # Cr√©er une copie pour ne pas modifier l'original
        output_data = {k: v for k, v in news_data.items()}
        output_data["events"] = events
        
        # Cr√©er le dossier data s'il n'existe pas
        os.makedirs(os.path.dirname(NEWS_JSON_PATH), exist_ok=True)
        
        with open(NEWS_JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"‚úÖ Fichier news.json mis √† jour avec succ√®s")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la mise √† jour du fichier: {str(e)}")
        return False

def generate_themes_json(news_data):
    """G√©n√®re un fichier JSON avec les th√®mes dominants sur diff√©rentes p√©riodes"""
    
    # D√©finir les p√©riodes d'analyse
    periods = {
        "weekly": 7,
        "monthly": 30,
        "quarterly": 90
    }
    
    # Extraire les th√®mes dominants pour chaque p√©riode
    themes_data = {
        period: extract_top_themes(news_data, days=days) 
        for period, days in periods.items()
    }
    
    # Ajouter un r√©sum√© automatique GPT-like √† chaque th√®me
    for period, axes in themes_data.items():
        for axe, themes in axes.items():
            for theme_name, theme_data in themes.items():
                summary = build_theme_summary(theme_name, theme_data)
                themes_data[period][axe][theme_name]["gpt_summary"] = summary
    
    # Ajouter des m√©tadonn√©es
    themes_output = {
        "themes": themes_data,
        "lastUpdated": datetime.now().isoformat(),
        "analysisCount": sum(len(articles) for articles in news_data.values() if isinstance(articles, list))
    }
    
    # Cr√©er le dossier data s'il n'existe pas
    os.makedirs(os.path.dirname(THEMES_JSON_PATH), exist_ok=True)
    
    # √âcrire dans le fichier
    try:
        with open(THEMES_JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(themes_output, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ Fichier themes.json mis √† jour avec succ√®s")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la mise √† jour du fichier themes.json: {str(e)}")
        return False

def main():
    """Fonction principale d'ex√©cution"""
    try:
        # 0. Lire les donn√©es existantes pour fallback
        existing_data = read_existing_news()
        
        # 1. R√©cup√©rer les diff√©rentes actualit√©s (avec la nouvelle approche par p√©riode)
        general_news = get_general_news()
        fmp_articles = get_fmp_articles()
        stock_news = get_stock_news()
        crypto_news = get_crypto_news()
        press_releases = get_press_releases()
        
        # 2. R√©cup√©rer les √©v√©nements
        earnings = get_earnings_calendar()
        economic = get_economic_calendar()
        
        # 2.b R√©cup√©rer les IPOs et M&A
        ipos = get_ipos_calendar()
        mergers = get_mergers_acquisitions()
        
        # 3. Organiser les sources d'actualit√©s
        news_sources = {
            "general_news": general_news,
            "fmp_articles": fmp_articles,
            "stock_news": stock_news,
            "crypto_news": crypto_news,
            "press_releases": press_releases
        }
        
        # Compter le nombre total d'actualit√©s
        total_news = sum(len(articles) for articles in news_sources.values())
        logger.info(f"Total des actualit√©s r√©cup√©r√©es: {total_news}")
        
        # V√©rifier si nous avons des donn√©es
        if total_news == 0:
            logger.warning("Aucune actualit√© r√©cup√©r√©e, utilisation des donn√©es existantes")
            if existing_data:
                return True
        
        # 4. Traiter et formater les donn√©es avec le nouveau syst√®me de scoring
        news_data = process_news_data(news_sources)
        events = process_events_data(earnings, economic)
        
        # 4.b Traiter les donn√©es IPO et M&A
        ipos_events = process_ipos_data(ipos)
        ma_events = process_ma_data(mergers)
        
        # Fusionner avec les autres √©v√©nements
        events.extend(ipos_events)
        events.extend(ma_events)
        
        # 5. Mettre √† jour le fichier JSON des actualit√©s
        success_news = update_news_json_file(news_data, events)
        
        # 6. G√©n√©rer le fichier des th√®mes dominants
        success_themes = generate_themes_json(news_data)
        
        # 7. Afficher les th√®mes dominants sur 30 jours (pour le log)
        top_themes = extract_top_themes(news_data, days=30)
        logger.info("üéØ Th√®mes dominants sur 30 jours:")
        for axe, themes in top_themes.items():
            logger.info(f"  {axe.capitalize()}:")
            for theme, details in themes.items():
                logger.info(f"    {theme} ({details['count']})")
                # Afficher la distribution des sentiments si disponible
                if "sentiment_distribution" in details:
                    sentiment = details["sentiment_distribution"]
                    logger.info(f"      Sentiment: {sentiment['positive']}% positif, {sentiment['negative']}% n√©gatif, {sentiment['neutral']}% neutre")
                if "keywords" in details and details["keywords"]:
                    for keyword, kw_details in details["keywords"].items():
                        logger.info(f"      - {keyword} ({kw_details['count']})")
        
        return success_news and success_themes
    except Exception as e:
        logger.error(f"‚ùå Erreur dans l'ex√©cution du script: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)
