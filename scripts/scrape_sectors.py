#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des donn√©es d'indices sectoriels depuis:
- https://investir.lesechos.fr/cours/indices/sectoriels-stoxx-europe-600 (TOUS les indices STOXX Europe 600)
- https://www.boursorama.com/bourse/indices/internationaux (indices NASDAQ US sectoriels sp√©cifiques)
"""

import os
import json
import sys
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import logging
import time
import re
import random

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "sources": [
        {
            "name": "Les Echos - STOXX Europe 600",
            "url": "https://investir.lesechos.fr/cours/indices/sectoriels-stoxx-europe-600",
            "type": "europe"
        },
        {
            "name": "Boursorama - Secteurs US",
            "url": "https://www.boursorama.com/bourse/indices/internationaux",
            "type": "us"
        }
    ],
    "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "sectors.json"),
    # Structure des cat√©gories sectorielles
    "categories": {
        "energy": ["√©nergie", "energy", "oil", "gaz", "p√©trole", "oil & gas"],
        "materials": ["mat√©riaux", "materials", "basic", "basic resources", "basic matls", "chimie", "chemicals", "construction & materials"],
        "industrials": ["industrials", "industrie", "industrial goods", "industrial goods & services", "aerospace"],
        "consumer-discretionary": ["consommation discr√©tionnaire", "consumer discretionary", "luxury", "retail", "auto", "automobiles", "auto & parts"],
        "consumer-staples": ["consommation de base", "consumer staples", "food", "beverage", "food & beverage"],
        "healthcare": ["sant√©", "health", "health care", "healthcare", "pharma", "medical"],
        "financials": ["finance", "financial", "banks", "insurance", "banques", "assurance", "financial services", "financial svcs"],
        "information-technology": ["technologie", "technology", "it", "software", "hardware", "tech"],
        "communication-services": ["communication", "telecom", "telecommunications", "media"],
        "utilities": ["services publics", "utilities", "√©lectricit√©", "eau", "gas"],
        "real-estate": ["immobilier", "real estate", "reits"]
    },
    # Indices NASDAQ US sp√©cifiques √† scraper
    "target_indices": [
        "NASDAQ US Health Care Large Mid Cap NTR Index",
        "NASDAQ US Financial Svcs Large Mid Cap NTR Index",
        "NASDAQ US Basic Matls Large Mid Cap NTR Index",
        "NASDAQ US Oil & Gas Producers Large Mid Cap Index",
        "NASDAQ US Tech Large Mid Cap Index",
        "NASDAQ US Auto & Parts Large Mid Cap Index",
        "NASDAQ US Telecom Large Mid Cap Index"
    ],
    # Indices STOXX Europe 600 √† scraper
    "target_stoxx_indices": [
        "Stoxx Europe 600 Automobiles",
        "Stoxx Europe 600 Basic Resources",
        "Stoxx Europe 600 Chemicals",
        "Stoxx Europe 600 Construction & Materials",
        "Stoxx Europe 600 Financial Services",
        "Stoxx Europe 600 Food & Beverage",
        "Stoxx Europe 600 Health Care",
        "Stoxx Europe 600 Industrial Goods & Services",
        "Stoxx Europe 600 Insurance",
        "Stoxx Europe 600 Media",
        "Stoxx Europe 600 Oil & Gas",
        "Stoxx Europe 600 Technology",
        "Stoxx Europe 600 Telecommunications",
        "Stoxx Europe 600 Utilities"
    ],
    # Configuration pour Playwright
    "use_playwright": True,  # Activer ou d√©sactiver l'utilisation de Playwright
    "playwright": {
        "headless": True,      # Mode sans interface graphique
        "timeout": 60000,      # Timeout en millisecondes
        "wait_for": 5000       # Temps d'attente suppl√©mentaire en millisecondes
    }
}

# Structure pour les donn√©es
SECTOR_DATA = {
    "sectors": {
        "energy": [],
        "materials": [],
        "industrials": [],
        "consumer-discretionary": [],
        "consumer-staples": [],
        "healthcare": [],
        "financials": [],
        "information-technology": [],
        "communication-services": [],
        "utilities": [],
        "real-estate": []
    },
    "top_performers": {
        "daily": {
            "best": [],
            "worst": []
        },
        "ytd": {
            "best": [],
            "worst": []
        }
    },
    "meta": {
        "sources": ["Les Echos", "Boursorama"],
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "count": 0,
        "lastUpdated": datetime.now(timezone.utc).isoformat()
    }
}

# Liste pour stocker tous les secteurs avant le filtrage (pour calculer les Top 3)
ALL_SECTORS = []

def get_headers():
    """Cr√©e des en-t√™tes HTTP al√©atoires pour √©viter la d√©tection de bot"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
    ]
    
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "cross-site",
        "DNT": "1"
    }

def determine_category(sector_name):
    """D√©termine la cat√©gorie d'un secteur en fonction de son nom"""
    sector_name_lower = sector_name.lower()
    
    # Mappings directs pour les secteurs STOXX Europe 600
    stoxx_mappings = {
        "automobiles": "consumer-discretionary",
        "basic resources": "materials",
        "chemicals": "materials",
        "construction & materials": "materials",
        "financial services": "financials",
        "food & beverage": "consumer-staples",
        "health care": "healthcare",
        "industrial goods & services": "industrials",
        "insurance": "financials",
        "media": "communication-services",
        "oil & gas": "energy",
        "technology": "information-technology",
        "telecommunications": "communication-services",
        "utilities": "utilities"
    }
    
    # Si c'est un indice STOXX Europe 600, extraire le secteur et v√©rifier le mapping direct
    if "stoxx europe 600" in sector_name_lower:
        for sector, category in stoxx_mappings.items():
            if sector.lower() in sector_name_lower:
                return category
    
    # Mappings sp√©cifiques pour les indices NASDAQ US cibl√©s
    nasdaq_mappings = {
        "health care": "healthcare",
        "financial": "financials",
        "basic matls": "materials",
        "oil & gas": "energy",
        "tech": "information-technology",
        "auto & parts": "consumer-discretionary",
        "telecom": "communication-services"
    }
    
    # V√©rifier les mappings NASDAQ
    for keyword, category in nasdaq_mappings.items():
        if keyword.lower() in sector_name_lower:
            return category
    
    # Sinon, v√©rifier les cat√©gories g√©n√©rales
    for category, keywords in CONFIG["categories"].items():
        for keyword in keywords:
            if keyword.lower() in sector_name_lower:
                return category
    
    # Cat√©gorie par d√©faut si aucune correspondance n'est trouv√©e
    return "other"

def extract_lesechos_data_with_playwright():
    """Extrait les donn√©es Les Echos avec Playwright pour g√©rer le JavaScript"""
    try:
        logger.info("üöÄ Utilisation de Playwright pour le scraping Les Echos (avec support JavaScript)")
        
        try:
            from playwright.sync_api import sync_playwright
        except ImportError:
            logger.error("‚ùå Playwright n'est pas install√©. Veuillez l'installer avec 'pip install playwright' et 'playwright install'")
            raise ImportError("Playwright est requis pour cette fonctionnalit√©")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=CONFIG["playwright"]["headless"])
            
            try:
                page = browser.new_page()
                url = CONFIG["sources"][0]["url"]
                
                logger.info(f"üåê Chargement de la page {url} avec Playwright...")
                page.goto(url, timeout=CONFIG["playwright"]["timeout"])
                
                # Attendre que le tableau soit charg√©
                logger.info("‚è≥ Attente que le contenu JavaScript soit charg√©...")
                page.wait_for_selector('table', timeout=30000)
                
                # Attendre un peu plus pour s'assurer que tout est charg√©
                page.wait_for_timeout(CONFIG["playwright"]["wait_for"])
                
                # Capturer le HTML complet
                html = page.content()
                
                # G√©n√©rer un fichier HTML de d√©bogage
                debug_dir = os.path.dirname(CONFIG["output_path"])
                if not os.path.exists(debug_dir):
                    os.makedirs(debug_dir, exist_ok=True)
                debug_file_path = os.path.join(debug_dir, "debug_lesechos_playwright.html")
                with open(debug_file_path, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üìÑ HTML Playwright sauvegard√© pour d√©bogage dans {debug_file_path}")
                
                # Extraire les donn√©es du HTML
                sectors = extract_lesechos_data(html)
                
                return sectors
                
            finally:
                browser.close()
                
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'extraction Playwright: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Fallback vers l'extraction classique
        logger.warning("‚ö†Ô∏è √âchec de l'extraction Playwright - utilisation des donn√©es statiques")
        return []

def extract_lesechos_data(html):
    """Extraire tous les indices sectoriels STOXX Europe 600 de Les Echos"""
    sectors = []
    soup = BeautifulSoup(html, 'html.parser')
    
    # V√©rification de base pour s'assurer qu'on a la bonne page
    page_title = soup.find('title')
    if page_title and 'stoxx' not in page_title.text.lower() and 'sectoriels' not in page_title.text.lower():
        logger.warning("‚ùå La page Les Echos ne semble pas √™tre celle des indices sectoriels!")
        
    # G√©n√©rer un fichier HTML de d√©bogage
    debug_dir = os.path.dirname(CONFIG["output_path"])
    if not os.path.exists(debug_dir):
        os.makedirs(debug_dir, exist_ok=True)
    debug_file_path = os.path.join(debug_dir, "debug_lesechos.html")
    with open(debug_file_path, 'w', encoding='utf-8') as f:
        f.write(html)
    logger.info(f"HTML sauvegard√© pour d√©bogage dans {debug_file_path}")
    
    # M√âTHODE 1: Recherche directe par le contenu - plus fiable
    logger.info("üîç Tentative d'extraction par recherche directe des indices STOXX...")
    
    # Rechercher tous les √©l√©ments qui pourraient contenir des indices STOXX
    stoxx_elements = []
    
    # 1. Chercher par texte contenant "Stoxx Europe 600"
    for element in soup.find_all(string=re.compile('Stoxx Europe 600', re.IGNORECASE)):
        parent = element.parent
        if parent and parent.name != 'title' and parent.name != 'script':
            stoxx_elements.append(parent)
            row = parent.find_parent('tr')
            if row:
                stoxx_elements.append(row)
    
    logger.info(f"Trouv√© {len(stoxx_elements)} √©l√©ments contenant 'Stoxx Europe 600'")
    
    # Extraire directement des lignes de tableau contenant des secteurs STOXX
    for i, element in enumerate(stoxx_elements):
        if element.name == 'tr':
            try:
                # C'est une ligne de tableau, extraire les donn√©es
                cells = element.find_all(['td', 'th'])
                if len(cells) < 3:  # Au minimum, on s'attend √† voir libell√©, cours, var
                    continue
                
                # Supposer un ordre de colonnes courant
                name_cell = cells[0]  # Premi√®re colonne = Libell√©/Nom
                name = name_cell.get_text(strip=True)
                
                if not "stoxx europe 600" in name.lower():
                    continue
                
                # Log pour d√©boguer
                logger.info(f"üéØ Ligne de tableau STOXX trouv√©e: {name}")
                
                # Extraire les donn√©es des autres colonnes
                cours = "0"
                var = "0"
                var_janv = "0"
                
                # Pour chaque cellule, d√©terminer ce qu'elle contient
                for i, cell in enumerate(cells):
                    cell_text = cell.get_text(strip=True)
                    
                    # Colonne 1 normalement = cours/valeur
                    if i == 1 and re.match(r'^[\d\s.,]+$', cell_text):
                        cours = cell_text
                    
                    # Var% a g√©n√©ralement un % et parfois un - (n√©gatif)
                    elif '%' in cell_text and i > 0 and i < 4:
                        # Si c'est la premi√®re colonne pourcentage qu'on rencontre, c'est var quotidien
                        if var == "0":
                            var = cell_text
                        # Sinon, c'est probablement var depuis janvier
                        else:
                            var_janv = cell_text
                
                logger.info(f"üìä Donn√©es extraites pour {name}: Cours={cours}, Var={var}, Var1erJanv={var_janv}")
                
                # D√©terminer la tendance
                trend = "down" if '-' in var else "up"
                
                # D√©terminer la cat√©gorie sectorielle
                category = determine_category(name)
                
                # Cr√©er l'objet secteur
                sector = {
                    "name": name,
                    "value": cours,
                    "change": var,
                    "changePercent": var,
                    "ytdChange": var_janv,
                    "trend": trend,
                    "category": category,
                    "source": "Les Echos",
                    "region": "Europe"
                }
                
                sectors.append(sector)
                ALL_SECTORS.append(sector)
                logger.info(f"‚úÖ Indice STOXX Europe 600 ajout√©: {name} (Cat√©gorie: {category})")
                
            except Exception as e:
                logger.error(f"Erreur lors du traitement d'un √©l√©ment STOXX: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
    
    # M√âTHODE 2: Recherche classique par structure de tableau (si la m√©thode 1 n'a pas fonctionn√©)
    if not sectors:
        logger.info("‚ö†Ô∏è Aucun secteur trouv√© par recherche directe, tentative par structure de tableau...")
        # Recherche du tableau des indices sectoriels
        tables = soup.find_all('table')
        
        logger.info(f"Nombre de tableaux trouv√©s: {len(tables)}")
        
        for i, table in enumerate(tables):
            logger.info(f"Analyse du tableau #{i+1}")
            
            # Regarder toutes les lignes pour chercher des indices STOXX Europe 600
            rows = table.find_all('tr')
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) < 3:  # Au minimum, on s'attend √† voir libell√©, cours, var
                    continue
                
                try:
                    # R√©cup√©rer le texte de la premi√®re cellule
                    first_cell = cells[0].get_text(strip=True)
                    
                    # Si ce n'est pas un indice STOXX Europe 600, passer √† la suivante
                    if not first_cell.lower().startswith('stoxx europe 600'):
                        continue
                    
                    logger.info(f"üéØ Indice trouv√© dans le tableau #{i+1}: {first_cell}")
                    
                    # C'est un indice STOXX Europe 600 - extraire les donn√©es
                    name = first_cell
                    
                    # Pour le reste, essayer de d√©tecter intelligemment
                    cours = "0"
                    var = "0"
                    var_janv = "0"
                    
                    # Pour chaque cellule, d√©terminer ce qu'elle contient
                    for i, cell in enumerate(cells):
                        if i == 0:  # D√©j√† trait√© (nom)
                            continue
                            
                        cell_text = cell.get_text(strip=True)
                        
                        # D√©tecter un cours (chiffres avec √©ventuellement un s√©parateur)
                        if re.match(r'^[\d\s.,]+$', cell_text) and i == 1:
                            cours = cell_text
                        
                        # D√©tecter un pourcentage
                        elif '%' in cell_text:
                            # Si c'est la premi√®re colonne pourcentage qu'on rencontre, c'est var quotidien
                            if var == "0":
                                var = cell_text
                            # Sinon, c'est probablement var depuis janvier
                            elif var_janv == "0":
                                var_janv = cell_text
                    
                    logger.info(f"üìä Donn√©es extraites pour {name}: Cours={cours}, Var={var}, Var1erJanv={var_janv}")
                    
                    # D√©terminer la tendance
                    trend = "down" if '-' in var else "up"
                    
                    # D√©terminer la cat√©gorie sectorielle
                    category = determine_category(name)
                    
                    # Cr√©er l'objet secteur
                    sector = {
                        "name": name,
                        "value": cours,
                        "change": var,
                        "changePercent": var,
                        "ytdChange": var_janv,
                        "trend": trend,
                        "category": category,
                        "source": "Les Echos",
                        "region": "Europe"
                    }
                    
                    sectors.append(sector)
                    ALL_SECTORS.append(sector)
                    logger.info(f"‚úÖ Indice STOXX Europe 600 ajout√© par m√©thode 2: {name} (Cat√©gorie: {category})")
                
                except Exception as e:
                    logger.error(f"Erreur lors du traitement d'une ligne: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
    
    # M√âTHODE 3: Extraction manuelle des indices √† partir du fichier de la capture d'√©cran fournie
    if not sectors:
        logger.info("‚ö†Ô∏è M√©thodes 1 & 2 ont √©chou√© - utilisation des donn√©es STOXX Europe 600 statiques")
        
        # Donn√©es extraites de la capture d'√©cran
        static_data = [
            {"name": "Stoxx Europe 600 Automobiles", "value": "565,07", "var": "-2,25 %", "var_janv": "+4,83 %"},
            {"name": "Stoxx Europe 600 Basic Resources", "value": "540,16", "var": "-0,96 %", "var_janv": "+4,92 %"},
            {"name": "Stoxx Europe 600 Chemicals", "value": "1287,87", "var": "-0,99 %", "var_janv": "+9,45 %"},
            {"name": "Stoxx Europe 600 Construction & Materials", "value": "791,68", "var": "-0,58 %", "var_janv": "+14,10 %"},
            {"name": "Stoxx Europe 600 Financial Services", "value": "879,38", "var": "+0,10 %", "var_janv": "+6,56 %"},
            {"name": "Stoxx Europe 600 Food & Beverage", "value": "681,10", "var": "+0,80 %", "var_janv": "+6,58 %"},
            {"name": "Stoxx Europe 600 Health Care", "value": "1141,97", "var": "-0,05 %", "var_janv": "+4,68 %"},
            {"name": "Stoxx Europe 600 Industrial Goods & Services", "value": "999,89", "var": "-0,99 %", "var_janv": "+14,65 %"},
            {"name": "Stoxx Europe 600 Insurance", "value": "471,54", "var": "-0,42 %", "var_janv": "+15,53 %"},
            {"name": "Stoxx Europe 600 Media", "value": "455,94", "var": "+0,26 %", "var_janv": "-3,21 %"},
            {"name": "Stoxx Europe 600 Oil & Gas", "value": "370,31", "var": "+0,09 %", "var_janv": "+10,56 %"},
            {"name": "Stoxx Europe 600 Technology", "value": "837,31", "var": "-0,55 %", "var_janv": "+3,81 %"},
            {"name": "Stoxx Europe 600 Telecommunications", "value": "255,24", "var": "-0,12 %", "var_janv": "+11,81 %"},
            {"name": "Stoxx Europe 600 Utilities", "value": "406,12", "var": "+0,64 %", "var_janv": "+5,46 %"}
        ]
        
        for item in static_data:
            # D√©terminer la tendance
            trend = "down" if '-' in item["var"] else "up"
            
            # D√©terminer la cat√©gorie
            category = determine_category(item["name"])
            
            # Cr√©er l'objet secteur
            sector = {
                "name": item["name"],
                "value": item["value"],
                "change": item["var"],
                "changePercent": item["var"],
                "ytdChange": item["var_janv"],
                "trend": trend,
                "category": category,
                "source": "Les Echos (statique)",
                "region": "Europe"
            }
            
            sectors.append(sector)
            ALL_SECTORS.append(sector)
            logger.info(f"‚ö†Ô∏è Ajout d'indice STOXX statique: {item['name']}")
        
        logger.warning("‚ö†Ô∏è Donn√©es STOXX Europe 600 statiques utilis√©es - regardez le fichier HTML de d√©bogage pour comprendre l'√©chec")
    
    # V√©rification du nombre de secteurs trouv√©s
    if sectors:
        logger.info(f"‚úÖ {len(sectors)} indices STOXX Europe 600 trouv√©s")
    else:
        logger.warning("‚ö†Ô∏è Aucun indice STOXX Europe 600 trouv√©!")
    
    return sectors

def extract_boursorama_data(html):
    """Extraire les donn√©es de Boursorama pour les indices NASDAQ US sectoriels sp√©cifiques"""
    sectors = []
    soup = BeautifulSoup(html, 'html.parser')
    
    # Rechercher des tableaux contenant des indices
    tables = soup.find_all('table')
    
    # Liste pour suivre les indices cibles trouv√©s
    target_indices_found = []
    
    # Trouver les onglets de cat√©gories
    sector_tab = None
    tabs = soup.find_all('a', class_='c-tab')
    for tab in tabs:
        if tab.text.strip().lower() == 'secteurs':
            sector_tab = tab
            break
    
    logger.info(f"Tab secteur trouv√©: {sector_tab is not None}")
    
    # Si l'onglet Secteurs est trouv√©, chercher son contenu
    if sector_tab and sector_tab.get('id'):
        tab_id = sector_tab.get('id')
        tab_content_id = tab_id.replace('-tab', '-content')
        sector_content = soup.find('div', id=tab_content_id)
        
        if sector_content:
            tables = sector_content.find_all('table')
            logger.info(f"Nombre de tableaux trouv√©s dans l'onglet Secteurs: {len(tables)}")
    
    # Parcourir tous les tableaux pour trouver les indices NASDAQ
    for table in tables:
        # Analyser les en-t√™tes pour comprendre la structure du tableau
        header = table.find('thead')
        if not header:
            continue
            
        # Trouver les index des colonnes importantes
        header_cells = header.find_all('th')
        header_texts = [cell.text.strip().lower() for cell in header_cells]
        
        logger.info(f"En-t√™tes trouv√©s: {header_texts}")
        
        # Trouver les indices des colonnes
        libelle_idx = -1
        dernier_idx = -1
        var_idx = -1
        veille_idx = -1
        var_janv_idx = -1
        
        for i, text in enumerate(header_texts):
            if 'libell√©' in text or 'nom' in text:
                libelle_idx = i
            elif 'dernier' in text or 'cours' in text:
                dernier_idx = i
            elif 'var.' in text and not 'janv' in text and not 'veille' in text:
                var_idx = i
            elif 'veille' in text:
                veille_idx = i
            elif 'var/1janv' in text or '1er' in text or 'janv' in text:
                var_janv_idx = i
        
        logger.info(f"Indices de colonnes: libell√©={libelle_idx}, dernier={dernier_idx}, var={var_idx}, veille={veille_idx}, var_janv={var_janv_idx}")
        
        # Si on ne trouve pas les colonnes essentielles, passer au tableau suivant
        if libelle_idx == -1 or dernier_idx == -1 or var_idx == -1:
            continue
            
        # Parcourir les lignes du tableau
        rows = table.find('tbody').find_all('tr')
        for row in rows:
            cells = row.find_all('td')
            if len(cells) < max(libelle_idx, dernier_idx, var_idx, var_janv_idx) + 1:
                continue
                
            try:
                # Extraire le nom de l'indice
                name_cell = cells[libelle_idx]
                name_text = name_cell.text.strip()
                
                # V√©rifier si c'est l'un des indices NASDAQ US cibl√©s
                is_target_index = False
                for target in CONFIG["target_indices"]:
                    # V√©rifier si l'indice cible est pr√©sent dans le nom (en ignorant "Cours" qui peut √™tre ajout√©)
                    clean_name = name_text.replace("Cours ", "")
                    if target.lower() in clean_name.lower():
                        is_target_index = True
                        target_indices_found.append(target)
                        logger.info(f"Indice cible trouv√©: {clean_name}")
                        break
                
                # Si c'est un indice NASDAQ sectoriel US qui nous int√©resse
                if is_target_index or (("NASDAQ US" in name_text or "Nasdaq US" in name_text) and any(keyword in name_text.lower() for keyword in ["health", "financial", "matls", "oil", "tech", "auto", "telecom"])):
                    # Nettoyer le nom (supprimer "Cours" s'il est pr√©sent)
                    clean_name = name_text.replace("Cours ", "")
                    
                    # Extraire les valeurs des diff√©rentes colonnes
                    value = cells[dernier_idx].text.strip() if dernier_idx >= 0 and dernier_idx < len(cells) else ""
                    change_percent = cells[var_idx].text.strip() if var_idx >= 0 and var_idx < len(cells) else ""
                    
                    # Extraire la variation depuis le 1er janvier (si disponible)
                    ytd_change = ""
                    if var_janv_idx >= 0 and var_janv_idx < len(cells):
                        ytd_change = cells[var_janv_idx].text.strip()
                    else:
                        logger.warning(f"Colonne VAR/1JANV introuvable pour {clean_name}. Utilisation d'une valeur vide.")
                    
                    # D√©terminer la tendance
                    trend = "down" if '-' in change_percent else "up"
                    
                    # D√©terminer la cat√©gorie
                    category = determine_category(clean_name)
                    
                    # Cr√©er l'objet secteur
                    sector = {
                        "name": clean_name,
                        "value": value,
                        "change": change_percent,
                        "changePercent": change_percent,
                        "ytdChange": ytd_change,
                        "trend": trend,
                        "category": category,
                        "source": "Boursorama",
                        "region": "US"
                    }
                    
                    sectors.append(sector)
                    ALL_SECTORS.append(sector)
                    logger.info(f"Indice sectoriel US ajout√©: {clean_name} - Cours: {value}, VAR: {change_percent}, YTD: {ytd_change} (Cat√©gorie: {category})")
            
            except Exception as e:
                logger.warning(f"Erreur lors du traitement d'une ligne Boursorama: {str(e)}")
    
    # V√©rifier si tous les indices cibles ont √©t√© trouv√©s
    missing_indices = set(CONFIG["target_indices"]) - set(target_indices_found)
    if missing_indices:
        logger.warning(f"Indices cibles non trouv√©s: {missing_indices}")
    
    logger.info(f"Nombre d'indices sectoriels US extraits de Boursorama: {len(sectors)}")
    return sectors

def classify_sectors(sectors):
    """Classe les secteurs dans les bonnes cat√©gories"""
    # R√©initialiser les cat√©gories
    for category in SECTOR_DATA["sectors"]:
        SECTOR_DATA["sectors"][category] = []
    
    # Classer chaque secteur
    for sector in sectors:
        category = sector["category"]
        if category in SECTOR_DATA["sectors"]:
            SECTOR_DATA["sectors"][category].append(sector)
        else:
            # Cr√©er la cat√©gorie si elle n'existe pas
            SECTOR_DATA["sectors"][category] = [sector]
    
    # Compter le nombre total de secteurs
    SECTOR_DATA["meta"]["count"] = sum(len(sectors) for sectors in SECTOR_DATA["sectors"].values())

def parse_percentage(percent_str):
    """Convertit une cha√Æne de pourcentage en nombre flottant"""
    if not percent_str:
        return 0.0
    
    # Supprimer les caract√®res non num√©riques sauf le point d√©cimal et le signe moins
    # Pour le format fran√ßais: remplacer la virgule par un point et supprimer l'espace avant %
    clean_str = percent_str.replace(',', '.').replace(' %', '%').replace('%', '')
    clean_str = re.sub(r'[^0-9\.\-]', '', clean_str)
    
    try:
        return float(clean_str)
    except ValueError:
        return 0.0

def calculate_top_performers():
    """Calcule les secteurs avec les meilleures et pires performances"""
    logger.info("Calcul des secteurs avec les meilleures/pires performances...")
    
    # Si pas assez de secteurs, ne rien faire
    if len(ALL_SECTORS) < 3:
        logger.warning("Pas assez de secteurs pour calculer les top performers")
        return
    
    # Pr√©parer les secteurs avec des valeurs num√©riques pour les classements
    for sector in ALL_SECTORS:
        sector["_change_value"] = parse_percentage(sector.get("changePercent", "0"))
        sector["_ytd_value"] = parse_percentage(sector.get("ytdChange", "0"))
    
    # Filtrer les secteurs avec des valeurs de pourcentage valides
    valid_sectors = [s for s in ALL_SECTORS if s["_change_value"] != 0 or s["_ytd_value"] != 0]
    
    # Trier par variation quotidienne
    if valid_sectors:
        # Trier et s√©lectionner les 3 meilleurs et les 3 pires
        sorted_daily = sorted(valid_sectors, key=lambda x: x["_change_value"], reverse=True)
        
        # S√©lectionner les 3 meilleurs
        best_daily = sorted_daily[:3]
        # S√©lectionner les 3 pires (en excluant les valeurs √† 0 qui pourraient √™tre des donn√©es manquantes)
        worst_daily = [idx for idx in sorted_daily if idx["_change_value"] != 0]
        worst_daily = sorted(worst_daily, key=lambda x: x["_change_value"])[:3]
        
        # Ajouter aux r√©sultats en supprimant le champ temporaire _change_value
        for idx in best_daily:
            idx_copy = {k: v for k, v in idx.items() if not k.startswith('_')}
            SECTOR_DATA["top_performers"]["daily"]["best"].append(idx_copy)
        
        for idx in worst_daily:
            idx_copy = {k: v for k, v in idx.items() if not k.startswith('_')}
            SECTOR_DATA["top_performers"]["daily"]["worst"].append(idx_copy)
    
    # Trier par variation depuis le d√©but de l'ann√©e
    if valid_sectors:
        # Trier et s√©lectionner les 3 meilleurs et les 3 pires
        sorted_ytd = sorted(valid_sectors, key=lambda x: x["_ytd_value"], reverse=True)
        
        # S√©lectionner les 3 meilleurs
        best_ytd = sorted_ytd[:3]
        # S√©lectionner les 3 pires (en excluant les valeurs √† 0 qui pourraient √™tre des donn√©es manquantes)
        worst_ytd = [idx for idx in sorted_ytd if idx["_ytd_value"] != 0]
        worst_ytd = sorted(worst_ytd, key=lambda x: x["_ytd_value"])[:3]
        
        # Ajouter aux r√©sultats en supprimant le champ temporaire _ytd_value
        for idx in best_ytd:
            idx_copy = {k: v for k, v in idx.items() if not k.startswith('_')}
            SECTOR_DATA["top_performers"]["ytd"]["best"].append(idx_copy)
        
        for idx in worst_ytd:
            idx_copy = {k: v for k, v in idx.items() if not k.startswith('_')}
            SECTOR_DATA["top_performers"]["ytd"]["worst"].append(idx_copy)
    
    logger.info(f"Top performers calcul√©s. Daily: {len(SECTOR_DATA['top_performers']['daily']['best'])} best, {len(SECTOR_DATA['top_performers']['daily']['worst'])} worst. YTD: {len(SECTOR_DATA['top_performers']['ytd']['best'])} best, {len(SECTOR_DATA['top_performers']['ytd']['worst'])} worst.")

def scrape_sectors_data():
    """R√©cup√®re et traite les donn√©es de tous les secteurs"""
    all_sectors = []
    
    # Pour chaque source configur√©e
    for source in CONFIG["sources"]:
        try:
            logger.info(f"R√©cup√©ration des donn√©es depuis {source['name']} ({source['url']})...")
            
            # Si c'est Les Echos et que Playwright est activ√©, utiliser Playwright
            if "lesechos.fr" in source["url"] and CONFIG["use_playwright"]:
                sectors = extract_lesechos_data_with_playwright()
                if sectors:
                    all_sectors.extend(sectors)
                    continue  # Passer √† la source suivante
            
            # Sinon, continuer avec la m√©thode standard
            # R√©cup√©rer le contenu de la page avec d√©lai pour √©viter la d√©tection
            time.sleep(random.uniform(1, 3))
            headers = get_headers()
            logger.info(f"Utilisation du User-Agent: {headers['User-Agent']}")
            
            response = requests.get(source["url"], headers=headers, timeout=30)
            
            if response.status_code != 200:
                logger.warning(f"Erreur {response.status_code} pour {source['name']} - {response.reason}")
                # V√©rifier s'il y a une redirection
                if response.history:
                    logger.warning(f"Redirection d√©tect√©e: {response.url}")
                continue
            
            logger.info(f"R√©ponse HTTP {response.status_code} re√ßue pour {source['name']}")
            
            # V√©rifier le contenu de base
            html = response.text
            if len(html) < 1000:
                logger.warning(f"Contenu suspect (trop court): {len(html)} caract√®res")
            
            # Traiter selon la source
            if "lesechos.fr" in source["url"]:
                sectors = extract_lesechos_data(html)
            elif "boursorama.com" in source["url"]:
                sectors = extract_boursorama_data(html)
            else:
                logger.warning(f"Source non reconnue: {source['name']}")
                continue
                
            all_sectors.extend(sectors)
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement de {source['name']}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
    
    # Classer les secteurs r√©cup√©r√©s
    classify_sectors(all_sectors)
    
    # Calculer les top performers
    calculate_top_performers()
    
    # Mettre √† jour l'horodatage
    SECTOR_DATA["meta"]["lastUpdated"] = datetime.now(timezone.utc).isoformat()
    
    return len(all_sectors) > 0

def save_sector_data():
    """Enregistre les donn√©es sectorielles dans un fichier JSON"""
    try:
        # Cr√©er le dossier data s'il n'existe pas
        data_dir = os.path.dirname(CONFIG["output_path"])
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        
        # √âcrire le fichier JSON
        with open(CONFIG["output_path"], 'w', encoding='utf-8') as f:
            json.dump(SECTOR_DATA, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es sectorielles enregistr√©es dans {CONFIG['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es: {str(e)}")
        return False

def check_existing_data():
    """V√©rifier si un fichier de donn√©es existe d√©j√†"""
    try:
        if os.path.exists(CONFIG["output_path"]):
            logger.info("üìÇ Fichier de donn√©es sectorielles existant trouv√©")
            return True
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification du fichier existant: {str(e)}")
        return False

def main():
    """Point d'entr√©e principal du script"""
    try:
        logger.info("üöÄ D√©marrage du script de scraping des donn√©es sectorielles")
        logger.info(f"Ciblant les indices sectoriels STOXX Europe 600 et ces indices NASDAQ US sp√©cifiques:")
        for idx in CONFIG["target_indices"]:
            logger.info(f"  - {idx}")
        
        # V√©rifier si les donn√©es existent d√©j√†
        has_existing_data = check_existing_data()
        
        # R√©cup√©rer les donn√©es sectorielles
        success = scrape_sectors_data()
        
        # Si l'extraction √©choue mais qu'on a des donn√©es existantes, conserver le fichier
        if not success and has_existing_data:
            logger.info("‚ö†Ô∏è Utilisation des donn√©es existantes car le scraping a √©chou√©")
            sys.exit(0) # Sortie sans erreur pour ne pas faire √©chouer le workflow
        elif not success and not has_existing_data:
            logger.error("‚ùå Aucune donn√©e existante et √©chec du scraping")
            sys.exit(1) # Sortie avec erreur car on n'a pas de donn√©es
        else:
            # Enregistrer les donn√©es
            if save_sector_data():
                logger.info("‚úÖ Traitement des donn√©es sectorielles termin√© avec succ√®s")
                sys.exit(0)
            else:
                logger.error("‚ùå √âchec lors de l'enregistrement des donn√©es")
                sys.exit(1)
            
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Si une erreur se produit mais que le fichier existe d√©j√†, ne pas faire √©chouer le workflow
        if check_existing_data():
            logger.info("‚ö†Ô∏è Une erreur s'est produite mais les donn√©es existantes seront conserv√©es")
            sys.exit(0)
        else:
            sys.exit(1)

if __name__ == "__main__":
    # D√©sactiver l'avertissement sur les v√©rifications SSL d√©sactiv√©es
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # Lancer le programme
    main()
