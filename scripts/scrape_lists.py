#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des donn√©es des actions du NASDAQ Composite depuis Boursorama
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es
Version d√©di√©e pour la page liste.html
"""

import os
import json
import sys
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import re
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "base_url": "https://www.boursorama.com/bourse/actions/cotations/international/",
    "params": {
        "country": "1",  # √âtats-Unis
        "market": "$COMPX",  # NASDAQ Composite
    },
    "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "lists.json"),
    "alphabet": list("ABCDEFGHIJKLMNOPQRSTUVWXYZ")
}

# Structure pour les donn√©es
LIST_DATA = {
    "stocks": [],
    "meta": {
        "source": "Boursorama",
        "url": CONFIG["base_url"],
        "description": "Actions du NASDAQ Composite (√âtats-Unis)",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "count": 0
    }
}

def get_headers():
    """Cr√©e des en-t√™tes HTTP pour √©viter la d√©tection de bot"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/"
    }

def get_stock_list_url(letter, page=1):
    """G√©n√®re l'URL pour obtenir la liste des actions pour une lettre donn√©e"""
    params = {
        "international_quotation_az_filter[country]": CONFIG["params"]["country"],
        "international_quotation_az_filter[market]": CONFIG["params"]["market"],
        "international_quotation_az_filter[letter]": letter,
        "international_quotation_az_filter[filter]": "",
        "page": page
    }
    query_params = "&".join([f"{k}={v}" for k, v in params.items()])
    return f"{CONFIG['base_url']}?{query_params}"

def extract_stock_data(row):
    """Extrait les donn√©es d'une action √† partir d'une ligne de tableau"""
    try:
        cells = row.find_all('td')
        if not cells or len(cells) < 8:
            return None
            
        # R√©cup√©rer le libell√© et le lien
        libelle_cell = cells[0]
        libelle_link = libelle_cell.find('a')
        libelle = libelle_link.text.strip() if libelle_link else ""
        link = libelle_link.get('href') if libelle_link else ""
        
        # R√©cup√©rer le cours et les autres valeurs
        dernier = cells[1].text.strip()
        variation = cells[2].text.strip()
        ouverture = cells[3].text.strip()
        plus_haut = cells[4].text.strip()
        plus_bas = cells[5].text.strip()
        var_ytd = cells[6].text.strip()
        volume = cells[7].text.strip()
        
        # D√©terminer la tendance en fonction de la variation
        trend = "up" if variation and not variation.startswith('-') and variation != "0,00%" else "down"
        if variation == "0,00%":
            trend = "neutral"
            
        # Cr√©er l'objet stock
        stock_data = {
            "symbol": link.split('/')[-1] if link else "",
            "name": libelle,
            "last": dernier,
            "change": variation,
            "open": ouverture,
            "high": plus_haut,
            "low": plus_bas,
            "ytd": var_ytd,
            "volume": volume,
            "trend": trend,
            "link": f"https://www.boursorama.com{link}" if link else ""
        }
        
        return stock_data
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des donn√©es d'une action: {str(e)}")
        return None

def scrape_stock_page(letter, page=1):
    """Scrape une page de la liste des actions"""
    url = get_stock_list_url(letter, page)
    logger.info(f"R√©cup√©ration des donn√©es pour la lettre {letter}, page {page}: {url}")
    
    try:
        response = requests.get(url, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Trouver le tableau des actions
        table = soup.find('table', class_='c-table')
        if not table:
            logger.warning(f"Aucun tableau trouv√© pour la lettre {letter}, page {page}")
            return []
            
        # Trouver toutes les lignes de donn√©es (ignorer l'en-t√™te)
        rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
        
        stocks = []
        for row in rows:
            stock_data = extract_stock_data(row)
            if stock_data:
                stocks.append(stock_data)
                
        logger.info(f"Trouv√© {len(stocks)} actions pour la lettre {letter}, page {page}")
        
        # V√©rifier s'il y a une pagination
        has_next_page = False
        pagination = soup.find('ul', class_='c-pagination')
        if pagination:
            next_button = pagination.find('li', class_='c-pagination__item--next')
            has_next_page = next_button and not next_button.has_attr('disabled')
            
        return stocks, has_next_page
        
    except Exception as e:
        logger.error(f"Erreur lors du scraping de la lettre {letter}, page {page}: {str(e)}")
        return [], False

def scrape_all_stocks():
    """Scrape toutes les actions du NASDAQ Composite"""
    all_stocks = []
    
    for letter in CONFIG["alphabet"]:
        page = 1
        has_next_page = True
        
        while has_next_page:
            stocks, has_next_page = scrape_stock_page(letter, page)
            all_stocks.extend(stocks)
            
            # Si pas de page suivante, sortir de la boucle
            if not has_next_page:
                break
                
            # Passer √† la page suivante
            page += 1
            
            # Attente pour √©viter de surcharger le serveur
            time.sleep(1)
    
    return all_stocks

def save_data():
    """Enregistre les donn√©es dans un fichier JSON"""
    try:
        # Cr√©er le dossier data s'il n'existe pas
        data_dir = os.path.dirname(CONFIG["output_path"])
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        
        # Mettre √† jour le compteur
        LIST_DATA["meta"]["count"] = len(LIST_DATA["stocks"])
        
        # Mettre √† jour l'horodatage
        now = datetime.now(timezone.utc)
        LIST_DATA["meta"]["timestamp"] = now.isoformat()
        
        # Convertir en format compatible avec la structure attendue par liste-script.js
        compatible_data = convert_to_compatible_format(LIST_DATA)
        
        # √âcrire le fichier JSON
        with open(CONFIG["output_path"], 'w', encoding='utf-8') as f:
            json.dump(compatible_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es enregistr√©es dans {CONFIG['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es: {str(e)}")
        return False

def convert_to_compatible_format(data):
    """Convertit les donn√©es au format compatible avec liste-script.js"""
    # Organiser les actions par premi√®re lettre
    stocks_by_letter = {}
    for letter in CONFIG["alphabet"]:
        stocks_by_letter[letter] = []
    
    # Trier les actions par premi√®re lettre
    for stock in data["stocks"]:
        first_letter = stock["name"][0].upper() if stock["name"] else "A"
        if first_letter in stocks_by_letter:
            stocks_by_letter[first_letter].append(stock)
    
    # Cr√©er la structure compatible
    compatible_data = {
        "indices": {
            "a": stocks_by_letter["A"],
            "b": stocks_by_letter["B"],
            "c": stocks_by_letter["C"],
            "d": stocks_by_letter["D"],
            "e": stocks_by_letter["E"],
            "f": stocks_by_letter["F"],
            "g": stocks_by_letter["G"],
            "h": stocks_by_letter["H"],
            "i": stocks_by_letter["I"],
            "j": stocks_by_letter["J"],
            "k": stocks_by_letter["K"],
            "l": stocks_by_letter["L"],
            "m": stocks_by_letter["M"],
            "n": stocks_by_letter["N"],
            "o": stocks_by_letter["O"],
            "p": stocks_by_letter["P"],
            "q": stocks_by_letter["Q"],
            "r": stocks_by_letter["R"],
            "s": stocks_by_letter["S"],
            "t": stocks_by_letter["T"],
            "u": stocks_by_letter["U"],
            "v": stocks_by_letter["V"],
            "w": stocks_by_letter["W"],
            "x": stocks_by_letter["X"],
            "y": stocks_by_letter["Y"],
            "z": stocks_by_letter["Z"]
        },
        "top_performers": {
            "daily": {
                "best": get_top_performers(data["stocks"], "change", reverse=True),
                "worst": get_top_performers(data["stocks"], "change", reverse=False)
            },
            "ytd": {
                "best": get_top_performers(data["stocks"], "ytd", reverse=True),
                "worst": get_top_performers(data["stocks"], "ytd", reverse=False)
            }
        },
        "meta": data["meta"]
    }
    
    return compatible_data

def get_top_performers(stocks, sort_field, reverse=True, limit=10):
    """R√©cup√®re les top/bottom performers bas√©s sur un champ donn√©"""
    def extract_value(value_str):
        if not value_str:
            return 0
        # Nettoyer la cha√Æne pour extraire le nombre
        cleaned = value_str.replace('%', '').replace(',', '.').replace(' ', '')
        try:
            return float(cleaned)
        except:
            return 0
    
    # Trier les actions en fonction du champ
    sorted_stocks = sorted(
        [s for s in stocks if s.get(sort_field)], 
        key=lambda x: extract_value(x.get(sort_field, "0")), 
        reverse=reverse
    )
    
    # Prendre les premiers √©l√©ments
    return sorted_stocks[:limit]

def main():
    """Point d'entr√©e principal"""
    try:
        logger.info("üöÄ D√©marrage du script de collecte des actions du NASDAQ Composite")
        
        # Scraper toutes les actions
        stocks = scrape_all_stocks()
        
        if stocks:
            # Mettre √† jour les donn√©es
            LIST_DATA["stocks"] = stocks
            
            # Enregistrer les donn√©es
            if save_data():
                logger.info(f"‚úÖ Scraping termin√© avec succ√®s: {len(stocks)} actions r√©cup√©r√©es")
                sys.exit(0)
            else:
                logger.error("‚ùå √âchec lors de l'enregistrement des donn√©es")
                sys.exit(1)
        else:
            logger.error("‚ùå Aucune action r√©cup√©r√©e")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
