#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des donn√©es boursi√®res depuis Boursorama
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es
Version am√©lior√©e pour extraire le pays et la variation depuis janvier
"""

import os
import json
import sys
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "source_url": "https://www.boursorama.com/bourse/indices/internationaux",
    "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "markets.json"),
    # Indices prioritaires √† afficher (les autres seront filtr√©s)
    "priority_indices": {
        "europe": [
            "CAC 40", "DAX", "FTSE 100", "EURO STOXX 50", "Euro Stoxx 50", "BEL 20", "IBEX", "FTSE MIB", "AEX", "SMI",
            "Zone Euro", "Belgique", "France", "Allemagne", "Royaume-Uni", "Espagne", "Italie", "Pays-Bas", "Suisse"
        ],
        "us": [
            "DOW JONES", "Dow Jones", "S&P 500", "NASDAQ", "NASDAQ COMPOSITE", "RUSSELL 2000", 
            "DJIA", "US", "√âtats-Unis"
        ],
        "asia": [
            "NIKKEI", "NIKKEI 225", "HANG SENG", "SHANGHAI COMPOSITE", "SHENZHEN COMPONENT", 
            "KOSPI", "Taiwan", "TAIEX", "Japon", "Chine", "Hong Kong", "Cor√©e du Sud"
        ],
        "other": [
            "MSCI WORLD", "MSCI", "MERVAL", "BOVESPA", "Br√©sil", "Argentine", "Chili", "Mexique", "IPC MEXICO"
        ]
    },
    # Structure des r√©gions pour la classification des indices
    "regions": {
        "europe": [
            "CAC", "DAX", "FTSE", "IBEX", "MIB", "AEX", "BEL", "SMI", "ATX",
            "OMX", "OMXS", "ISEQ", "PSI", "ATHEX", "OSEBX", "STOXX", "EURO", "EURONEXT",
            "FRANCE", "ALLEMAGNE", "ROYAUME-UNI", "ESPAGNE", "ITALIE", "PAYS-BAS", "SUISSE", "BELGIQUE"
        ],
        "us": [
            "DOW", "S&P", "NASDAQ", "RUSSELL", "CBOE", "NYSE", "AMEX", "DJIA", "US", "√âTATS-UNIS"
        ],
        "asia": [
            "NIKKEI", "HANG SENG", "SHANGHAI", "SHENZHEN", "KOSPI", "SENSEX",
            "BSE", "TAIEX", "STRAITS", "JAKARTA", "KLSE", "KOSDAQ", "ASX",
            "JAPON", "CHINE", "HONG KONG", "COR√âE DU SUD", "TAIWAN"
        ],
        "other": [
            "MERVAL", "BOVESPA", "IPC", "IPSA", "COLCAP", "BVLG", "IBC", "CASE",
            "ISE", "TA", "QE", "FTSE/JSE", "MOEX", "MSX30", "MSCI",
            "BR√âSIL", "ARGENTINE", "CHILI", "MEXIQUE"
        ]
    },
    # Mapping des s√©lecteurs DOM pour les diff√©rentes r√©gions
    "region_selectors": {
        "europe": "#europe-tab",
        "us": "#etats-unis-tab",
        "asia": "#asie-tab",
        "other": "#autres-tab"
    },
    # Mapping des pays connus pour extraction
    "country_mapping": {
        "france": "France",
        "allemagne": "Allemagne",
        "royaume-uni": "Royaume-Uni",
        "espagne": "Espagne",
        "italie": "Italie",
        "belgique": "Belgique",
        "pays-bas": "Pays-Bas",
        "suisse": "Suisse",
        "√©tats-unis": "√âtats-Unis",
        "usa": "√âtats-Unis",
        "japon": "Japon",
        "chine": "Chine",
        "hong kong": "Hong Kong",
        "cor√©e du sud": "Cor√©e du Sud",
        "taiwan": "Taiwan",
        "br√©sil": "Br√©sil",
        "argentine": "Argentine",
        "chili": "Chili",
        "mexique": "Mexique",
        "australie": "Australie"
    }
}

# Structure pour les donn√©es
MARKET_DATA = {
    "indices": {
        "europe": [],
        "us": [],
        "asia": [],
        "other": []
    },
    "top_performers": {
        "daily": {
            "best": [],
            "worst": []
        },
        "ytd": {
            "best": [],
            "worst": []
        }
    },
    "meta": {
        "source": "Boursorama",
        "url": CONFIG["source_url"],
        "timestamp": datetime.now().isoformat(),
        "count": 0,
        "lastUpdated": datetime.now().isoformat()
    }
}

# Liste pour stocker tous les indices avant le filtrage (pour calculer les Top 3)
ALL_INDICES = []

def get_headers():
    """Cr√©e des en-t√™tes HTTP al√©atoires pour √©viter la d√©tection de bot"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/"
    }

def extract_country_from_name(name):
    """Extrait le pays √† partir du nom de l'indice"""
    # Si le pays est entre parenth√®ses
    if "(" in name and ")" in name:
        country_match = re.search(r'\((.*?)\)', name)
        if country_match:
            country = country_match.group(1).strip().lower()
            return CONFIG["country_mapping"].get(country, country.capitalize())
    
    # Chercher des correspondances directes de pays dans le nom
    name_lower = name.lower()
    for key, value in CONFIG["country_mapping"].items():
        if key in name_lower:
            return value
    
    # Si aucun pays n'a √©t√© trouv√©
    return ""

def extract_table_data(table):
    """Extrait les donn√©es d'un tableau avec am√©liorations pour le pays et la variation depuis janvier"""
    indices = []
    if not table:
        return indices
    
    # Trouver les en-t√™tes pour comprendre la structure des colonnes
    headers = []
    header_row = table.find('thead')
    if header_row:
        headers = [th.text.strip().lower() for th in header_row.find_all('th')]
        logger.info(f"En-t√™tes du tableau: {headers}")
    
    # D√©terminer les indices des colonnes importantes
    name_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['nom', 'indice', 'action'])), 0)
    value_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['dernier', 'cours', 'cl√¥ture'])), 1)
    change_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['var.', 'variation', 'abs', 'veille'])), 2)
    pct_idx = next((i for i, h in enumerate(headers) if '%' in h), 3)
    
    # Recherche sp√©cifique pour la variation depuis janvier
    ytd_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['1 janv', 'depuis le 1er', 'ytd', 'annuel'])), -1)
    
    # Trouver toutes les lignes de donn√©es
    rows = table.select('tbody tr')
    logger.info(f"Nombre de lignes trouv√©es: {len(rows)}")
    
    for row in rows:
        cells = row.find_all('td')
        if len(cells) < 3:
            continue
        
        try:
            # Extraire le nom
            name_cell = cells[name_idx] if name_idx < len(cells) else cells[0]
            name_el = name_cell.find('a') or name_cell
            name = name_el.text.strip()
            
            # Extraire le pays √† partir du nom
            country = extract_country_from_name(name)
            
            # Extraire la valeur (dernier cours)
            value_cell = cells[value_idx] if value_idx < len(cells) else cells[1]
            value = value_cell.text.strip()
            
            # Extraire la variation (par rapport √† la veille)
            change_cell = cells[change_idx] if change_idx < len(cells) else None
            change = change_cell.text.strip() if change_cell else ""
            
            # Extraire le pourcentage de variation
            pct_cell = cells[pct_idx] if pct_idx < len(cells) and pct_idx < len(cells) else None
            change_percent = pct_cell.text.strip() if pct_cell else ""
            
            # Extraire la variation depuis le 1er janvier (si disponible)
            ytd_change = ""
            if ytd_idx >= 0 and ytd_idx < len(cells):
                ytd_change = cells[ytd_idx].text.strip()
            
            # Extraire des donn√©es suppl√©mentaires si disponibles
            opening = cells[4].text.strip() if len(cells) > 4 else ""
            high = cells[5].text.strip() if len(cells) > 5 else ""
            low = cells[6].text.strip() if len(cells) > 6 else ""
            
            # Tenter de nettoyer les donn√©es pour high et low
            if high and "%" in high:
                # Si high contient un pourcentage, c'est probablement une autre donn√©e
                # Essayons de trouver la vraie valeur high dans une autre colonne
                for i in range(4, min(len(cells), 10)):
                    cell_text = cells[i].text.strip()
                    if cell_text and "%" not in cell_text and not cell_text.lower() == "voir":
                        high = cell_text
                        break
            
            # D√©terminer la tendance
            trend = "down" if (change and '-' in change) or (change_percent and '-' in change_percent) else "up"
            
            # Cr√©er l'objet indice
            if name and value:
                index_data = {
                    "name": name,
                    "country": country,
                    "value": value,
                    "change": change,
                    "changePercent": change_percent,
                    "ytdChange": ytd_change,
                    "opening": opening,
                    "high": high,
                    "low": low,
                    "trend": trend
                }
                indices.append(index_data)
                ALL_INDICES.append(index_data)  # Ajouter √† la liste globale pour le top 3
        
        except Exception as e:
            logger.warning(f"Erreur lors du traitement d'une ligne: {str(e)}")
    
    return indices

def is_priority_index(name, region):
    """D√©termine si un indice fait partie des indices prioritaires"""
    # Convertir en majuscules pour la comparaison
    name_upper = name.upper()
    
    # V√©rifier si l'indice est dans la liste des prioritaires pour sa r√©gion
    for priority_name in CONFIG["priority_indices"][region]:
        if priority_name.upper() in name_upper or name_upper in priority_name.upper():
            return True
    
    # Filtrer les indices sp√©cifiques √† exclure
    exclude_patterns = [
        "NASDAQ EUROZONE", 
        "NASDAQ EUROPE", 
        "NASDAQ NORDIC",
        "STOXX EUROPE SELECT"
    ]
    
    # Exclure certains indices qui correspondent aux patterns d'exclusion
    for pattern in exclude_patterns:
        if pattern.upper() in name_upper:
            return False
    
    # Pour les grands indices g√©n√©raux, les inclure m√™me s'ils ne sont pas explicitement list√©s
    general_indices = ["DOW JONES", "S&P", "FTSE", "CAC", "DAX", "NIKKEI", "HANG SENG"]
    for idx in general_indices:
        if idx.upper() in name_upper and len(name) < 30:  # √âviter les sous-indices trop longs
            return True
    
    # Par d√©faut, ne pas inclure
    return False

def classify_index(index_data):
    """Classe un indice dans la bonne r√©gion et ne garde que les indices prioritaires"""
    name = index_data["name"].upper()
    
    # V√©rifier chaque r√©gion
    for region, keywords in CONFIG["regions"].items():
        if any(keyword.upper() in name for keyword in keywords):
            # Ne l'ajouter que s'il s'agit d'un indice prioritaire
            if is_priority_index(index_data["name"], region):
                MARKET_DATA["indices"][region].append(index_data)
            return region
    
    # Par d√©faut, tenter d'ajouter √† "other" si prioritaire
    if is_priority_index(index_data["name"], "other"):
        MARKET_DATA["indices"]["other"].append(index_data)
    return "other"

def scrape_tab_data(soup, region):
    """R√©cup√®re les donn√©es d'un onglet sp√©cifique (Europe, US, Asie, Autres)"""
    logger.info(f"Traitement de l'onglet: {region}")
    
    # Trouver les tableaux dans cet onglet
    tab_content = soup.select(f"{CONFIG['region_selectors'][region]}-content")
    if tab_content:
        # Si on a trouv√© le contenu de l'onglet, chercher les tableaux dedans
        tables = tab_content[0].find_all('table')
    else:
        # Sinon, chercher tous les tableaux et filtrer par r√©gion
        tables = soup.find_all('table')
    
    logger.info(f"Nombre de tableaux trouv√©s pour {region}: {len(tables)}")
    
    # Traiter chaque tableau
    indices = []
    for i, table in enumerate(tables):
        logger.info(f"Traitement du tableau {i+1}/{len(tables)} pour {region}")
        table_indices = extract_table_data(table)
        
        # Filtrer pour ne garder que les indices prioritaires
        filtered_indices = []
        for index in table_indices:
            if is_priority_index(index["name"], region):
                filtered_indices.append(index)
        
        indices.extend(filtered_indices)
        logger.info(f"Indices prioritaires trouv√©s: {len(filtered_indices)}/{len(table_indices)}")
    
    return indices

def direct_scrape_approach(html):
    """Approche alternative qui recherche directement tous les tableaux"""
    logger.info("Utilisation de l'approche de scraping directe")
    
    soup = BeautifulSoup(html, 'html.parser')
    all_tables = soup.find_all('table')
    
    logger.info(f"Nombre total de tableaux trouv√©s: {len(all_tables)}")
    
    # Extraire les donn√©es de tous les tableaux
    all_indices = []
    for i, table in enumerate(all_tables):
        logger.info(f"Traitement du tableau {i+1}/{len(all_tables)}")
        indices = extract_table_data(table)
        all_indices.extend(indices)
    
    # Filtrer et classer les indices
    for index in all_indices:
        classify_index(index)
    
    return sum(len(indices) for indices in MARKET_DATA["indices"].values()) > 0

def get_page_content():
    """R√©cup√®re le contenu de la page avec retries"""
    headers = get_headers()
    max_retries = 3
    retry_delay = 2
    
    for retry in range(max_retries):
        try:
            logger.info(f"Tentative {retry+1}/{max_retries} de r√©cup√©ration de la page")
            response = requests.get(CONFIG["source_url"], headers=headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.warning(f"Erreur lors de la r√©cup√©ration: {str(e)}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
    
    return None

def parse_percentage(percent_str):
    """Convertit une cha√Æne de pourcentage en nombre flottant"""
    if not percent_str:
        return 0.0
    
    # Supprimer les caract√®res non num√©riques sauf le point d√©cimal et le signe moins
    clean_str = re.sub(r'[^0-9\\.\\-]', '', percent_str.replace(',', '.'))
    
    try:
        return float(clean_str)
    except ValueError:
        return 0.0

def calculate_top_performers():
    """Calcule les indices avec les meilleures et pires performances"""
    logger.info("Calcul des indices avec les meilleures/pires performances...")
    
    # Liste pour stocker tous les indices avec leur r√©gion
    indices_with_region = []
    
    # Ajouter la r√©gion √† chaque indice
    for index in ALL_INDICES:
        # S'assurer que l'indice a des donn√©es de variation
        if "changePercent" in index and index["changePercent"]:
            # Cr√©er une copie avec la r√©gion d√©termin√©e
            for region, indices_list in MARKET_DATA["indices"].items():
                if any(i["name"] == index["name"] for i in indices_list):
                    index_copy = index.copy()
                    index_copy["region"] = region
                    indices_with_region.append(index_copy)
                    break
    
    # Filtrer les indices avec des valeurs de pourcentage valides
    daily_indices = [idx for idx in indices_with_region if idx.get("changePercent")]
    ytd_indices = [idx for idx in indices_with_region if idx.get("ytdChange")]
    
    # Trier par variation quotidienne
    if daily_indices:
        # Convertir les pourcentages en valeurs num√©riques pour le tri
        for idx in daily_indices:
            idx["_change_value"] = parse_percentage(idx["changePercent"])
        
        # Trier et s√©lectionner les 3 meilleurs et les 3 pires
        sorted_daily = sorted(daily_indices, key=lambda x: x["_change_value"], reverse=True)
        
        # S√©lectionner les 3 meilleurs
        best_daily = sorted_daily[:3]
        # S√©lectionner les 3 pires (en excluant les valeurs √† 0 qui pourraient √™tre des donn√©es manquantes)
        worst_daily = [idx for idx in sorted_daily if idx["_change_value"] != 0]
        worst_daily = sorted(worst_daily, key=lambda x: x["_change_value"])[:3]
        
        # Ajouter aux r√©sultats en supprimant le champ temporaire _change_value
        for idx in best_daily:
            idx_copy = {k: v for k, v in idx.items() if k != "_change_value"}
            MARKET_DATA["top_performers"]["daily"]["best"].append(idx_copy)
        
        for idx in worst_daily:
            idx_copy = {k: v for k, v in idx.items() if k != "_change_value"}
            MARKET_DATA["top_performers"]["daily"]["worst"].append(idx_copy)
    
    # Trier par variation depuis le d√©but de l'ann√©e
    if ytd_indices:
        # Convertir les pourcentages en valeurs num√©riques pour le tri
        for idx in ytd_indices:
            idx["_ytd_value"] = parse_percentage(idx["ytdChange"])
        
        # Trier et s√©lectionner les 3 meilleurs et les 3 pires
        sorted_ytd = sorted(ytd_indices, key=lambda x: x["_ytd_value"], reverse=True)
        
        # S√©lectionner les 3 meilleurs
        best_ytd = sorted_ytd[:3]
        # S√©lectionner les 3 pires (en excluant les valeurs √† 0 qui pourraient √™tre des donn√©es manquantes)
        worst_ytd = [idx for idx in sorted_ytd if idx["_ytd_value"] != 0]
        worst_ytd = sorted(worst_ytd, key=lambda x: x["_ytd_value"])[:3]
        
        # Ajouter aux r√©sultats en supprimant le champ temporaire _ytd_value
        for idx in best_ytd:
            idx_copy = {k: v for k, v in idx.items() if k != "_ytd_value"}
            MARKET_DATA["top_performers"]["ytd"]["best"].append(idx_copy)
        
        for idx in worst_ytd:
            idx_copy = {k: v for k, v in idx.items() if k != "_ytd_value"}
            MARKET_DATA["top_performers"]["ytd"]["worst"].append(idx_copy)
    
    logger.info(f"Top performers calcul√©s. Daily: {len(MARKET_DATA['top_performers']['daily']['best'])} best, {len(MARKET_DATA['top_performers']['daily']['worst'])} worst. YTD: {len(MARKET_DATA['top_performers']['ytd']['best'])} best, {len(MARKET_DATA['top_performers']['ytd']['worst'])} worst.")

def scrape_market_data():
    """R√©cup√®re et parse la page de Boursorama"""
    logger.info(f"üîç R√©cup√©ration des donn√©es depuis {CONFIG['source_url']}...")
    
    try:
        # Vider la liste globale des indices
        ALL_INDICES.clear()
        
        # R√©cup√©rer le contenu de la page
        html = get_page_content()
        if not html:
            logger.error("Impossible de r√©cup√©rer la page")
            return False
        
        if len(html) < 1000:
            logger.warning(f"Page trop courte ({len(html)} caract√®res), possible erreur")
        
        # Parser le HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # V√©rifier la structure pour voir si on a des onglets
        has_tabs = any(soup.select(selector) for selector in CONFIG["region_selectors"].values())
        
        if has_tabs:
            logger.info("Structure avec onglets d√©tect√©e, traitement par onglet")
            
            # Traiter chaque onglet
            for region in CONFIG["regions"].keys():
                region_indices = scrape_tab_data(soup, region)
                # Pas besoin d'ajouter ici car scrape_tab_data filtre d√©j√†
                logger.info(f"Indices trouv√©s pour {region}: {len(region_indices)}")
        else:
            logger.info("Structure sans onglets d√©tect√©e, approche directe")
            if not direct_scrape_approach(html):
                logger.warning("L'approche directe n'a trouv√© aucun indice")
                return False
        
        # Calculer les indices avec les meilleures et pires performances
        calculate_top_performers()
        
        # Mettre √† jour le compteur
        MARKET_DATA["meta"]["count"] = sum(len(indices) for indices in MARKET_DATA["indices"].values())
        
        logger.info(f"‚úÖ Donn√©es extraites avec succ√®s: {MARKET_DATA['meta']['count']} indices")
        
        # V√©rifier qu'on a assez de donn√©es
        if MARKET_DATA["meta"]["count"] < 3:
            logger.warning(f"‚ö†Ô∏è Trop peu d'indices trouv√©s: {MARKET_DATA['meta']['count']}")
            return False
        
        # Trier les indices par nom dans chaque r√©gion
        for region in MARKET_DATA["indices"]:
            MARKET_DATA["indices"][region] = sorted(MARKET_DATA["indices"][region], key=lambda x: x["name"])
        
        # Mise √† jour de l'horodatage
        now = datetime.now()
        MARKET_DATA["meta"]["timestamp"] = now.isoformat()
        MARKET_DATA["meta"]["lastUpdated"] = now.isoformat()
        
        # Enregistrer les donn√©es
        save_market_data()
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def save_market_data():
    """Enregistre les donn√©es dans un fichier JSON"""
    try:
        # Cr√©er le dossier data s'il n'existe pas
        data_dir = os.path.dirname(CONFIG["output_path"])
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        
        # √âcrire le fichier JSON
        with open(CONFIG["output_path"], 'w', encoding='utf-8') as f:
            json.dump(MARKET_DATA, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es enregistr√©es dans {CONFIG['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es: {str(e)}")
        return False

def check_existing_data():
    """V√©rifier si un fichier de donn√©es existe d√©j√†"""
    try:
        if os.path.exists(CONFIG["output_path"]):
            logger.info("üìÇ Fichier de donn√©es existant trouv√©")
            return True
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification du fichier existant: {str(e)}")
        return False

def main():
    """Point d'entr√©e principal avec gestion d'erreur robuste"""
    try:
        logger.info("üöÄ D√©marrage du script de scraping des donn√©es de march√©")
        
        # V√©rifier si les donn√©es existent d√©j√†
        has_existing_data = check_existing_data()
        
        # Tenter d'extraire les nouvelles donn√©es
        scraping_success = scrape_market_data()
        
        # Si l'extraction √©choue mais qu'on a des donn√©es existantes, conserver le fichier
        if not scraping_success and has_existing_data:
            logger.info("‚ö†Ô∏è Utilisation des donn√©es existantes car le scraping a √©chou√©")
            sys.exit(0) # Sortie sans erreur pour ne pas faire √©chouer le workflow
        elif not scraping_success and not has_existing_data:
            logger.error("‚ùå Aucune donn√©e existante et √©chec du scraping")
            sys.exit(1) # Sortie avec erreur car on n'a pas de donn√©es
        else:
            logger.info("‚úÖ Scraping termin√© avec succ√®s")
            sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Si une erreur se produit mais que le fichier existe d√©j√†, ne pas faire √©chouer le workflow
        if check_existing_data():
            logger.info("‚ö†Ô∏è Une erreur s'est produite mais les donn√©es existantes seront conserv√©es")
            sys.exit(0)
        else:
            sys.exit(1)

if __name__ == "__main__":
    # D√©sactiver l'avertissement sur les v√©rifications SSL d√©sactiv√©es
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # Lancer le programme
    main()