#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des donn√©es boursi√®res depuis Boursorama
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es
Version am√©lior√©e pour extraire correctement le pays et le libell√© d'indice
"""

import os
import json
import sys
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import re
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "source_url": "https://www.boursorama.com/bourse/indices/internationaux",
    "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "markets.json"),
    # Indices prioritaires √† afficher (les autres seront filtr√©s)
    "priority_indices": {
        "europe": [
            "CAC 40", "DAX", "FTSE 100", "EURO STOXX 50", "Euro Stoxx 50", "BEL 20", "IBEX", "FTSE MIB", "AEX", "SMI",
            "Zone Euro", "Belgique", "France", "Allemagne", "Royaume-Uni", "Espagne", "Italie", "Pays-Bas", "Suisse"
        ],
        "north-america": [
            "DOW JONES", "Dow Jones", "S&P 500", "NASDAQ", "NASDAQ COMPOSITE", "RUSSELL 2000", 
            "DJIA", "US", "√âtats-Unis", "Canada", "TSX", "S&P/TSX"
        ],
        "latin-america": [
            "MERVAL", "BOVESPA", "Br√©sil", "Argentine", "Chili", "Mexique", "IPC MEXICO", "IPSA", "COLCAP"
        ],
        "asia": [
            "NIKKEI", "NIKKEI 225", "HANG SENG", "SHANGHAI COMPOSITE", "SHENZHEN COMPONENT", 
            "KOSPI", "Taiwan", "TAIEX", "Japon", "Chine", "Hong Kong", "Cor√©e du Sud"
        ],
        "other": [
            "MSCI WORLD", "MSCI", "ASX", "Australie", "Nouvelle-Z√©lande", "Isra√´l", 
            "√âmirats Arabes Unis", "Qatar", "Afrique du Sud", "Maroc"
        ]
    },
    # Structure des r√©gions pour la classification des indices
    "regions": {
        "europe": [
            "CAC", "DAX", "FTSE", "IBEX", "MIB", "AEX", "BEL", "SMI", "ATX",
            "OMX", "OMXS", "ISEQ", "PSI", "ATHEX", "OSEBX", "STOXX", "EURO", "EURONEXT",
            "FRANCE", "ALLEMAGNE", "ROYAUME-UNI", "ESPAGNE", "ITALIE", "PAYS-BAS", "SUISSE", "BELGIQUE",
            "AUTRICHE", "DANEMARK", "FINLANDE", "IRLANDE", "POLOGNE", "PORTUGAL", "NORV√àGE", "SU√àDE", "UK"
        ],
        "north-america": [
            "DOW", "S&P", "NASDAQ", "RUSSELL", "CBOE", "NYSE", "AMEX", "DJIA", "US", "√âTATS-UNIS",
            "CANADA", "TSX", "S&P/TSX"
        ],
        "latin-america": [
            "MERVAL", "BOVESPA", "IPC", "IPSA", "COLCAP", "BVLG", "IBC",
            "BR√âSIL", "ARGENTINE", "CHILI", "MEXIQUE", "COLOMBIE", "P√âROU"
        ],
        "asia": [
            "NIKKEI", "HANG SENG", "SHANGHAI", "SHENZHEN", "KOSPI", "SENSEX",
            "BSE", "TAIEX", "STRAITS", "JAKARTA", "KLSE", "KOSDAQ",
            "JAPON", "CHINE", "HONG KONG", "COR√âE DU SUD", "TAIWAN", "INDE", "INDON√âSIE",
            "MALAISIE", "PHILIPPINES", "THA√èLANDE", "SINGAPOUR"
        ],
        "other": [
            "ASX", "NZX", "TA", "QE", "FTSE/JSE", "MOEX", "MSX30", "MSCI",
            "AUSTRALIE", "NOUVELLE-Z√âLANDE", "ISRA√ãL", "√âMIRATS ARABES UNIS", "QATAR", 
            "AFRIQUE DU SUD", "MAROC", "INTERNATIONAL"
        ]
    },
    # Mapping des s√©lecteurs DOM pour les diff√©rentes r√©gions
    "region_selectors": {
        "europe": "#europe-tab",
        "north-america": "#etats-unis-tab",
        "latin-america": "#autres-tab",
        "asia": "#asie-tab",
        "other": "#autres-tab"
    },
    # Mapping des pays connus pour extraction
    "country_mapping": {
        "france": "France",
        "allemagne": "Allemagne",
        "royaume-uni": "Royaume-Uni",
        "royaume uni": "Royaume-Uni",
        "uk": "Royaume-Uni",
        "great britain": "Royaume-Uni",
        "espagne": "Espagne",
        "italie": "Italie",
        "belgique": "Belgique",
        "pays-bas": "Pays-Bas",
        "suisse": "Suisse",
        "autriche": "Autriche",
        "danemark": "Danemark",
        "finlande": "Finlande",
        "irlande": "Irlande",
        "pologne": "Pologne",
        "portugal": "Portugal",
        "norv√®ge": "Norv√®ge",
        "su√®de": "Su√®de",
        "√©tats-unis": "√âtats-Unis",
        "usa": "√âtats-Unis",
        "canada": "Canada",
        "japon": "Japon",
        "chine": "Chine",
        "hong kong": "Hong Kong",
        "cor√©e du sud": "Cor√©e du Sud",
        "taiwan": "Taiwan",
        "inde": "Inde",
        "indon√©sie": "Indon√©sie",
        "malaisie": "Malaisie",
        "philippines": "Philippines",
        "tha√Ølande": "Tha√Ølande",
        "singapour": "Singapour",
        "br√©sil": "Br√©sil",
        "argentine": "Argentine",
        "chili": "Chili",
        "mexique": "Mexique",
        "colombie": "Colombie",
        "p√©rou": "P√©rou",
        "australie": "Australie",
        "nouvelle-z√©lande": "Nouvelle-Z√©lande",
        "isra√´l": "Isra√´l",
        "√©mirats arabes unis": "√âmirats Arabes Unis",
        "qatar": "Qatar",
        "afrique du sud": "Afrique du Sud",
        "maroc": "Maroc",
        "zone euro": "Zone Euro"
    },
    # Indices standards pour chaque pays
    "standard_indices": {
        "France": "CAC 40",
        "Allemagne": "DAX",
        "Royaume-Uni": "FTSE 100",
        "Espagne": "IBEX 35",
        "Italie": "FTSE MIB",
        "Belgique": "BEL 20",
        "Pays-Bas": "AEX",
        "Suisse": "SMI",
        "Zone Euro": "EURO STOXX 50",
        "√âtats-Unis": "S&P 500",
        "Canada": "S&P/TSX",
        "Japon": "NIKKEI 225",
        "Chine": "SHANGHAI COMPOSITE",
        "Hong Kong": "HANG SENG",
        "Cor√©e du Sud": "KOSPI",
        "Br√©sil": "BOVESPA",
        "Argentine": "MERVAL",
        "Chili": "IPSA",
        "Mexique": "IPC MEXICO",
        "Australie": "ASX 200"
    },
    # Mapping des r√©gions pour chaque pays
    "country_region_mapping": {
        "France": "europe",
        "Allemagne": "europe",
        "Royaume-Uni": "europe",
        "Royaume Uni": "europe",
        "Espagne": "europe",
        "Italie": "europe",
        "Belgique": "europe",
        "Pays-Bas": "europe",
        "Suisse": "europe",
        "Autriche": "europe",
        "Danemark": "europe",
        "Finlande": "europe",
        "Irlande": "europe",
        "Pologne": "europe",
        "Portugal": "europe",
        "Norv√®ge": "europe",
        "Su√®de": "europe",
        "Zone Euro": "europe",
        
        "√âtats-Unis": "north-america",
        "Canada": "north-america",
        
        "Br√©sil": "latin-america",
        "Argentine": "latin-america",
        "Chili": "latin-america",
        "Mexique": "latin-america",
        "Colombie": "latin-america",
        "P√©rou": "latin-america",
        
        "Japon": "asia",
        "Chine": "asia",
        "Hong Kong": "asia",
        "Cor√©e du Sud": "asia",
        "Taiwan": "asia",
        "Inde": "asia",
        "Indon√©sie": "asia",
        "Malaisie": "asia",
        "Philippines": "asia",
        "Tha√Ølande": "asia",
        "Singapour": "asia",
        "Ta√Øwan": "asia",
        
        "Australie": "other",
        "Nouvelle-Z√©lande": "other",
        "Isra√´l": "other",
        "√âmirats Arabes Unis": "other",
        "Qatar": "other",
        "Afrique du Sud": "other",
        "Maroc": "other",
        "International": "other",
        "√âgypte": "other",
        "Turquie": "other"
    }
}

# Structure pour les donn√©es
MARKET_DATA = {
    "indices": {
        "europe": [],
        "north-america": [],
        "latin-america": [],
        "asia": [],
        "other": []
    },
    "top_performers": {
        "daily": {
            "best": [],
            "worst": []
        },
        "ytd": {
            "best": [],
            "worst": []
        }
    },
    "meta": {
        "source": "Boursorama",
        "url": CONFIG["source_url"],
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "count": 0,
        "lastUpdated": datetime.now(timezone.utc).isoformat()
    }
}

# Liste pour stocker tous les indices avant le filtrage (pour calculer les Top 3)
ALL_INDICES = []

def get_headers():
    """Cr√©e des en-t√™tes HTTP al√©atoires pour √©viter la d√©tection de bot"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/"
    }

def normalize_country_name(country):
    """Normalise le nom du pays pour √©viter les probl√®mes d'espace ou de trait d'union"""
    if not country:
        return "International"
    
    # Standardiser le nom des pays qui pourraient avoir plusieurs formes
    normalized = country.replace(" Uni", "-Uni").replace("Royaume ", "Royaume-")
    
    # Pour le cas particulier du Royaume-Uni
    if normalized.lower() in ['uk', 'grande bretagne', 'grande-bretagne', 'royaume uni', 'royaume-uni']:
        return "Royaume-Uni"
    
    return normalized

def separate_country_and_index(name):
    """
    S√©pare le nom du pays et le libell√© de l'indice √† partir du nom complet
    """
    # Essayer de trouver un pays connu dans le nom
    found_country = None
    index_name = name
    
    # V√©rifier s'il y a une parenth√®se qui pourrait contenir le pays
    if "(" in name and ")" in name:
        parts = name.split("(")
        index_part = parts[0].strip()
        country_part = parts[1].replace(")", "").strip().lower()
        
        if country_part in CONFIG["country_mapping"]:
            found_country = CONFIG["country_mapping"][country_part]
            index_name = index_part
    
    # Si aucun pays n'a √©t√© trouv√© dans les parenth√®ses, chercher dans le nom
    if not found_country:
        # Recherche de correspondances directes
        for country_key, country_name in CONFIG["country_mapping"].items():
            if country_key in name.lower():
                found_country = country_name
                # Ne pas modifier l'index_name ici car nous ne savons pas exactement o√π est le pays dans le nom
                break
    
    # Si toujours pas de pays trouv√©, essayer de d√©terminer √† partir des indices connus
    if not found_country:
        for standard_country, standard_index in CONFIG["standard_indices"].items():
            if standard_index.upper() in name.upper():
                found_country = standard_country
                break
    
    # Si nous avons un pays mais pas clairement un indice, essayer de d√©terminer l'indice standard
    if found_country and (index_name == name or len(index_name.strip()) < 3):
        if found_country in CONFIG["standard_indices"]:
            index_name = CONFIG["standard_indices"][found_country]
    
    # Nettoyage final du nom de l'indice
    if index_name == name and found_country:
        # Essayer de remplacer le nom du pays dans le nom complet si nous avons trouv√© un pays
        index_name = name.replace(found_country, "").strip()
        if len(index_name) < 3:  # Si le nom de l'indice est trop court apr√®s suppression
            index_name = name
    
    # Si aucun pays n'a √©t√© trouv√©, utiliser un pays par d√©faut bas√© sur la r√©gion
    if not found_country:
        if "DAX" in name or "XETRA" in name:
            found_country = "Allemagne"
        elif "CAC" in name or "PARIS" in name:
            found_country = "France"
        elif "FTSE" in name or "LONDON" in name or "UK" in name.upper():
            found_country = "Royaume-Uni"
        elif "NASDAQ" in name or "S&P" in name or "DOW" in name:
            found_country = "√âtats-Unis"
        elif "TSX" in name or "TORONTO" in name:
            found_country = "Canada"
        elif "NIKKEI" in name or "TOKYO" in name:
            found_country = "Japon"
        elif "SHANGHAI" in name or "SHENZHEN" in name:
            found_country = "Chine"
        elif "HANG SENG" in name:
            found_country = "Hong Kong"
        elif "BOVESPA" in name or "BRAZIL" in name:
            found_country = "Br√©sil"
        elif "MERVAL" in name:
            found_country = "Argentine"
        elif "IPSA" in name:
            found_country = "Chili"
        elif "IPC" in name and "MEXICO" in name:
            found_country = "Mexique"
        else:
            found_country = "International"
    
    # Normaliser le nom du pays
    found_country = normalize_country_name(found_country)
    
    return found_country, index_name

def extract_table_data(table):
    """Extrait les donn√©es d'un tableau avec s√©paration pays/indice"""
    indices = []
    if not table:
        return indices
    
    # Trouver les en-t√™tes pour comprendre la structure des colonnes
    headers = []
    header_row = table.find('thead')
    if header_row:
        headers = [th.text.strip().lower() for th in header_row.find_all('th')]
        logger.info(f"En-t√™tes du tableau: {headers}")
    
    # D√©terminer les indices des colonnes importantes
    name_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['nom', 'indice', 'action', 'libell√©'])), 0)
    value_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['dernier', 'cours', 'cl√¥ture'])), 1)
    change_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['var.', 'variation', 'abs', 'veille'])), 2)
    pct_idx = next((i for i, h in enumerate(headers) if '%' in h), 3)
    
    # Recherche sp√©cifique pour la variation depuis janvier
    ytd_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['1 janv', 'depuis le 1er', 'ytd', 'annuel', 'var/1janv'])), -1)
    
    # V√©rifier s'il y a d√©j√† une colonne pays
    country_idx = next((i for i, h in enumerate(headers) if h == 'pays'), -1)
    
    # Trouver toutes les lignes de donn√©es
    rows = table.select('tbody tr')
    logger.info(f"Nombre de lignes trouv√©es: {len(rows)}")
    
    for row in rows:
        cells = row.find_all('td')
        if len(cells) < 3:
            continue
        
        try:
            # Extraire le nom
            name_cell = cells[name_idx] if name_idx < len(cells) else cells[0]
            name_el = name_cell.find('a') or name_cell
            name = name_el.text.strip()
            
            # Extraire le pays (si disponible directement)
            country = None
            if country_idx >= 0 and country_idx < len(cells):
                country = cells[country_idx].text.strip()
            
            # Si le pays n'est pas disponible directement, l'extraire du nom
            if not country:
                country, index_name = separate_country_and_index(name)
            else:
                # Si le pays est disponible, le nom de l'indice est simplement le nom
                index_name = name
            
            # Normaliser le nom du pays
            country = normalize_country_name(country)
            
            # Extraire la valeur (dernier cours)
            value_cell = cells[value_idx] if value_idx < len(cells) else cells[1]
            value = value_cell.text.strip()
            
            # Extraire la variation (par rapport √† la veille)
            change_cell = cells[change_idx] if change_idx < len(cells) else None
            change = change_cell.text.strip() if change_cell else ""
            
            # Extraire le pourcentage de variation
            pct_cell = cells[pct_idx] if pct_idx < len(cells) and pct_idx < len(cells) else None
            change_percent = pct_cell.text.strip() if pct_cell else ""
            
            # Extraire la variation depuis le 1er janvier (si disponible)
            ytd_change = ""
            if ytd_idx >= 0 and ytd_idx < len(cells):
                ytd_change = cells[ytd_idx].text.strip()
            
            # Extraire des donn√©es suppl√©mentaires si disponibles
            opening = cells[4].text.strip() if len(cells) > 4 else ""
            high = cells[5].text.strip() if len(cells) > 5 else ""
            low = cells[6].text.strip() if len(cells) > 6 else ""
            
            # D√©terminer la tendance
            trend = "down" if (change and '-' in change) or (change_percent and '-' in change_percent) else "up"
            
            # Cr√©er l'objet indice avec la structure correcte
            if name and value:
                index_data = {
                    "country": country,           # Pays
                    "index_name": index_name,     # Libell√© de l'indice
                    "value": value,               # Dernier cours
                    "change": change,             # Variation
                    "changePercent": change_percent,  # Variation %
                    "ytdChange": ytd_change,      # Variation depuis janvier
                    "opening": opening,           # Ouverture
                    "high": high,                 # Plus haut
                    "low": low,                   # Plus bas
                    "trend": trend                # Tendance
                }
                indices.append(index_data)
                ALL_INDICES.append(index_data)  # Ajouter √† la liste globale pour le top 3
        
        except Exception as e:
            logger.warning(f"Erreur lors du traitement d'une ligne: {str(e)}")
    
    return indices

def determine_region(country):
    """D√©termine la r√©gion bas√©e sur le pays"""
    # Normaliser le nom du pays pour la recherche
    normalized_country = normalize_country_name(country)
    
    if normalized_country in CONFIG["country_region_mapping"]:
        return CONFIG["country_region_mapping"][normalized_country]
    
    # Si le pays n'est pas trouv√© dans le mapping, chercher par correspondance partielle
    country_upper = country.upper()
    for region, keywords in CONFIG["regions"].items():
        if any(keyword.upper() in country_upper for keyword in keywords):
            return region
    
    # Par d√©faut, classer comme "other"
    return "other"

def is_priority_index(index_data, region):
    """D√©termine si un indice fait partie des indices prioritaires"""
    # Convertir en majuscules pour la comparaison
    country_upper = index_data["country"].upper()
    index_name_upper = index_data["index_name"].upper()
    
    # V√©rifier si le pays ou l'indice est dans la liste des prioritaires pour sa r√©gion
    for priority_name in CONFIG["priority_indices"][region]:
        if (priority_name.upper() in country_upper or 
            country_upper in priority_name.upper() or
            priority_name.upper() in index_name_upper or 
            index_name_upper in priority_name.upper()):
            return True
    
    # Filtrer les indices sp√©cifiques √† exclure
    exclude_patterns = [
        "NASDAQ EUROZONE", 
        "NASDAQ EUROPE", 
        "NASDAQ NORDIC",
        "STOXX EUROPE SELECT"
    ]
    
    # Exclure certains indices qui correspondent aux patterns d'exclusion
    for pattern in exclude_patterns:
        if pattern.upper() in index_name_upper:
            return False
    
    # Pour les grands indices g√©n√©raux, les inclure m√™me s'ils ne sont pas explicitement list√©s
    general_indices = ["DOW JONES", "S&P", "FTSE", "CAC", "DAX", "NIKKEI", "HANG SENG", "BOVESPA", "IPC", "MERVAL", "IPSA", "TSX"]
    for idx in general_indices:
        if idx.upper() in index_name_upper and len(index_name_upper) < 30:  # √âviter les sous-indices trop longs
            return True
    
    # Par d√©faut, ne pas inclure
    return False

def classify_index(index_data):
    """Classe un indice dans la bonne r√©gion et ne garde que les indices prioritaires"""
    # D√©terminer la r√©gion en fonction du pays
    country = index_data["country"]
    region = determine_region(country)
    
    # Ne l'ajouter que s'il s'agit d'un indice prioritaire
    if is_priority_index(index_data, region):
        MARKET_DATA["indices"][region].append(index_data)
    
    return region

def scrape_tab_data(soup, region):
    """R√©cup√®re les donn√©es d'un onglet sp√©cifique (Europe, US, Asie, Autres)"""
    logger.info(f"Traitement de l'onglet: {region}")
    
    # Trouver les tableaux dans cet onglet
    tab_content = soup.select(f"{CONFIG['region_selectors'][region]}-content")
    if tab_content:
        # Si on a trouv√© le contenu de l'onglet, chercher les tableaux dedans
        tables = tab_content[0].find_all('table')
    else:
        # Sinon, chercher tous les tableaux et filtrer par r√©gion
        tables = soup.find_all('table')
    
    logger.info(f"Nombre de tableaux trouv√©s pour {region}: {len(tables)}")
    
    # Traiter chaque tableau
    indices = []
    for i, table in enumerate(tables):
        logger.info(f"Traitement du tableau {i+1}/{len(tables)} pour {region}")
        table_indices = extract_table_data(table)
        
        # Filtrer pour ne garder que les indices prioritaires
        filtered_indices = []
        for index in table_indices:
            if is_priority_index(index, region):
                filtered_indices.append(index)
        
        indices.extend(filtered_indices)
        logger.info(f"Indices prioritaires trouv√©s: {len(filtered_indices)}/{len(table_indices)}")
    
    return indices

def direct_scrape_approach(html):
    """Approche alternative qui recherche directement tous les tableaux"""
    logger.info("Utilisation de l'approche de scraping directe")
    
    soup = BeautifulSoup(html, 'html.parser')
    all_tables = soup.find_all('table')
    
    logger.info(f"Nombre total de tableaux trouv√©s: {len(all_tables)}")
    
    # Extraire les donn√©es de tous les tableaux
    all_indices = []
    for i, table in enumerate(all_tables):
        logger.info(f"Traitement du tableau {i+1}/{len(all_tables)}")
        indices = extract_table_data(table)
        all_indices.extend(indices)
    
    # Filtrer et classer les indices
    for index in all_indices:
        classify_index(index)
    
    return sum(len(indices) for indices in MARKET_DATA["indices"].values()) > 0

def get_page_content():
    """R√©cup√®re le contenu de la page avec retries"""
    headers = get_headers()
    max_retries = 3
    retry_delay = 2
    
    for retry in range(max_retries):
        try:
            logger.info(f"Tentative {retry+1}/{max_retries} de r√©cup√©ration de la page")
            response = requests.get(CONFIG["source_url"], headers=headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.warning(f"Erreur lors de la r√©cup√©ration: {str(e)}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
    
    return None

def parse_percentage(percent_str):
    """Convertit une cha√Æne de pourcentage en nombre flottant"""
    if not percent_str:
        return 0.0
    
    # Supprimer les caract√®res non num√©riques sauf le point d√©cimal et le signe moins
    clean_str = re.sub(r'[^0-9\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\-]/g', '', percent_str.replace(',', '.'))
    
    try:
        return float(clean_str)
    except ValueError:
        return 0.0

def calculate_top_performers():
    """Calcule les indices avec les meilleures et pires performances"""
    logger.info("Calcul des indices avec les meilleures/pires performances...")
    
    # Liste pour stocker tous les indices avec leur r√©gion
    indices_with_region = []
    
    # Ajouter la r√©gion √† chaque indice
    for index in ALL_INDICES:
        # S'assurer que l'indice a des donn√©es de variation
        if "changePercent" in index and index["changePercent"]:
            # D√©terminer la r√©gion
            country = index["country"]
            region = determine_region(country)
            
            # Cr√©er une copie avec la r√©gion d√©termin√©e
            index_copy = index.copy()
            index_copy["region"] = region
            indices_with_region.append(index_copy)
    
    # Filtrer les indices avec des valeurs de pourcentage valides
    daily_indices = [idx for idx in indices_with_region if idx.get("changePercent")]
    ytd_indices = [idx for idx in indices_with_region if idx.get("ytdChange")]
    
    # Trier par variation quotidienne
    if daily_indices:
        # Convertir les pourcentages en valeurs num√©riques pour le tri
        for idx in daily_indices:
            idx["_change_value"] = parse_percentage(idx["changePercent"])
        
        # Trier et s√©lectionner les 3 meilleurs et les 3 pires
        sorted_daily = sorted(daily_indices, key=lambda x: x["_change_value"], reverse=True)
        
        # S√©lectionner les 3 meilleurs
        best_daily = sorted_daily[:3]
        # S√©lectionner les 3 pires (en excluant les valeurs √† 0 qui pourraient √™tre des donn√©es manquantes)
        worst_daily = [idx for idx in sorted_daily if idx["_change_value"] != 0]
        worst_daily = sorted(worst_daily, key=lambda x: x["_change_value"])[:3]
        
        # Ajouter aux r√©sultats en supprimant le champ temporaire _change_value
        for idx in best_daily:
            idx_copy = {k: v for k, v in idx.items() if k != "_change_value"}
            MARKET_DATA["top_performers"]["daily"]["best"].append(idx_copy)
        
        for idx in worst_daily:
            idx_copy = {k: v for k, v in idx.items() if k != "_change_value"}
            MARKET_DATA["top_performers"]["daily"]["worst"].append(idx_copy)
    
    # Trier par variation depuis le d√©but de l'ann√©e
    if ytd_indices:
        # Convertir les pourcentages en valeurs num√©riques pour le tri
        for idx in ytd_indices:
            idx["_ytd_value"] = parse_percentage(idx["ytdChange"])
        
        # Trier et s√©lectionner les 3 meilleurs et les 3 pires
        sorted_ytd = sorted(ytd_indices, key=lambda x: x["_ytd_value"], reverse=True)
        
        # S√©lectionner les 3 meilleurs
        best_ytd = sorted_ytd[:3]
        # S√©lectionner les 3 pires (en excluant les valeurs √† 0 qui pourraient √™tre des donn√©es manquantes)
        worst_ytd = [idx for idx in sorted_ytd if idx["_ytd_value"] != 0]
        worst_ytd = sorted(worst_ytd, key=lambda x: x["_ytd_value"])[:3]
        
        # Ajouter aux r√©sultats en supprimant le champ temporaire _ytd_value
        for idx in best_ytd:
            idx_copy = {k: v for k, v in idx.items() if k != "_ytd_value"}
            MARKET_DATA["top_performers"]["ytd"]["best"].append(idx_copy)
        
        for idx in worst_ytd:
            idx_copy = {k: v for k, v in idx.items() if k != "_ytd_value"}
            MARKET_DATA["top_performers"]["ytd"]["worst"].append(idx_copy)
    
    logger.info(f"Top performers calcul√©s. Daily: {len(MARKET_DATA['top_performers']['daily']['best'])} best, {len(MARKET_DATA['top_performers']['daily']['worst'])} worst. YTD: {len(MARKET_DATA['top_performers']['ytd']['best'])} best, {len(MARKET_DATA['top_performers']['ytd']['worst'])} worst.")

def scrape_market_data():
    """R√©cup√®re et parse la page de Boursorama"""
    logger.info(f"üîç R√©cup√©ration des donn√©es depuis {CONFIG['source_url']}...")
    
    try:
        # Vider la liste globale des indices
        ALL_INDICES.clear()
        
        # R√©initialiser les donn√©es d'indice
        for region in MARKET_DATA["indices"]:
            MARKET_DATA["indices"][region] = []
        
        # R√©cup√©rer le contenu de la page
        html = get_page_content()
        if not html:
            logger.error("Impossible de r√©cup√©rer la page")
            return False
        
        if len(html) < 1000:
            logger.warning(f"Page trop courte ({len(html)} caract√®res), possible erreur")
        
        # Parser le HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # V√©rifier la structure pour voir si on a des onglets
        has_tabs = any(soup.select(selector) for selector in CONFIG["region_selectors"].values())
        
        if has_tabs:
            logger.info("Structure avec onglets d√©tect√©e, traitement par onglet")
            
            # Traiter chaque onglet (ancien format)
            for region_key in CONFIG["region_selectors"].keys():
                region_indices = scrape_tab_data(soup, region_key)
                # Pas besoin d'ajouter ici car scrape_tab_data filtre d√©j√†
                logger.info(f"Indices trouv√©s pour {region_key}: {len(region_indices)}")
        else:
            logger.info("Structure sans onglets d√©tect√©e, approche directe")
            if not direct_scrape_approach(html):
                logger.warning("L'approche directe n'a trouv√© aucun indice")
                return False
        
        # Traiter tous les indices non class√©s
        for index in ALL_INDICES:
            # V√©rifier s'il est d√©j√† dans une r√©gion
            is_already_added = False
            for region_indices in MARKET_DATA["indices"].values():
                if any(i["country"] == index["country"] and i["index_name"] == index["index_name"] for i in region_indices):
                    is_already_added = True
                    break
            
            # S'il n'est pas d√©j√† ajout√©, le classifier dans la bonne r√©gion
            if not is_already_added:
                classify_index(index)
        
        # Calculer les indices avec les meilleures et pires performances
        calculate_top_performers()
        
        # Mettre √† jour le compteur
        MARKET_DATA["meta"]["count"] = sum(len(indices) for indices in MARKET_DATA["indices"].values())
        
        logger.info(f"‚úÖ Donn√©es extraites avec succ√®s: {MARKET_DATA['meta']['count']} indices")
        
        # V√©rifier qu'on a assez de donn√©es
        if MARKET_DATA["meta"]["count"] < 3:
            logger.warning(f"‚ö†Ô∏è Trop peu d'indices trouv√©s: {MARKET_DATA['meta']['count']}")
            return False
        
        # Trier les indices par nom dans chaque r√©gion
        for region in MARKET_DATA["indices"]:
            MARKET_DATA["indices"][region] = sorted(MARKET_DATA["indices"][region], key=lambda x: x["country"])
        
        # Mise √† jour de l'horodatage
        now = datetime.now(timezone.utc)
        MARKET_DATA["meta"]["timestamp"] = now.isoformat()
        MARKET_DATA["meta"]["lastUpdated"] = now.isoformat()
        
        # Enregistrer les donn√©es
        save_market_data()
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def save_market_data():
    """Enregistre les donn√©es dans un fichier JSON"""
    try:
        # Cr√©er le dossier data s'il n'existe pas
        data_dir = os.path.dirname(CONFIG["output_path"])
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        
        # √âcrire le fichier JSON
        with open(CONFIG["output_path"], 'w', encoding='utf-8') as f:
            json.dump(MARKET_DATA, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es enregistr√©es dans {CONFIG['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es: {str(e)}")
        return False

def check_existing_data():
    """V√©rifier si un fichier de donn√©es existe d√©j√†"""
    try:
        if os.path.exists(CONFIG["output_path"]):
            logger.info("üìÇ Fichier de donn√©es existant trouv√©")
            return True
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification du fichier existant: {str(e)}")
        return False

def main():
    """Point d'entr√©e principal avec gestion d'erreur robuste"""
    try:
        logger.info("üöÄ D√©marrage du script de scraping des donn√©es de march√©")
        
        # V√©rifier si les donn√©es existent d√©j√†
        has_existing_data = check_existing_data()
        
        # Tenter d'extraire les nouvelles donn√©es
        scraping_success = scrape_market_data()
        
        # Si l'extraction √©choue mais qu'on a des donn√©es existantes, conserver le fichier
        if not scraping_success and has_existing_data:
            logger.info("‚ö†Ô∏è Utilisation des donn√©es existantes car le scraping a √©chou√©")
            sys.exit(0) # Sortie sans erreur pour ne pas faire √©chouer le workflow
        elif not scraping_success and not has_existing_data:
            logger.error("‚ùå Aucune donn√©e existante et √©chec du scraping")
            sys.exit(1) # Sortie avec erreur car on n'a pas de donn√©es
        else:
            logger.info("‚úÖ Scraping termin√© avec succ√®s")
            sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Si une erreur se produit mais que le fichier existe d√©j√†, ne pas faire √©chouer le workflow
        if check_existing_data():
            logger.info("‚ö†Ô∏è Une erreur s'est produite mais les donn√©es existantes seront conserv√©es")
            sys.exit(0)
        else:
            sys.exit(1)

if __name__ == "__main__":
    # D√©sactiver l'avertissement sur les v√©rifications SSL d√©sactiv√©es
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # Lancer le programme
    main()
