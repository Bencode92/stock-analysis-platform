#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des donn√©es boursi√®res depuis Boursorama
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es
"""

import os
import json
import sys
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "source_url": "https://www.boursorama.com/bourse/indices/internationaux",
    "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "markets.json"),
    # Structure des r√©gions pour la classification des indices
    "regions": {
        "europe": [
            "CAC", "DAX", "FTSE", "IBEX", "MIB", "AEX", "BEL", "SMI", "ATX",
            "OMX", "OMXS", "ISEQ", "PSI", "ATHEX", "OSEBX", "STOXX", "EURO"
        ],
        "us": [
            "DOW", "S&P", "NASDAQ", "RUSSELL", "CBOE", "NYSE", "AMEX"
        ],
        "asia": [
            "NIKKEI", "HANG SENG", "SHANGHAI", "SHENZHEN", "KOSPI", "SENSEX",
            "BSE", "TAIEX", "STRAITS", "JAKARTA", "KLSE", "KOSDAQ", "ASX"
        ],
        "other": [
            "MERVAL", "BOVESPA", "IPC", "IPSA", "COLCAP", "BVLG", "IBC", "CASE",
            "ISE", "TA", "QE", "FTSE/JSE", "MOEX", "MSX30"
        ]
    },
    # Mapping des s√©lecteurs DOM pour les diff√©rentes r√©gions
    "region_selectors": {
        "europe": "#europe-tab",
        "us": "#etats-unis-tab",
        "asia": "#asie-tab",
        "other": "#autres-tab"
    }
}

# Structure pour les donn√©es
MARKET_DATA = {
    "indices": {
        "europe": [],
        "us": [],
        "asia": [],
        "other": []
    },
    "meta": {
        "source": "Boursorama",
        "url": CONFIG["source_url"],
        "timestamp": datetime.now().isoformat(),
        "count": 0,
        "lastUpdated": datetime.now().isoformat()
    }
}

def get_headers():
    """Cr√©e des en-t√™tes HTTP al√©atoires pour √©viter la d√©tection de bot"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/"
    }

def extract_table_data(table):
    """Extrait les donn√©es d'un tableau"""
    indices = []
    if not table:
        return indices
    
    # Trouver les en-t√™tes pour comprendre la structure des colonnes
    headers = []
    header_row = table.find('thead')
    if header_row:
        headers = [th.text.strip().lower() for th in header_row.find_all('th')]
        logger.info(f"En-t√™tes du tableau: {headers}")
    
    # D√©terminer les indices des colonnes importantes
    name_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['nom', 'indice', 'action'])), 0)
    value_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['dernier', 'cours', 'cl√¥ture'])), 1)
    change_idx = next((i for i, h in enumerate(headers) if any(keyword in h for keyword in ['var.', 'variation', 'abs'])), 2)
    pct_idx = next((i for i, h in enumerate(headers) if '%' in h), 3)
    
    # Trouver toutes les lignes de donn√©es
    rows = table.select('tbody tr')
    logger.info(f"Nombre de lignes trouv√©es: {len(rows)}")
    
    for row in rows:
        cells = row.find_all('td')
        if len(cells) < 3:
            continue
        
        try:
            # Extraire le nom
            name_cell = cells[name_idx] if name_idx < len(cells) else cells[0]
            name_el = name_cell.find('a') or name_cell
            name = name_el.text.strip()
            
            # Extraire la valeur
            value_cell = cells[value_idx] if value_idx < len(cells) else cells[1]
            value = value_cell.text.strip()
            
            # Extraire la variation
            change_cell = cells[change_idx] if change_idx < len(cells) else None
            change = change_cell.text.strip() if change_cell else ""
            
            # Extraire le pourcentage
            pct_cell = cells[pct_idx] if pct_idx < len(cells) and pct_idx < len(cells) else None
            change_percent = pct_cell.text.strip() if pct_cell else ""
            
            # Extraire des donn√©es suppl√©mentaires si disponibles
            opening = cells[4].text.strip() if len(cells) > 4 else ""
            high = cells[5].text.strip() if len(cells) > 5 else ""
            low = cells[6].text.strip() if len(cells) > 6 else ""
            
            # D√©terminer la tendance
            trend = "down" if (change and '-' in change) or (change_percent and '-' in change_percent) else "up"
            
            # Cr√©er l'objet indice
            if name and value:
                index_data = {
                    "name": name,
                    "value": value,
                    "change": change,
                    "changePercent": change_percent,
                    "opening": opening,
                    "high": high,
                    "low": low,
                    "trend": trend
                }
                indices.append(index_data)
        
        except Exception as e:
            logger.warning(f"Erreur lors du traitement d'une ligne: {str(e)}")
    
    return indices

def classify_indices(indices):
    """Classe les indices dans les bonnes r√©gions"""
    for index in indices:
        name = index["name"].upper()
        
        # V√©rifier chaque r√©gion
        classified = False
        for region, keywords in CONFIG["regions"].items():
            if any(keyword in name for keyword in keywords):
                MARKET_DATA["indices"][region].append(index)
                classified = True
                break
        
        # Par d√©faut, ajouter √† "other"
        if not classified:
            MARKET_DATA["indices"]["other"].append(index)

def scrape_tab_data(soup, region):
    """R√©cup√®re les donn√©es d'un onglet sp√©cifique (Europe, US, Asie, Autres)"""
    logger.info(f"Traitement de l'onglet: {region}")
    
    # Trouver les tableaux dans cet onglet
    tab_content = soup.select(f"{CONFIG['region_selectors'][region]}-content")
    if tab_content:
        # Si on a trouv√© le contenu de l'onglet, chercher les tableaux dedans
        tables = tab_content[0].find_all('table')
    else:
        # Sinon, chercher tous les tableaux et filtrer par r√©gion
        tables = soup.find_all('table')
    
    logger.info(f"Nombre de tableaux trouv√©s pour {region}: {len(tables)}")
    
    # Traiter chaque tableau
    indices = []
    for i, table in enumerate(tables):
        logger.info(f"Traitement du tableau {i+1}/{len(tables)} pour {region}")
        table_indices = extract_table_data(table)
        indices.extend(table_indices)
    
    return indices

def direct_scrape_approach(html):
    """Approche alternative qui recherche directement tous les tableaux"""
    logger.info("Utilisation de l'approche de scraping directe")
    
    soup = BeautifulSoup(html, 'html.parser')
    all_tables = soup.find_all('table')
    
    logger.info(f"Nombre total de tableaux trouv√©s: {len(all_tables)}")
    
    # Extraire les donn√©es de tous les tableaux
    all_indices = []
    for i, table in enumerate(all_tables):
        logger.info(f"Traitement du tableau {i+1}/{len(all_tables)}")
        indices = extract_table_data(table)
        all_indices.extend(indices)
    
    # Classer les indices
    classify_indices(all_indices)
    
    return len(all_indices) > 0

def get_page_content():
    """R√©cup√®re le contenu de la page avec retries"""
    headers = get_headers()
    max_retries = 3
    retry_delay = 2
    
    for retry in range(max_retries):
        try:
            logger.info(f"Tentative {retry+1}/{max_retries} de r√©cup√©ration de la page")
            response = requests.get(CONFIG["source_url"], headers=headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.warning(f"Erreur lors de la r√©cup√©ration: {str(e)}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
    
    return None

def scrape_market_data():
    """R√©cup√®re et parse la page de Boursorama"""
    logger.info(f"üîç R√©cup√©ration des donn√©es depuis {CONFIG['source_url']}...")
    
    try:
        # R√©cup√©rer le contenu de la page
        html = get_page_content()
        if not html:
            logger.error("Impossible de r√©cup√©rer la page")
            return False
        
        if len(html) < 1000:
            logger.warning(f"Page trop courte ({len(html)} caract√®res), possible erreur")
        
        # Parser le HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # V√©rifier la structure pour voir si on a des onglets
        has_tabs = all(soup.select(selector) for selector in CONFIG["region_selectors"].values())
        
        if has_tabs:
            logger.info("Structure avec onglets d√©tect√©e, traitement par onglet")
            
            # Traiter chaque onglet
            for region in CONFIG["regions"].keys():
                region_indices = scrape_tab_data(soup, region)
                MARKET_DATA["indices"][region].extend(region_indices)
                logger.info(f"Indices trouv√©s pour {region}: {len(region_indices)}")
        else:
            logger.info("Structure sans onglets d√©tect√©e, approche directe")
            if not direct_scrape_approach(html):
                logger.warning("L'approche directe n'a trouv√© aucun indice")
                return False
        
        # Mettre √† jour le compteur
        MARKET_DATA["meta"]["count"] = sum(len(indices) for indices in MARKET_DATA["indices"].values())
        
        logger.info(f"‚úÖ Donn√©es extraites avec succ√®s: {MARKET_DATA['meta']['count']} indices")
        
        # V√©rifier qu'on a assez de donn√©es
        if MARKET_DATA["meta"]["count"] < 5:
            logger.warning(f"‚ö†Ô∏è Trop peu d'indices trouv√©s: {MARKET_DATA['meta']['count']}")
            return False
        
        # Mise √† jour de l'horodatage
        now = datetime.now()
        MARKET_DATA["meta"]["timestamp"] = now.isoformat()
        MARKET_DATA["meta"]["lastUpdated"] = now.isoformat()
        
        # Enregistrer les donn√©es
        save_market_data()
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def save_market_data():
    """Enregistre les donn√©es dans un fichier JSON"""
    try:
        # Cr√©er le dossier data s'il n'existe pas
        data_dir = os.path.dirname(CONFIG["output_path"])
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        
        # √âcrire le fichier JSON
        with open(CONFIG["output_path"], 'w', encoding='utf-8') as f:
            json.dump(MARKET_DATA, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es enregistr√©es dans {CONFIG['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es: {str(e)}")
        return False

def check_existing_data():
    """V√©rifier si un fichier de donn√©es existe d√©j√†"""
    try:
        if os.path.exists(CONFIG["output_path"]):
            logger.info("üìÇ Fichier de donn√©es existant trouv√©")
            return True
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification du fichier existant: {str(e)}")
        return False

def main():
    """Point d'entr√©e principal avec gestion d'erreur robuste"""
    try:
        logger.info("üöÄ D√©marrage du script de scraping des donn√©es de march√©")
        
        # V√©rifier si les donn√©es existent d√©j√†
        has_existing_data = check_existing_data()
        
        # Tenter d'extraire les nouvelles donn√©es
        scraping_success = scrape_market_data()
        
        # Si l'extraction √©choue mais qu'on a des donn√©es existantes, conserver le fichier
        if not scraping_success and has_existing_data:
            logger.info("‚ö†Ô∏è Utilisation des donn√©es existantes car le scraping a √©chou√©")
            sys.exit(0) # Sortie sans erreur pour ne pas faire √©chouer le workflow
        elif not scraping_success and not has_existing_data:
            logger.error("‚ùå Aucune donn√©e existante et √©chec du scraping")
            sys.exit(1) # Sortie avec erreur car on n'a pas de donn√©es
        else:
            logger.info("‚úÖ Scraping termin√© avec succ√®s")
            sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Si une erreur se produit mais que le fichier existe d√©j√†, ne pas faire √©chouer le workflow
        if check_existing_data():
            logger.info("‚ö†Ô∏è Une erreur s'est produite mais les donn√©es existantes seront conserv√©es")
            sys.exit(0)
        else:
            sys.exit(1)

if __name__ == "__main__":
    # D√©sactiver l'avertissement sur les v√©rifications SSL d√©sactiv√©es
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # Lancer le programme
    main()
