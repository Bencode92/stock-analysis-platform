#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction des donn√©es boursi√®res depuis Boursorama
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es
"""

import os
import json
import sys
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "source_url": "https://www.boursorama.com/bourse/indices/internationaux",
    "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "markets.json"),
    # Structure des r√©gions pour la classification des indices
    "regions": {
        "europe": [
            "CAC", "DAX", "FTSE", "IBEX", "MIB", "AEX", "BEL", "SMI", "ATX",
            "OMX", "OMXS", "ISEQ", "PSI", "ATHEX", "OSEBX", "STOXX", "EURO"
        ],
        "us": [
            "DOW", "S&P", "NASDAQ", "RUSSELL", "CBOE", "NYSE", "AMEX"
        ],
        "asia": [
            "NIKKEI", "HANG SENG", "SHANGHAI", "SHENZHEN", "KOSPI", "SENSEX",
            "BSE", "TAIEX", "STRAITS", "JAKARTA", "KLSE", "KOSDAQ", "ASX"
        ],
        "other": [
            "MERVAL", "BOVESPA", "IPC", "IPSA", "COLCAP", "BVLG", "IBC", "CASE",
            "ISE", "TA", "QE", "FTSE/JSE", "MOEX", "MSX30"
        ]
    }
}

# Structure pour les donn√©es
MARKET_DATA = {
    "indices": {
        "europe": [],
        "us": [],
        "asia": [],
        "other": []
    },
    "meta": {
        "source": "Boursorama",
        "url": CONFIG["source_url"],
        "timestamp": datetime.now().isoformat(),
        "count": 0,
        "lastUpdated": datetime.now().isoformat()
    }
}

def classify_index(index_data):
    """Classe un indice dans la bonne r√©gion"""
    name = index_data["name"].upper()
    
    # V√©rifier chaque r√©gion
    for region, keywords in CONFIG["regions"].items():
        if any(keyword in name for keyword in keywords):
            MARKET_DATA["indices"][region].append(index_data)
            return
    
    # Par d√©faut, ajouter √† "other"
    MARKET_DATA["indices"]["other"].append(index_data)

def scrape_market_data():
    """R√©cup√®re et parse la page de Boursorama"""
    logger.info(f"üîç R√©cup√©ration des donn√©es depuis {CONFIG['source_url']}...")
    
    try:
        # En-t√™tes pour simuler un navigateur
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Referer": "https://www.google.com/"
        }
        
        # Faire la requ√™te avec retry
        max_retries = 3
        retry_delay = 2
        
        for retry in range(max_retries):
            try:
                response = requests.get(CONFIG["source_url"], headers=headers, timeout=30, verify=False)
                response.raise_for_status()
                break
            except (requests.RequestException, Exception) as e:
                logger.warning(f"Tentative {retry+1}/{max_retries} √©chou√©e: {str(e)}")
                if retry == max_retries - 1:
                    raise
                time.sleep(retry_delay)
        
        if response.status_code != 200:
            raise Exception(f"Erreur HTTP: {response.status_code}")
        
        html = response.text
        
        # V√©rifier qu'on a bien r√©cup√©r√© du HTML
        if not html or len(html) < 1000 or "<!DOCTYPE html>" not in html:
            raise Exception("R√©ponse HTML invalide")
        
        logger.info("‚úÖ Page r√©cup√©r√©e avec succ√®s")
        
        # Parser le HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # Trouver tous les tableaux
        tables = soup.find_all('table')
        logger.info(f"Nombre de tableaux trouv√©s: {len(tables)}")
        
        # Trouver le tableau des indices
        indices_table = None
        for table in tables:
            headers = [th.text.strip().lower() for th in table.find_all('th')]
            if any(keyword in "".join(headers) for keyword in ['indice', 'dernier', 'var', 'variation']):
                indices_table = table
                logger.info(f"Table des indices trouv√©e")
                break
        
        if not indices_table:
            logger.warning("‚ö†Ô∏è Tableau des indices non trouv√©")
            return False
        
        # Extraire les donn√©es des lignes
        rows = indices_table.find('tbody').find_all('tr') if indices_table.find('tbody') else indices_table.find_all('tr')
        logger.info(f"Nombre de lignes trouv√©es: {len(rows)}")
        
        # Parcourir les lignes
        for row in rows:
            try:
                cells = row.find_all('td')
                
                if len(cells) >= 3:
                    # Extraire le nom de l'indice
                    name_el = row.find('a')
                    name = name_el.text.strip() if name_el else ""
                    
                    # Si pas de lien, essayer la premi√®re ou deuxi√®me cellule
                    if not name and len(cells) > 0:
                        name = cells[0].text.strip()
                    if not name and len(cells) > 1:
                        name = cells[1].text.strip()
                    
                    # V√©rifier que c'est un nom d'indice valide
                    if name and len(name) > 1 and not re.match(r'^\d+', name):
                        # Extraire les valeurs
                        value = ""
                        change = ""
                        change_percent = ""
                        opening = ""
                        high = ""
                        low = ""
                        
                        # Parcourir les cellules pour extraire les donn√©es
                        for i in range(1, len(cells)):
                            text = cells[i].text.strip()
                            
                            # Si c'est un pourcentage, c'est probablement la variation en %
                            if "%" in text and not change_percent:
                                change_percent = text
                                continue
                            
                            # Si c'est un nombre avec +/-, c'est probablement la variation absolue
                            if ('+' in text or '-' in text) and re.search(r'[0-9]', text) and not change:
                                change = text
                                continue
                            
                            # Si c'est un nombre et qu'on n'a pas encore de valeur
                            if re.search(r'[0-9]', text) and not value:
                                value = text
                                continue
                            
                            # Si c'est un nombre et qu'on a d√©j√† une valeur mais pas d'ouverture
                            if re.search(r'[0-9]', text) and value and not opening:
                                opening = text
                                continue
                            
                            # Si c'est un nombre et qu'on a d√©j√† une valeur et une ouverture mais pas de plus haut
                            if re.search(r'[0-9]', text) and value and opening and not high:
                                high = text
                                continue
                            
                            # Si c'est un nombre et qu'on a d√©j√† une valeur, une ouverture et un plus haut mais pas de plus bas
                            if re.search(r'[0-9]', text) and value and opening and high and not low:
                                low = text
                                continue
                        
                        # Cr√©er l'indice uniquement si on a au moins une valeur
                        if value:
                            # D√©terminer la tendance (hausse/baisse)
                            trend = "down" if (change and '-' in change) or (change_percent and '-' in change_percent) else "up"
                            
                            # Cr√©er l'objet indice
                            index_data = {
                                "name": name,
                                "value": value,
                                "change": change or "",
                                "changePercent": change_percent or "",
                                "opening": opening or "",
                                "high": high or "",
                                "low": low or "",
                                "trend": trend
                            }
                            
                            # Classer l'indice
                            classify_index(index_data)
            except Exception as e:
                logger.warning(f"Erreur lors du traitement d'une ligne: {str(e)}")
        
        # Mettre √† jour le compteur
        MARKET_DATA["meta"]["count"] = sum(len(indices) for indices in MARKET_DATA["indices"].values())
        
        logger.info(f"‚úÖ Donn√©es extraites avec succ√®s: {MARKET_DATA['meta']['count']} indices")
        
        # V√©rifier qu'on a assez de donn√©es
        if MARKET_DATA["meta"]["count"] < 5:
            logger.warning(f"‚ö†Ô∏è Trop peu d'indices trouv√©s: {MARKET_DATA['meta']['count']}")
            return False
        
        # Mise √† jour de l'horodatage
        now = datetime.now()
        MARKET_DATA["meta"]["timestamp"] = now.isoformat()
        MARKET_DATA["meta"]["lastUpdated"] = now.isoformat()
        
        # Enregistrer les donn√©es
        save_market_data()
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def save_market_data():
    """Enregistre les donn√©es dans un fichier JSON"""
    try:
        # Cr√©er le dossier data s'il n'existe pas
        data_dir = os.path.dirname(CONFIG["output_path"])
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        
        # √âcrire le fichier JSON
        with open(CONFIG["output_path"], 'w', encoding='utf-8') as f:
            json.dump(MARKET_DATA, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es enregistr√©es dans {CONFIG['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es: {str(e)}")
        return False

def check_existing_data():
    """V√©rifier si un fichier de donn√©es existe d√©j√†"""
    try:
        if os.path.exists(CONFIG["output_path"]):
            logger.info("üìÇ Fichier de donn√©es existant trouv√©")
            return True
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification du fichier existant: {str(e)}")
        return False

def main():
    """Point d'entr√©e principal avec gestion d'erreur robuste"""
    try:
        logger.info("üöÄ D√©marrage du script de scraping des donn√©es de march√©")
        
        # V√©rifier si les donn√©es existent d√©j√†
        has_existing_data = check_existing_data()
        
        # Tenter d'extraire les nouvelles donn√©es
        scraping_success = scrape_market_data()
        
        # Si l'extraction √©choue mais qu'on a des donn√©es existantes, conserver le fichier
        if not scraping_success and has_existing_data:
            logger.info("‚ö†Ô∏è Utilisation des donn√©es existantes car le scraping a √©chou√©")
            sys.exit(0) # Sortie sans erreur pour ne pas faire √©chouer le workflow
        elif not scraping_success and not has_existing_data:
            logger.error("‚ùå Aucune donn√©e existante et √©chec du scraping")
            sys.exit(1) # Sortie avec erreur car on n'a pas de donn√©es
        else:
            logger.info("‚úÖ Scraping termin√© avec succ√®s")
            sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Si une erreur se produit mais que le fichier existe d√©j√†, ne pas faire √©chouer le workflow
        if check_existing_data():
            logger.info("‚ö†Ô∏è Une erreur s'est produite mais les donn√©es existantes seront conserv√©es")
            sys.exit(0)
        else:
            sys.exit(1)

if __name__ == "__main__":
    # D√©sactiver l'avertissement sur les v√©rifications SSL d√©sactiv√©es
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # Lancer le programme
    main()
