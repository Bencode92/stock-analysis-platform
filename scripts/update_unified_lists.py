#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script unifi√© d'extraction des donn√©es des actions du NASDAQ Composite et du DJ STOXX 600
Combine les fonctionnalit√©s de scrape_lists.py et scrape_stoxx.py
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es

IMPORTANT: Ce script met √† jour UNIQUEMENT les fichiers suivants:
- data/lists.json (donn√©es NASDAQ et STOXX unifi√©es)
- data/update_summary.json (r√©sum√© de la mise √† jour)
- data/global_top_performers.json (classement global NASDAQ + STOXX)
- data/top_nasdaq_performers.json (top performers NASDAQ)
- data/top_stoxx_performers.json (top performers STOXX)

Il ne modifie PAS le fichier markets.json qui est g√©r√© par le script scrape_markets.py
et le workflow 'Update Markets Data Only'.
"""

import os
import json
import sys
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "base_url": "https://www.boursorama.com/bourse/actions/cotations/international",
    "nasdaq": {
        "country": "1",  # √âtats-Unis
        "market": "$COMPX",  # NASDAQ Composite
        "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "lists.json"),
    },
    "stoxx": {
        "country": "EU",  # Europe
        "market": "2cSXXP",  # DJ STOXX 600
        "output_dir": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data"),
    },
    "alphabet": list("ABCDEFGHIJKLMNOPQRSTUVWXYZ"),
    "sleep_time": 1.5  # D√©lai entre les requ√™tes pour √©viter la d√©tection de bot
}

# Seuils pour les outliers
MAX_DAILY_GAIN_PERCENTAGE = 100.0  # Hausse journali√®re maximale autoris√©e
MIN_DAILY_LOSS_PERCENTAGE = -100.0  # Baisse journali√®re minimale autoris√©e
MIN_YTD_LOSS_PERCENTAGE = -100.0    # Baisse YTD minimale autoris√©e
# Remarque: Pas de limite pour les hausses YTD, car elles peuvent l√©gitimement d√©passer 100%

def get_headers():
    """Cr√©e des en-t√™tes HTTP pour √©viter la d√©tection de bot"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/"
    }

#
# Fonctions communes
#
def extract_stock_data(row):
    """Extrait les donn√©es d'une action √† partir d'une ligne de tableau"""
    try:
        cells = row.find_all('td')
        if not cells or len(cells) < 8:
            return None
            
        # R√©cup√©rer le libell√© et le lien
        libelle_cell = cells[0]
        libelle_link = libelle_cell.find('a')
        libelle = libelle_link.text.strip() if libelle_link else ""
        link = libelle_link.get('href') if libelle_link else ""
        
        # R√©cup√©rer le cours et les autres valeurs
        dernier = cells[1].text.strip()
        variation = cells[2].text.strip()
        ouverture = cells[3].text.strip()
        plus_haut = cells[4].text.strip()
        plus_bas = cells[5].text.strip()
        var_ytd = cells[6].text.strip()
        volume = cells[7].text.strip()
        
        # D√©terminer la tendance en fonction de la variation
        trend = "up" if variation and not variation.startswith('-') and variation != "0,00%" else "down"
        if variation == "0,00%":
            trend = "neutral"
            
        # Cr√©er l'objet stock
        stock_data = {
            "symbol": link.split('/')[-1] if link else "",
            "name": libelle,
            "last": dernier,
            "change": variation,
            "open": ouverture,
            "high": plus_haut,
            "low": plus_bas,
            "ytd": var_ytd,
            "volume": volume,
            "trend": trend,
            "link": f"https://www.boursorama.com{link}" if link else ""
        }
        
        return stock_data
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des donn√©es d'une action: {str(e)}")
        return None

def parse_percentage(value_str):
    """
    Convertit une cha√Æne de pourcentage en valeur num√©rique
    Compatible avec diff√©rents formats (virgule/point, avec/sans %)
    """
    if not value_str or value_str == "-":
        return 0.0
    
    # Nettoyer la cha√Æne pour extraire le nombre
    cleaned = value_str.replace('%', '').replace(',', '.').replace(' ', '')
    try:
        return float(cleaned)
    except:
        return 0.0

def get_top_performers(stocks, field='change', reverse=True, limit=10):
    """
    R√©cup√®re les top/bottom performers bas√©s sur un champ donn√©,
    avec d√©duplication stricte bas√©e sur le nom de l'action.
    
    Args:
        stocks (list): Liste d'actions
        field (str): Champ pour le tri ('change' ou 'ytd')
        reverse (bool): True pour ordre d√©croissant, False pour croissant
        limit (int): Nombre maximum d'√©l√©ments √† retourner
        
    Returns:
        list: Top performers d√©dupliqu√©s
    """
    try:
        # Filtrer les actions avec une valeur valide pour le champ sp√©cifi√©
        valid_stocks = []
        
        for stock in stocks:
            if not stock.get(field) or stock.get(field) == "-" or not stock.get("name"):
                continue
                
            # Convertir le pourcentage en valeur num√©rique
            percentage = parse_percentage(stock.get(field, "0"))
            
            # Appliquer les filtres selon les crit√®res demand√©s
            if field == 'change':  # Variations journali√®res
                # V√©rifier les seuils pour les variations journali√®res
                if reverse and percentage > MAX_DAILY_GAIN_PERCENTAGE:
                    logger.info(f"Outlier ignor√© (hausse journali√®re > {MAX_DAILY_GAIN_PERCENTAGE}%): {stock['name']} avec {stock[field]}")
                    continue
                elif not reverse and percentage < MIN_DAILY_LOSS_PERCENTAGE:
                    logger.info(f"Outlier ignor√© (baisse journali√®re < {MIN_DAILY_LOSS_PERCENTAGE}%): {stock['name']} avec {stock[field]}")
                    continue
            elif field == 'ytd':  # Variations YTD
                # Pas de limite sup√©rieure pour les hausses YTD
                if not reverse and percentage < MIN_YTD_LOSS_PERCENTAGE:
                    logger.info(f"Outlier ignor√© (baisse YTD < {MIN_YTD_LOSS_PERCENTAGE}%): {stock['name']} avec {stock[field]}")
                    continue
            
            # Stock passe les filtres, on l'ajoute avec sa valeur num√©rique
            valid_stocks.append((stock, percentage))
        
        # Trier les actions en fonction de la valeur num√©rique
        sorted_stocks = sorted(valid_stocks, key=lambda x: x[1], reverse=reverse)
        
        # D√©duplication stricte bas√©e sur le nom
        unique_stocks = []
        seen_names = set()
        
        for stock, _ in sorted_stocks:
            name = stock.get("name", "")
            if name and name not in seen_names:
                seen_names.add(name)
                unique_stocks.append(stock)
                
                # S'arr√™ter une fois que nous avons atteint la limite
                if len(unique_stocks) >= limit:
                    break
        
        # V√©rifier si nous avons exactement le nombre demand√©
        if len(unique_stocks) < limit:
            logger.warning(f"Attention: seulement {len(unique_stocks)}/{limit} actions uniques pour le top {field}, {'hausse' if reverse else 'baisse'}")
        
        return unique_stocks
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des top performers: {str(e)}")
        return []

#
# Fonctions pour NASDAQ
#
def get_nasdaq_url(letter, page=1):
    """G√©n√®re l'URL pour obtenir la liste des actions NASDAQ pour une lettre donn√©e"""
    # G√©n√©rer un timestamp similaire √† celui utilis√© par Boursorama
    pagination_timestamp = int(time.time())
    
    # Construire le chemin d'URL avec la page dans le chemin
    base_path = f"{CONFIG['base_url']}/page-{page}"
    
    # Param√®tres de requ√™te
    params = {
        "international_quotation_az_filter[country]": CONFIG["nasdaq"]["country"],
        "international_quotation_az_filter[market]": CONFIG["nasdaq"]["market"],
        "international_quotation_az_filter[letter]": letter,
        "international_quotation_az_filter[filter]": "",
        f"pagination_{pagination_timestamp}": ""
    }
    
    # Construire la cha√Æne de requ√™te
    query_params = "&".join([f"{k}={v}" for k, v in params.items()])
    
    # Retourner l'URL compl√®te
    return f"{base_path}?{query_params}"

def scrape_nasdaq_page(letter, page=1):
    """Scrape une page de la liste des actions NASDAQ"""
    url = get_nasdaq_url(letter, page)
    logger.info(f"R√©cup√©ration des donn√©es NASDAQ pour la lettre {letter}, page {page}: {url}")
    
    try:
        response = requests.get(url, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Trouver le tableau des actions
        table = soup.find('table', class_='c-table')
        if not table:
            logger.warning(f"Aucun tableau trouv√© pour la lettre {letter}, page {page}")
            return [], False
            
        # Trouver toutes les lignes de donn√©es (ignorer l'en-t√™te)
        rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
        
        stocks = []
        for row in rows:
            stock_data = extract_stock_data(row)
            if stock_data:
                stocks.append(stock_data)
                
        logger.info(f"Trouv√© {len(stocks)} actions NASDAQ pour la lettre {letter}, page {page}")
        
        # MISE √Ä JOUR: D√©tection de pagination pour le nouveau format
        has_next_page = False
        
        # Recherche du bloc de pagination (nouvelle structure)
        pagination = soup.select_one('.c-block-pagination__content')
        if pagination:
            # Rechercher le bouton "suivant"
            next_button = pagination.select_one('a.c-block-pagination__next-btn, a.c-pagination__item--next')
            has_next_page = next_button is not None and not ('disabled' in next_button.get('class', []))
            
            # Si nous ne trouvons pas de bouton sp√©cifique "suivant", v√©rifier s'il y a une page avec un num√©ro plus √©lev√©
            if not has_next_page:
                page_links = pagination.select('a.c-block-pagination__link')
                for link in page_links:
                    if link.text.isdigit() and int(link.text) > page:
                        has_next_page = True
                        break
        
        # V√©rification de l'ancienne structure de pagination au cas o√π
        if not has_next_page:
            pagination_old = soup.find('ul', class_='c-pagination')
            if pagination_old:
                next_button = pagination_old.find('li', class_='c-pagination__item--next')
                has_next_page = next_button and not next_button.has_attr('disabled')
        
        # V√©rification suppl√©mentaire: si nous avons des actions mais pas de pagination, 
        # essayons quand m√™me la page suivante
        if len(stocks) > 0 and not has_next_page and len(stocks) >= 20:  # 20 est la taille de page courante
            logger.info(f"Aucune pagination d√©tect√©e mais {len(stocks)} actions trouv√©es, on essaie la page suivante")
            has_next_page = True
            
        return stocks, has_next_page
        
    except Exception as e:
        logger.error(f"Erreur lors du scraping NASDAQ pour lettre {letter}, page {page}: {str(e)}")
        return [], False

def scrape_all_nasdaq_stocks():
    """Scrape toutes les actions du NASDAQ Composite"""
    all_stocks = []
    
    for letter in CONFIG["alphabet"]:
        page = 1
        has_next_page = True
        max_pages_per_letter = 10  # Limite de s√©curit√© pour √©viter les boucles infinies
        
        while has_next_page and page <= max_pages_per_letter:
            stocks, has_next_page = scrape_nasdaq_page(letter, page)
            all_stocks.extend(stocks)
            
            # Si pas de page suivante, sortir de la boucle
            if not has_next_page:
                break
                
            # Passer √† la page suivante
            page += 1
            
            # Attente pour √©viter de surcharger le serveur
            time.sleep(CONFIG["sleep_time"])
        
        if page > max_pages_per_letter:
            logger.warning(f"Atteint la limite de {max_pages_per_letter} pages pour la lettre {letter}")
    
    return all_stocks

#
# Fonctions pour STOXX
#
def get_stoxx_url(page=1, letter=""):
    """G√©n√®re l'URL pour obtenir la liste des actions STOXX pour une page et lettre donn√©es"""
    # G√©n√©rer un timestamp similaire √† celui utilis√© par Boursorama
    pagination_timestamp = int(time.time())
    
    # Construire le chemin d'URL avec la page dans le chemin
    base_path = f"{CONFIG['base_url']}/page-{page}"
    
    params = {
        "international_quotation_az_filter[country]": CONFIG["stoxx"]["country"],
        "international_quotation_az_filter[market]": CONFIG["stoxx"]["market"],
        "international_quotation_az_filter[letter]": letter,
        "international_quotation_az_filter[filter]": "",
        f"pagination_{pagination_timestamp}": ""
    }
    query_params = "&".join([f"{k}={v}" for k, v in params.items()])
    return f"{base_path}?{query_params}"

def scrape_stoxx_page(page=1, letter=""):
    """Scrape une page de la liste des actions du STOXX 600 pour une lettre donn√©e"""
    url = get_stoxx_url(page, letter)
    logger.info(f"R√©cup√©ration des donn√©es STOXX pour la lettre {letter}, page {page}: {url}")
    
    try:
        response = requests.get(url, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Trouver le tableau des actions
        table = soup.find('table', class_='c-table')
        if not table:
            logger.warning(f"Aucun tableau trouv√© pour la lettre STOXX {letter}, page {page}")
            return [], page, 1, False
            
        # Trouver toutes les lignes de donn√©es (ignorer l'en-t√™te)
        rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
        
        stocks = []
        for row in rows:
            stock_data = extract_stock_data(row)
            if stock_data:
                stocks.append(stock_data)
                
        logger.info(f"Trouv√© {len(stocks)} actions STOXX pour la lettre {letter}, page {page}")
        
        # D√©terminer le nombre total de pages
        total_pages = 1
        current_page = page
        pagination = soup.select_one('.c-block-pagination__content')
        if pagination:
            # Extraire le nombre de pages
            page_items = pagination.select('a.c-block-pagination__link')
            for item in page_items:
                if item.text.isdigit():
                    page_num = int(item.text)
                    total_pages = max(total_pages, page_num)
            
            # R√©cup√©rer la page actuelle
            current_item = pagination.select_one('span.c-block-pagination__link--current') 
            if current_item and current_item.text.isdigit():
                current_page = int(current_item.text)
        
        # MISE √Ä JOUR: D√©tection de pagination pour le nouveau format
        has_next_page = False
        
        # Recherche du bloc de pagination (nouvelle structure)
        if pagination:
            # Rechercher le bouton "suivant"
            next_button = pagination.select_one('a.c-block-pagination__next-btn, a.c-pagination__item--next')
            has_next_page = next_button is not None and not ('disabled' in next_button.get('class', []))
            
            # Si nous ne trouvons pas de bouton sp√©cifique "suivant", v√©rifier s'il y a une page avec un num√©ro plus √©lev√©
            if not has_next_page:
                page_links = pagination.select('a.c-block-pagination__link')
                for link in page_links:
                    if link.text.isdigit() and int(link.text) > page:
                        has_next_page = True
                        break
        
        # V√©rification de l'ancienne structure de pagination au cas o√π
        if not has_next_page:
            pagination_old = soup.find('ul', class_='c-pagination')
            if pagination_old:
                next_button = pagination_old.find('li', class_='c-pagination__item--next')
                has_next_page = next_button and not next_button.has_attr('disabled')
        
        # V√©rification suppl√©mentaire: si nous avons des actions mais pas de pagination, 
        # essayons quand m√™me la page suivante
        if len(stocks) > 0 and not has_next_page and len(stocks) >= 20:  # 20 est la taille de page courante
            logger.info(f"Aucune pagination d√©tect√©e mais {len(stocks)} actions trouv√©es, on essaie la page suivante")
            has_next_page = True
        
        return stocks, current_page, total_pages, has_next_page
        
    except Exception as e:
        logger.error(f"Erreur lors du scraping STOXX lettre {letter}, page {page}: {str(e)}")
        return [], page, 1, False

def scrape_all_stoxx():
    """Scrape toutes les actions du STOXX 600 par lettre et par page"""
    all_stocks = []
    max_pages_per_letter = 10  # Limite de s√©curit√©
    total_pages_overall = 0
    total_stocks = 0
    
    try:
        # Parcourir chaque lettre de l'alphabet
        for letter in CONFIG["alphabet"]:
            logger.info(f"üìä R√©cup√©ration des donn√©es STOXX pour la lettre {letter}...")
            
            # R√©cup√©rer les donn√©es de la premi√®re page pour cette lettre
            letter_stocks, current_page, total_pages, has_next_page = scrape_stoxx_page(1, letter)
            
            if letter_stocks:
                all_stocks.extend(letter_stocks)
                total_stocks += len(letter_stocks)
                
                # Mettre √† jour le nombre total de pages pour cette lettre
                letter_page_count = 1
                
                # R√©cup√©rer les pages suivantes pour cette lettre
                page = 2
                while has_next_page and page <= min(total_pages + 1, max_pages_per_letter + 1):
                    # Attente pour √©viter de surcharger le serveur
                    time.sleep(CONFIG["sleep_time"])
                    
                    # R√©cup√©rer les donn√©es de la page
                    page_stocks, _, _, page_has_next = scrape_stoxx_page(page, letter)
                    
                    if page_stocks:
                        all_stocks.extend(page_stocks)
                        total_stocks += len(page_stocks)
                        letter_page_count += 1
                    
                    # Mettre √† jour le flag pour la prochaine page
                    has_next_page = page_has_next
                    
                    # Passer √† la page suivante
                    page += 1
                    
                # Mettre √† jour le nombre total de pages global
                total_pages_overall = max(total_pages_overall, letter_page_count)
            
            # Attente entre les lettres pour √©viter de surcharger le serveur
            time.sleep(CONFIG["sleep_time"])
        
        logger.info(f"‚úÖ Scraping STOXX termin√© avec succ√®s: {total_stocks} actions r√©cup√©r√©es sur toutes les lettres")
        return {
            "status": "success",
            "pages": total_pages_overall,
            "stocks": total_stocks,
            "all_stocks": all_stocks
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scraping STOXX: {str(e)}")
        return {
            "status": "error",
            "message": str(e),
            "pages": 0,
            "stocks": 0,
            "all_stocks": []
        }

def create_global_rankings(nasdaq_stocks, stoxx_result):
    """Cr√©e un classement global combin√© NASDAQ + STOXX et le sauvegarde"""
    logger.info("üåê Cr√©ation du classement global NASDAQ + STOXX...")
    
    try:
        # Ajouter l'information du march√© pour chaque action NASDAQ
        nasdaq_with_source = []
        for stock in nasdaq_stocks:
            stock_with_source = stock.copy()
            stock_with_source['market'] = 'NASDAQ'
            stock_with_source['marketIcon'] = '<i class="fas fa-flag-usa text-xs ml-1" title="NASDAQ"></i>'
            nasdaq_with_source.append(stock_with_source)
        
        # R√©cup√©rer toutes les actions STOXX depuis stoxx_result
        all_stoxx_stocks = stoxx_result.get('all_stocks', [])

        # Ajouter l'information du march√© pour chaque action STOXX
        stoxx_with_source = []
        for stock in all_stoxx_stocks:
            stock_with_source = stock.copy()
            stock_with_source['market'] = 'STOXX'
            stock_with_source['marketIcon'] = '<i class="fas fa-globe-europe text-xs ml-1" title="STOXX"></i>'
            stoxx_with_source.append(stock_with_source)
        
        # Combiner toutes les actions
        all_stocks = nasdaq_with_source + stoxx_with_source
        
        # Utiliser get_top_performers avec la fonction parse_percentage pour d√©duplication
        daily_best = get_top_performers(all_stocks, 'change', True, 10)
        daily_worst = get_top_performers(all_stocks, 'change', False, 10)
        ytd_best = get_top_performers(all_stocks, 'ytd', True, 10)
        ytd_worst = get_top_performers(all_stocks, 'ytd', False, 10)
        
        # Cr√©er la structure de donn√©es pour le classement global
        global_rankings = {
            "daily": {
                "best": daily_best,
                "worst": daily_worst
            },
            "ytd": {
                "best": ytd_best,
                "worst": ytd_worst
            },
            "meta": {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "count": len(all_stocks),
                "description": "Classement global combin√© (NASDAQ + STOXX)"
            }
        }
        
        # Sauvegarder dans un fichier JSON
        output_path = os.path.join(CONFIG["stoxx"]["output_dir"], "global_top_performers.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(global_rankings, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Classement global enregistr√© dans {output_path}")
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation du classement global: {str(e)}")
        return False

def create_market_top_performers_file(stocks, market_name, timestamp):
    """
    Cr√©e un fichier JSON s√©par√© pour les top performers d'un march√© sp√©cifique.
    
    Args:
        stocks (list): Liste des actions du march√©
        market_name (str): Nom du march√© (NASDAQ ou STOXX)
        timestamp (str): Horodatage pour les m√©tadonn√©es
    """
    try:
        # Ajouter/v√©rifier les indicateurs de march√© pour chaque action
        for stock in stocks:
            if "market" not in stock:
                stock["market"] = market_name
            
            # Ajouter l'ic√¥ne du march√© si pas d√©j√† pr√©sente
            if "marketIcon" not in stock:
                if market_name == "NASDAQ":
                    stock["marketIcon"] = '<i class="fas fa-flag-usa text-xs ml-1" title="NASDAQ"></i>'
                else:
                    stock["marketIcon"] = '<i class="fas fa-globe-europe text-xs ml-1" title="STOXX"></i>'
        
        # R√©cup√©rer les tops avec notre fonction am√©lior√©e de d√©duplication
        daily_best = get_top_performers(stocks, 'change', True, 10)
        daily_worst = get_top_performers(stocks, 'change', False, 10)
        ytd_best = get_top_performers(stocks, 'ytd', True, 10)
        ytd_worst = get_top_performers(stocks, 'ytd', False, 10)
        
        # Structure du fichier JSON
        market_tops = {
            "daily": {
                "best": daily_best,
                "worst": daily_worst
            },
            "ytd": {
                "best": ytd_best,
                "worst": ytd_worst
            },
            "meta": {
                "timestamp": timestamp,
                "count": len(stocks),
                "description": f"Top performers du march√© {market_name}"
            }
        }
        
        # Nom du fichier
        filename = f"top_{market_name.lower()}_performers.json"
        file_path = os.path.join(CONFIG["stoxx"]["output_dir"], filename)
        
        # √âcrire le fichier JSON
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(market_tops, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Fichier {filename} cr√©√© avec succ√®s")
        
        # Valider le contenu g√©n√©r√©
        validate_top_performers(market_tops, market_name)
        
        return market_tops
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation du fichier top performers pour {market_name}: {str(e)}")
        return None

def validate_top_performers(data, market_name):
    """
    V√©rifie que les tops performers sont correctement g√©n√©r√©s
    
    Args:
        data (dict): Donn√©es des top performers
        market_name (str): Nom du march√© pour les logs
        
    Returns:
        bool: True si valide, False sinon
    """
    validation_issues = []
    
    # V√©rifier la pr√©sence des cat√©gories
    categories = [("daily", "best"), ("daily", "worst"), ("ytd", "best"), ("ytd", "worst")]
    for cat1, cat2 in categories:
        if cat1 not in data or cat2 not in data[cat1]:
            validation_issues.append(f"Cat√©gorie manquante: {cat1}.{cat2} dans {market_name}")
            continue
            
        # V√©rifier le nombre d'√©l√©ments (devrait √™tre 10)
        items = data[cat1][cat2]
        if len(items) != 10:
            validation_issues.append(f"Nombre d'√©l√©ments incorrect: {len(items)}/10 dans {market_name}.{cat1}.{cat2}")
        
        # V√©rifier l'unicit√© des noms
        names = [item.get("name", "") for item in items]
        unique_names = set(names)
        if len(unique_names) != len(names):
            validation_issues.append(f"Doublons d√©tect√©s dans {market_name}.{cat1}.{cat2}")
    
    # Journaliser les probl√®mes ou confirmer la validation
    if validation_issues:
        for issue in validation_issues:
            logger.warning(issue)
        return False
    else:
        logger.info(f"‚úÖ Validation des tops performers {market_name} r√©ussie")
        return True

def ensure_data_directory():
    """S'assure que le r√©pertoire de donn√©es existe"""
    data_dir = os.path.dirname(CONFIG["nasdaq"]["output_path"])
    if not os.path.exists(data_dir):
        os.makedirs(data_dir, exist_ok=True)
        logger.info(f"‚úÖ R√©pertoire de donn√©es cr√©√©: {data_dir}")

def verify_no_markets_conflict():
    """V√©rifie que ce script ne modifie pas le fichier markets.json"""
    markets_file = os.path.join(os.path.dirname(CONFIG["nasdaq"]["output_path"]), "markets.json")
    if os.path.exists(markets_file):
        logger.info(f"‚úÖ V√©rification: Le fichier markets.json ne sera pas modifi√© par ce script")
    return True

def main():
    """Point d'entr√©e principal"""
    try:
        logger.info("üöÄ D√©marrage du script unifi√© d'extraction des donn√©es NASDAQ et STOXX")

        ensure_data_directory()
        verify_no_markets_conflict()

        # Cr√©er la structure de base pour les deux march√©s
        combined_data = {
            "nasdaq": {
                "indices": {letter: [] for letter in "abcdefghijklmnopqrstuvwxyz"},
                "top_performers": {
                    "daily": {"best": [], "worst": []},
                    "ytd": {"best": [], "worst": []}
                },
                "meta": {
                    "source": "Boursorama",
                    "description": "Actions du NASDAQ Composite (√âtats-Unis)",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "count": 0
                }
            },
            "stoxx": {
                "indices": {letter: [] for letter in "abcdefghijklmnopqrstuvwxyz"},
                "top_performers": {
                    "daily": {"best": [], "worst": []},
                    "ytd": {"best": [], "worst": []}
                },
                "meta": {
                    "source": "Boursorama",
                    "description": "Actions du DJ STOXX 600 (Europe)",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "count": 0,
                    "pagination": {
                        "currentPage": 1,
                        "totalPages": 1
                    }
                }
            }
        }

        # Extraction des donn√©es NASDAQ
        logger.info("üìä D√©but du scraping NASDAQ...")
        nasdaq_stocks = scrape_all_nasdaq_stocks()
        
        if nasdaq_stocks:
            # Organiser par lettre alphab√©tique
            nasdaq_by_letter = {letter: [] for letter in "abcdefghijklmnopqrstuvwxyz"}
            for stock in nasdaq_stocks:
                if stock.get("name"):
                    first_letter = stock["name"][0].lower()
                    if first_letter in nasdaq_by_letter:
                        nasdaq_by_letter[first_letter].append(stock)
            
            # Mettre √† jour les donn√©es NASDAQ
            combined_data["nasdaq"]["indices"] = nasdaq_by_letter
            combined_data["nasdaq"]["top_performers"]["daily"]["best"] = get_top_performers(nasdaq_stocks, "change", True)
            combined_data["nasdaq"]["top_performers"]["daily"]["worst"] = get_top_performers(nasdaq_stocks, "change", False)
            combined_data["nasdaq"]["top_performers"]["ytd"]["best"] = get_top_performers(nasdaq_stocks, "ytd", True)
            combined_data["nasdaq"]["top_performers"]["ytd"]["worst"] = get_top_performers(nasdaq_stocks, "ytd", False)
            combined_data["nasdaq"]["meta"]["count"] = len(nasdaq_stocks)
            combined_data["nasdaq"]["meta"]["timestamp"] = datetime.now(timezone.utc).isoformat()

        # Extraction des donn√©es STOXX
        logger.info("üìä D√©but du scraping STOXX...")
        stoxx_result = scrape_all_stoxx()
        stoxx_stocks = stoxx_result.get("all_stocks", [])
        
        if stoxx_stocks:
            # Organiser par lettre alphab√©tique
            stoxx_by_letter = {letter: [] for letter in "abcdefghijklmnopqrstuvwxyz"}
            for stock in stoxx_stocks:
                if stock.get("name"):
                    first_letter = stock["name"][0].lower()
                    if first_letter in stoxx_by_letter:
                        stoxx_by_letter[first_letter].append(stock)
            
            # Mettre √† jour les donn√©es STOXX
            combined_data["stoxx"]["indices"] = stoxx_by_letter
            combined_data["stoxx"]["top_performers"]["daily"]["best"] = get_top_performers(stoxx_stocks, "change", True)
            combined_data["stoxx"]["top_performers"]["daily"]["worst"] = get_top_performers(stoxx_stocks, "change", False)
            combined_data["stoxx"]["top_performers"]["ytd"]["best"] = get_top_performers(stoxx_stocks, "ytd", True)
            combined_data["stoxx"]["top_performers"]["ytd"]["worst"] = get_top_performers(stoxx_stocks, "ytd", False)
            combined_data["stoxx"]["meta"]["count"] = len(stoxx_stocks)
            combined_data["stoxx"]["meta"]["timestamp"] = datetime.now(timezone.utc).isoformat()
            
            # Si nous avons des informations de pagination
            if stoxx_result.get("pages"):
                combined_data["stoxx"]["meta"]["pagination"]["totalPages"] = stoxx_result.get("pages", 1)

        # Chemin explicite pour lists.json
        lists_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "lists.json")
        logger.info(f"üìù Sauvegarde des donn√©es combin√©es dans: {lists_path}")

        # V√©rifier que les donn√©es √† √©crire sont bien structur√©es
        logger.info(f"üìä Structure des donn√©es: {len(combined_data.keys())} march√©s, " 
                  f"NASDAQ: {combined_data['nasdaq']['meta']['count']} actions, "
                  f"STOXX: {combined_data['stoxx']['meta']['count']} actions")

        # Sauvegarder les donn√©es dans lists.json avec gestion d'erreur
        try:
            with open(lists_path, 'w', encoding='utf-8') as f:
                json.dump(combined_data, f, ensure_ascii=False, indent=2)
            logger.info(f"‚úÖ Donn√©es sauvegard√©es avec succ√®s dans {lists_path}")
        except Exception as e:
            logger.error(f"‚ùå ERREUR lors de la sauvegarde dans lists.json: {str(e)}")
            import traceback
            traceback.print_exc()

        # Cr√©er le classement global pour compatibilit√©
        if nasdaq_stocks and stoxx_stocks:
            logger.info("üìä Cr√©ation du classement global NASDAQ + STOXX...")
            create_global_rankings(nasdaq_stocks, stoxx_result)

        # Cr√©er des fichiers s√©par√©s pour les top performers de chaque march√©
        timestamp_str = datetime.now(timezone.utc).isoformat()

        # Cr√©ation du fichier top_nasdaq_performers.json
        if nasdaq_stocks:
            logger.info(f"üìä Cr√©ation du fichier top_nasdaq_performers.json avec d√©duplication stricte...")
            create_market_top_performers_file(nasdaq_stocks, "NASDAQ", timestamp_str)

        # Cr√©ation du fichier top_stoxx_performers.json
        if stoxx_stocks:
            logger.info(f"üìä Cr√©ation du fichier top_stoxx_performers.json avec d√©duplication stricte...")
            create_market_top_performers_file(stoxx_stocks, "STOXX", timestamp_str)

        # R√©sum√© de la mise √† jour
        result_summary = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "nasdaq": {
                "count": len(nasdaq_stocks),
                "status": "success" if nasdaq_stocks else "error"
            },
            "stoxx": {
                "count": len(stoxx_stocks),
                "status": "success" if stoxx_stocks else "error"
            },
            "combined_file": "lists.json",
            "global_ranking": {
                "status": "success" if nasdaq_stocks and stoxx_stocks else "error",
                "file": "global_top_performers.json"
            },
            "market_tops": {
                "nasdaq": "top_nasdaq_performers.json",
                "stoxx": "top_stoxx_performers.json"
            }
        }

        summary_path = os.path.join(CONFIG["stoxx"]["output_dir"], "update_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(result_summary, f, ensure_ascii=False, indent=2)

        logger.info(f"üìä R√©sum√©: {json.dumps(result_summary, indent=2)}")
        logger.info("‚úÖ Script unifi√© termin√© avec succ√®s")

        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()