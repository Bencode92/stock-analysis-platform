#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script unifi√© d'extraction des donn√©es des actions du NASDAQ Composite et du DJ STOXX 600
Combine les fonctionnalit√©s de scrape_lists.py et scrape_stoxx.py
Utilis√© par GitHub Actions pour mettre √† jour r√©guli√®rement les donn√©es

IMPORTANT: Ce script met √† jour UNIQUEMENT les fichiers suivants:
- data/lists.json (donn√©es NASDAQ)
- data/stoxx_page_*.json (donn√©es STOXX)
- data/update_summary.json (r√©sum√© de la mise √† jour)

Il ne modifie PAS le fichier markets.json qui est g√©r√© par le script scrape_markets.py
et le workflow 'Update Markets Data Only'.
"""

import os
import json
import sys
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import logging
import time

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    "base_url": "https://www.boursorama.com/bourse/actions/cotations/international/",
    "nasdaq": {
        "country": "1",  # √âtats-Unis
        "market": "$COMPX",  # NASDAQ Composite
        "output_path": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "lists.json"),
    },
    "stoxx": {
        "country": "EU",  # Europe
        "market": "2cSXXP",  # DJ STOXX 600
        "output_dir": os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data"),
    },
    "alphabet": list("ABCDEFGHIJKLMNOPQRSTUVWXYZ"),
    "sleep_time": 1.5  # D√©lai entre les requ√™tes pour √©viter la d√©tection de bot
}

def get_headers():
    """Cr√©e des en-t√™tes HTTP pour √©viter la d√©tection de bot"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.google.com/"
    }

#
# Fonctions communes
#
def extract_stock_data(row):
    """Extrait les donn√©es d'une action √† partir d'une ligne de tableau"""
    try:
        cells = row.find_all('td')
        if not cells or len(cells) < 8:
            return None
            
        # R√©cup√©rer le libell√© et le lien
        libelle_cell = cells[0]
        libelle_link = libelle_cell.find('a')
        libelle = libelle_link.text.strip() if libelle_link else ""
        link = libelle_link.get('href') if libelle_link else ""
        
        # R√©cup√©rer le cours et les autres valeurs
        dernier = cells[1].text.strip()
        variation = cells[2].text.strip()
        ouverture = cells[3].text.strip()
        plus_haut = cells[4].text.strip()
        plus_bas = cells[5].text.strip()
        var_ytd = cells[6].text.strip()
        volume = cells[7].text.strip()
        
        # D√©terminer la tendance en fonction de la variation
        trend = "up" if variation and not variation.startswith('-') and variation != "0,00%" else "down"
        if variation == "0,00%":
            trend = "neutral"
            
        # Cr√©er l'objet stock
        stock_data = {
            "symbol": link.split('/')[-1] if link else "",
            "name": libelle,
            "last": dernier,
            "change": variation,
            "open": ouverture,
            "high": plus_haut,
            "low": plus_bas,
            "ytd": var_ytd,
            "volume": volume,
            "trend": trend,
            "link": f"https://www.boursorama.com{link}" if link else ""
        }
        
        return stock_data
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des donn√©es d'une action: {str(e)}")
        return None

def get_top_performers(stocks, sort_field, reverse=True, limit=10):
    """R√©cup√®re les top/bottom performers bas√©s sur un champ donn√©"""
    def extract_value(value_str):
        if not value_str:
            return 0
        # Nettoyer la cha√Æne pour extraire le nombre
        cleaned = value_str.replace('%', '').replace(',', '.').replace(' ', '')
        try:
            return float(cleaned)
        except:
            return 0
    
    # Trier les actions en fonction du champ
    sorted_stocks = sorted(
        [s for s in stocks if s.get(sort_field)], 
        key=lambda x: extract_value(x.get(sort_field, "0")), 
        reverse=reverse
    )
    
    # Prendre les premiers √©l√©ments
    return sorted_stocks[:limit]

#
# Fonctions pour NASDAQ
#
def get_nasdaq_url(letter, page=1):
    """G√©n√®re l'URL pour obtenir la liste des actions NASDAQ pour une lettre donn√©e"""
    params = {
        "international_quotation_az_filter[country]": CONFIG["nasdaq"]["country"],
        "international_quotation_az_filter[market]": CONFIG["nasdaq"]["market"],
        "international_quotation_az_filter[letter]": letter,
        "international_quotation_az_filter[filter]": "",
        "page": page
    }
    query_params = "&".join([f"{k}={v}" for k, v in params.items()])
    return f"{CONFIG['base_url']}?{query_params}"

def scrape_nasdaq_page(letter, page=1):
    """Scrape une page de la liste des actions NASDAQ"""
    url = get_nasdaq_url(letter, page)
    logger.info(f"R√©cup√©ration des donn√©es NASDAQ pour la lettre {letter}, page {page}: {url}")
    
    try:
        response = requests.get(url, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Trouver le tableau des actions
        table = soup.find('table', class_='c-table')
        if not table:
            logger.warning(f"Aucun tableau trouv√© pour la lettre {letter}, page {page}")
            return []
            
        # Trouver toutes les lignes de donn√©es (ignorer l'en-t√™te)
        rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
        
        stocks = []
        for row in rows:
            stock_data = extract_stock_data(row)
            if stock_data:
                stocks.append(stock_data)
                
        logger.info(f"Trouv√© {len(stocks)} actions NASDAQ pour la lettre {letter}, page {page}")
        
        # V√©rifier s'il y a une pagination
        has_next_page = False
        pagination = soup.find('ul', class_='c-pagination')
        if pagination:
            next_button = pagination.find('li', class_='c-pagination__item--next')
            has_next_page = next_button and not next_button.has_attr('disabled')
            
        return stocks, has_next_page
        
    except Exception as e:
        logger.error(f"Erreur lors du scraping NASDAQ pour lettre {letter}, page {page}: {str(e)}")
        return [], False

def scrape_all_nasdaq_stocks():
    """Scrape toutes les actions du NASDAQ Composite"""
    all_stocks = []
    
    for letter in CONFIG["alphabet"]:
        page = 1
        has_next_page = True
        
        while has_next_page:
            stocks, has_next_page = scrape_nasdaq_page(letter, page)
            all_stocks.extend(stocks)
            
            # Si pas de page suivante, sortir de la boucle
            if not has_next_page:
                break
                
            # Passer √† la page suivante
            page += 1
            
            # Attente pour √©viter de surcharger le serveur
            time.sleep(CONFIG["sleep_time"])
    
    return all_stocks

def save_nasdaq_data(stocks):
    """Enregistre les donn√©es NASDAQ dans un fichier JSON"""
    try:
        # Organiser les actions par premi√®re lettre
        stocks_by_letter = {}
        for letter in "abcdefghijklmnopqrstuvwxyz":
            stocks_by_letter[letter] = []
        
        # Trier les actions par premi√®re lettre
        for stock in stocks:
            first_letter = stock["name"][0].lower() if stock["name"] else "a"
            if first_letter.isalpha() and first_letter in stocks_by_letter:
                stocks_by_letter[first_letter].append(stock)
        
        # Cr√©er la structure compatible
        compatible_data = {
            "indices": stocks_by_letter,
            "top_performers": {
                "daily": {
                    "best": get_top_performers(stocks, "change", reverse=True),
                    "worst": get_top_performers(stocks, "change", reverse=False)
                },
                "ytd": {
                    "best": get_top_performers(stocks, "ytd", reverse=True),
                    "worst": get_top_performers(stocks, "ytd", reverse=False)
                }
            },
            "meta": {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "count": len(stocks),
                "source": "Boursorama",
                "description": "Actions du NASDAQ Composite (√âtats-Unis)"
            }
        }
        
        # √âcrire le fichier JSON
        with open(CONFIG["nasdaq"]["output_path"], 'w', encoding='utf-8') as f:
            json.dump(compatible_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es NASDAQ enregistr√©es dans {CONFIG['nasdaq']['output_path']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es NASDAQ: {str(e)}")
        return False

#
# Fonctions pour STOXX
#
def get_stoxx_url(page=1, letter=""):
    """G√©n√®re l'URL pour obtenir la liste des actions STOXX pour une page donn√©e"""
    params = {
        "international_quotation_az_filter[country]": CONFIG["stoxx"]["country"],
        "international_quotation_az_filter[market]": CONFIG["stoxx"]["market"],
        "international_quotation_az_filter[letter]": letter,
        "international_quotation_az_filter[filter]": "",
        "pagination_1231311441": page
    }
    query_params = "&".join([f"{k}={v}" for k, v in params.items()])
    return f"{CONFIG['base_url']}?{query_params}"

def scrape_stoxx_page(page=1):
    """Scrape une page de la liste des actions du STOXX 600"""
    url = get_stoxx_url(page)
    logger.info(f"R√©cup√©ration des donn√©es STOXX pour la page {page}: {url}")
    
    try:
        response = requests.get(url, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Trouver le tableau des actions
        table = soup.find('table', class_='c-table')
        if not table:
            logger.warning(f"Aucun tableau trouv√© pour la page STOXX {page}")
            return [], 0, 1
            
        # Trouver toutes les lignes de donn√©es (ignorer l'en-t√™te)
        rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
        
        stocks = []
        for row in rows:
            stock_data = extract_stock_data(row)
            if stock_data:
                stocks.append(stock_data)
                
        logger.info(f"Trouv√© {len(stocks)} actions STOXX pour la page {page}")
        
        # D√©terminer le nombre total de pages
        total_pages = 1
        current_page = page
        pagination = soup.select_one('.c-block-pagination__content')
        if pagination:
            # Extraire le nombre de pages
            page_items = pagination.select('a.c-block-pagination__link')
            for item in page_items:
                if item.text.isdigit():
                    page_num = int(item.text)
                    total_pages = max(total_pages, page_num)
            
            # R√©cup√©rer la page actuelle
            current_item = pagination.select_one('span.c-block-pagination__link--current') 
            if current_item and current_item.text.isdigit():
                current_page = int(current_item.text)
        
        return stocks, current_page, total_pages
        
    except Exception as e:
        logger.error(f"Erreur lors du scraping STOXX de la page {page}: {str(e)}")
        return [], page, 1

def save_stoxx_data_for_page(stocks, page, total_pages):
    """Enregistre les donn√©es STOXX pour une page sp√©cifique"""
    try:
        # Organiser les actions par lettre
        stocks_by_letter = {}
        for letter in "abcdefghijklmnopqrstuvwxyz":
            stocks_by_letter[letter] = []
            
        # R√©partir les actions par lettre
        for stock in stocks:
            if not stock.get("name"):
                continue
                
            first_letter = stock["name"][0].lower()
            if first_letter.isalpha() and first_letter in stocks_by_letter:
                stocks_by_letter[first_letter].append(stock)
        
        # Pr√©parer les donn√©es de top performers
        top_performers = {
            "daily": {
                "best": get_top_performers(stocks, "change", reverse=True),
                "worst": get_top_performers(stocks, "change", reverse=False)
            },
            "ytd": {
                "best": get_top_performers(stocks, "ytd", reverse=True),
                "worst": get_top_performers(stocks, "ytd", reverse=False)
            }
        }
        
        # Cr√©er l'objet de donn√©es final
        data = {
            "indices": stocks_by_letter,
            "top_performers": top_performers,
            "meta": {
                "source": "Boursorama",
                "url": CONFIG["base_url"],
                "description": "Actions du DJ STOXX 600 (Europe)",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "count": len(stocks),
                "pagination": {
                    "currentPage": page,
                    "totalPages": total_pages
                }
            }
        }
        
        # Chemin du fichier de sortie
        output_path = os.path.join(CONFIG["stoxx"]["output_dir"], f"stoxx_page_{page}.json")
        
        # √âcrire le fichier JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Donn√©es STOXX pour la page {page} enregistr√©es dans {output_path}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement des donn√©es STOXX pour la page {page}: {str(e)}")
        return False

def scrape_all_stoxx():
    """Scrape toutes les pages du STOXX 600"""
    try:
        # R√©cup√©rer les donn√©es de la premi√®re page pour obtenir le nombre total de pages
        first_page_stocks, current_page, total_pages = scrape_stoxx_page(1)
        
        if not first_page_stocks:
            logger.error("‚ùå Aucune action STOXX r√©cup√©r√©e sur la premi√®re page")
            return {
                "status": "error",
                "message": "Aucune donn√©e r√©cup√©r√©e",
                "pages": 0,
                "stocks": 0
            }
            
        # Enregistrer les donn√©es de la premi√®re page
        save_stoxx_data_for_page(first_page_stocks, 1, total_pages)
        
        # Nombre total d'actions
        total_stocks = len(first_page_stocks)
        
        # R√©cup√©rer les donn√©es des autres pages
        for page in range(2, total_pages + 1):
            # Attente pour √©viter de surcharger le serveur
            time.sleep(CONFIG["sleep_time"])
            
            # R√©cup√©rer les donn√©es de la page
            stocks, _, _ = scrape_stoxx_page(page)
            
            if stocks:
                # Ajouter au compteur total
                total_stocks += len(stocks)
                
                # Enregistrer les donn√©es de la page
                save_stoxx_data_for_page(stocks, page, total_pages)
        
        logger.info(f"‚úÖ Scraping STOXX termin√© avec succ√®s: {total_pages} pages, {total_stocks} actions r√©cup√©r√©es")
        return {
            "status": "success",
            "pages": total_pages,
            "stocks": total_stocks
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scraping STOXX: {str(e)}")
        return {
            "status": "error",
            "message": str(e),
            "pages": 0,
            "stocks": 0
        }

def ensure_data_directory():
    """S'assure que le r√©pertoire de donn√©es existe"""
    data_dir = os.path.dirname(CONFIG["nasdaq"]["output_path"])
    if not os.path.exists(data_dir):
        os.makedirs(data_dir, exist_ok=True)
        logger.info(f"‚úÖ R√©pertoire de donn√©es cr√©√©: {data_dir}")

def verify_no_markets_conflict():
    """V√©rifie que ce script ne modifie pas le fichier markets.json"""
    markets_file = os.path.join(os.path.dirname(CONFIG["nasdaq"]["output_path"]), "markets.json")
    if os.path.exists(markets_file):
        logger.info(f"‚úÖ V√©rification: Le fichier markets.json ne sera pas modifi√© par ce script")
    return True

def main():
    """Point d'entr√©e principal"""
    try:
        logger.info("üöÄ D√©marrage du script unifi√© d'extraction des donn√©es NASDAQ et STOXX")
        
        # S'assurer que le r√©pertoire de donn√©es existe
        ensure_data_directory()
        
        # V√©rifier qu'il n'y a pas de conflit avec markets.json
        verify_no_markets_conflict()
        
        # 1. Scraper les donn√©es NASDAQ
        logger.info("üìä D√©but du scraping NASDAQ...")
        nasdaq_stocks = scrape_all_nasdaq_stocks()
        
        if nasdaq_stocks:
            # Enregistrer les donn√©es NASDAQ
            save_nasdaq_data(nasdaq_stocks)
            logger.info(f"‚úÖ Scraping NASDAQ termin√©: {len(nasdaq_stocks)} actions r√©cup√©r√©es")
        else:
            logger.error("‚ùå Aucune action NASDAQ r√©cup√©r√©e")
        
        # 2. Scraper les donn√©es STOXX
        logger.info("üìä D√©but du scraping STOXX...")
        stoxx_result = scrape_all_stoxx()
        
        # 3. Enregistrer un r√©sum√© global
        result_summary = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "nasdaq": {
                "count": len(nasdaq_stocks),
                "status": "success" if nasdaq_stocks else "error"
            },
            "stoxx": stoxx_result
        }
        
        # Sauvegarder le r√©sum√©
        summary_path = os.path.join(CONFIG["stoxx"]["output_dir"], "update_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(result_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üìä R√©sum√©: {json.dumps(result_summary, indent=2)}")
        logger.info("‚úÖ Script unifi√© termin√© avec succ√®s")
        
        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()