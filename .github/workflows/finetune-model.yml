name: Fine-tune Custom FinBERT Model

on:
  workflow_dispatch:
    inputs:
      dataset_path:
        description: 'Chemin vers le dataset de fine-tuning'
        required: true
        default: 'ml/feedback/feedback_data.json'
      model_name:
        description: 'Nom du modÃ¨le custom'
        required: true
        default: 'tradepulse-finbert-v1'
      push_to_hub:
        description: 'Push model to HuggingFace Hub'
        type: boolean
        required: false
        default: false
      epochs:
        description: 'Nombre d epochs'
        required: false
        default: '3'

permissions:
  contents: write

jobs:
  finetune-model:
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch transformers huggingface_hub datasets accelerate
          pip install scikit-learn pandas numpy
      
      # ğŸš€ SOLUTION CORRIGÃ‰E : Action officielle HuggingFace
      - name: ğŸ” Login to Hugging Face
        if: ${{ github.event.inputs.push_to_hub == 'true' }}
        uses: huggingface/huggingface-cli-login@v1
        with:
          hf_token: ${{ secrets.HF_TOKEN }}
      
      - name: ğŸ“ Create directories
        run: |
          mkdir -p models/${{ github.event.inputs.model_name }}
          mkdir -p logs/${{ github.event.inputs.model_name }}
          mkdir -p ml/feedback
      
      - name: ğŸ§  Create fine-tuning script
        run: |
          cat > finetune.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import json
          import torch
          from transformers import (
              AutoTokenizer, AutoModelForSequenceClassification,
              TrainingArguments, Trainer, DataCollatorWithPadding
          )
          from datasets import Dataset
          import sys
          from datetime import datetime

          def main():
              # Configuration depuis les inputs GitHub
              dataset_path = "${{ github.event.inputs.dataset_path }}"
              model_name = "${{ github.event.inputs.model_name }}"
              epochs = int("${{ github.event.inputs.epochs }}")
              push_to_hub = "${{ github.event.inputs.push_to_hub }}" == "true"
              base_model = "yiyanghkust/finbert-tone"
              
              print(f"ğŸš€ Starting fine-tuning for {model_name}")
              print(f"ğŸ“Š Base model: {base_model}")
              print(f"ğŸ“š Dataset: {dataset_path}")
              print(f"ğŸ”¢ Epochs: {epochs}")
              print(f"ğŸ”„ Push to Hub: {push_to_hub}")
              
              # Charger les donnÃ©es de feedback
              if not os.path.exists(dataset_path):
                  print(f"âŒ Dataset file not found: {dataset_path}")
                  # CrÃ©er un dataset minimal pour tester
                  sample_data = [
                      {"content": "Stock prices are rising significantly", "userFeedback": "positive"},
                      {"content": "Market crash looms ahead", "userFeedback": "negative"},
                      {"content": "Neutral market conditions persist", "userFeedback": "neutral"},
                      {"content": "Strong earnings beat expectations", "userFeedback": "positive"},
                      {"content": "Economic uncertainty continues", "userFeedback": "negative"},
                      {"content": "Markets remain stable", "userFeedback": "neutral"},
                      {"content": "Bullish momentum accelerates", "userFeedback": "positive"},
                      {"content": "Bearish trends emerge", "userFeedback": "negative"},
                      {"content": "Trading volumes steady", "userFeedback": "neutral"},
                      {"content": "Profits surge dramatically", "userFeedback": "positive"},
                  ]
                  print(f"ğŸ“ Using sample dataset with {len(sample_data)} entries")
                  feedback_data = sample_data
              else:
                  with open(dataset_path, 'r', encoding='utf-8') as f:
                      feedback_data = json.load(f)
                  print(f"ğŸ“š Loaded {len(feedback_data)} feedback samples from {dataset_path}")
              
              if len(feedback_data) < 5:
                  print(f"âš ï¸ Not enough data ({len(feedback_data)}), minimum 5 required")
                  sys.exit(1)
              
              # PrÃ©parer les donnÃ©es
              texts = []
              labels = []
              label_map = {"positive": 2, "neutral": 1, "negative": 0}
              
              for item in feedback_data:
                  if isinstance(item, dict):
                      # Format avec userFeedback
                      if "userFeedback" in item and "content" in item:
                          texts.append(str(item["content"]))
                          labels.append(label_map.get(item["userFeedback"], 1))
                      # Format avec impact
                      elif "impact" in item and ("content" in item or "title" in item):
                          content = item.get("content", item.get("title", ""))
                          texts.append(str(content))
                          labels.append(label_map.get(item["impact"], 1))
              
              if len(texts) < 5:
                  print(f"âš ï¸ Insufficient data after filtering: {len(texts)}")
                  sys.exit(1)
              
              print(f"ğŸ“Š Prepared {len(texts)} training samples")
              
              # CrÃ©er le dataset
              dataset = Dataset.from_dict({
                  "text": texts,
                  "labels": labels
              })
              
              # Charger le modÃ¨le et tokenizer
              print(f"ğŸ¤– Loading model and tokenizer: {base_model}")
              tokenizer = AutoTokenizer.from_pretrained(base_model)
              model = AutoModelForSequenceClassification.from_pretrained(base_model)
              
              # Function de tokenisation
              def tokenize_function(examples):
                  return tokenizer(
                      examples["text"],
                      truncation=True,
                      padding="max_length",
                      max_length=512
                  )
              
              # Tokeniser le dataset
              print("ğŸ”¤ Tokenizing dataset...")
              tokenized_dataset = dataset.map(tokenize_function, batched=True)
              
              # Split train/eval (80/20)
              train_size = max(1, int(0.8 * len(tokenized_dataset)))
              train_dataset = tokenized_dataset.select(range(train_size))
              eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))
              
              print(f"ğŸ¯ Train samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}")
              
              # Configuration d'entraÃ®nement
              hub_model_id = f"Bencode92/{model_name}" if push_to_hub else None
              
              training_args = TrainingArguments(
                  output_dir=f"./models/{model_name}",
                  num_train_epochs=epochs,
                  per_device_train_batch_size=4,  # RÃ©duit pour Ã©viter OOM
                  per_device_eval_batch_size=4,
                  warmup_steps=min(100, len(train_dataset) // 4),
                  weight_decay=0.01,
                  logging_dir=f"./logs/{model_name}",
                  logging_steps=max(1, len(train_dataset) // 10),
                  evaluation_strategy="steps" if len(eval_dataset) > 0 else "no",
                  eval_steps=max(10, len(train_dataset) // 5) if len(eval_dataset) > 0 else None,
                  save_steps=max(20, len(train_dataset) // 2),
                  load_best_model_at_end=len(eval_dataset) > 0,
                  metric_for_best_model="eval_loss" if len(eval_dataset) > 0 else None,
                  push_to_hub=push_to_hub,
                  hub_model_id=hub_model_id,
                  report_to=None,  # Disable wandb
                  dataloader_pin_memory=False,
                  dataloader_num_workers=0,
              )
              
              # Data collator
              data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
              
              # Trainer
              trainer = Trainer(
                  model=model,
                  args=training_args,
                  train_dataset=train_dataset,
                  eval_dataset=eval_dataset if len(eval_dataset) > 0 else None,
                  tokenizer=tokenizer,
                  data_collator=data_collator,
              )
              
              # EntraÃ®nement
              print("ğŸ‹ï¸ Starting training...")
              trainer.train()
              
              # Ã‰valuation finale
              if len(eval_dataset) > 0:
                  print("ğŸ“Š Final evaluation...")
                  eval_results = trainer.evaluate()
                  print(f"Final eval loss: {eval_results.get('eval_loss', 'N/A')}")
              
              # Sauvegarder le modÃ¨le
              print("ğŸ’¾ Saving model...")
              trainer.save_model()
              tokenizer.save_pretrained(f"./models/{model_name}")
              
              # CrÃ©er le README du modÃ¨le
              readme_content = f"""# {model_name}

              Custom FinBERT model fine-tuned for financial sentiment analysis.

              ## Model Details
              - Base model: {base_model}
              - Fine-tuned on: {len(texts)} samples
              - Training epochs: {epochs}
              - Created: {datetime.now().isoformat()}

              ## Usage
              ```python
              from transformers import AutoTokenizer, AutoModelForSequenceClassification
              
              tokenizer = AutoTokenizer.from_pretrained("Bencode92/{model_name}")
              model = AutoModelForSequenceClassification.from_pretrained("Bencode92/{model_name}")
              ```

              ## Labels
              - 0: Negative
              - 1: Neutral  
              - 2: Positive
              """
              
              with open(f"./models/{model_name}/README.md", 'w') as f:
                  f.write(readme_content)
              
              if push_to_hub:
                  print("ğŸš€ Pushing to Hub...")
                  trainer.push_to_hub(commit_message=f"Fine-tuned {model_name} on financial data")
                  tokenizer.push_to_hub(hub_model_id, commit_message=f"Add tokenizer for {model_name}")
                  print(f"âœ… Model pushed to: https://huggingface.co/{hub_model_id}")
              
              print("âœ… Fine-tuning completed successfully!")

          if __name__ == "__main__":
              main()
          EOF
          
          chmod +x finetune.py
      
      - name: ğŸ‹ï¸ Run fine-tuning
        run: |
          python finetune.py
        env:
          TOKENIZERS_PARALLELISM: false
          TRANSFORMERS_CACHE: ./cache
      
      - name: ğŸ“Š Generate training summary
        run: |
          echo "=== TradePulse FinBERT Training Summary ===" > training_summary.txt
          echo "Model: ${{ github.event.inputs.model_name }}" >> training_summary.txt
          echo "Dataset: ${{ github.event.inputs.dataset_path }}" >> training_summary.txt
          echo "Epochs: ${{ github.event.inputs.epochs }}" >> training_summary.txt
          echo "Push to Hub: ${{ github.event.inputs.push_to_hub }}" >> training_summary.txt
          echo "Timestamp: $(date)" >> training_summary.txt
          echo "" >> training_summary.txt
          
          if [ -d "models/${{ github.event.inputs.model_name }}" ]; then
            echo "âœ… Model saved locally" >> training_summary.txt
            echo "Files:" >> training_summary.txt
            ls -la "models/${{ github.event.inputs.model_name }}" >> training_summary.txt
          else
            echo "âŒ Model save failed" >> training_summary.txt
          fi
          
          echo "" >> training_summary.txt
          echo "=== Configuration ===" >> training_summary.txt
          echo "Python version: $(python --version)" >> training_summary.txt
          echo "PyTorch version: $(python -c 'import torch; print(torch.__version__)')" >> training_summary.txt
          echo "Transformers version: $(python -c 'import transformers; print(transformers.__version__)')" >> training_summary.txt
          
          cat training_summary.txt
      
      - name: ğŸ·ï¸ Commit and upload artifacts
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          
          # Ajouter les fichiers de modÃ¨le
          git add models/ logs/ training_summary.txt || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Add fine-tuned model: ${{ github.event.inputs.model_name }} [skip ci]"
            git push
          fi
      
      - name: ğŸ“¤ Upload model as artifact
        uses: actions/upload-artifact@v3
        with:
          name: ${{ github.event.inputs.model_name }}
          path: |
            models/${{ github.event.inputs.model_name }}/
            training_summary.txt
          retention-days: 30
