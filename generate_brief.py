"""
generate_brief.py - G√©n√©rateur de brief strat√©gique pour TradePulse
Ce script analyse les donn√©es financi√®res via GPT pour produire un r√©sum√© strat√©gique
utilis√© ensuite par le g√©n√©rateur de portefeuilles.
"""

import os
import sys
import json
import requests
import datetime
import logging
import locale
import time
import re
from collections import defaultdict
from dotenv import load_dotenv

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Configuration de la locale fran√ßaise pour les dates
try:
    locale.setlocale(locale.LC_TIME, "fr_FR.UTF-8")
except Exception:
    try:
        locale.setlocale(locale.LC_TIME, "fr_FR")
    except Exception:
        logger.warning("‚ö†Ô∏è Impossible de configurer la locale fran√ßaise, utilisation de la locale par d√©faut")

# Chargement des cl√©s si local
load_dotenv()

# R√©cup√©ration de la cl√© API (environnement ou GitHub secrets)
API_KEY = os.environ.get("API_CHAT")
if not API_KEY:
    raise ValueError("La cl√© API OpenAI (API_CHAT) n'est pas d√©finie.")

# Mod√®le param√©trable via variable d'environnement
MODEL_NAME = os.environ.get("TRADEPULSE_LLM_MODEL", "gpt-4o-mini")

# Paths
DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
THEMES_PATH = os.path.join(DATA_PATH, "themes.json")
NEWS_PATH = os.path.join(DATA_PATH, "news.json")
MARKET_PATH = os.path.join(DATA_PATH, "markets.json")
SECTOR_PATH = os.path.join(DATA_PATH, "sectors.json")
BRIEF_PATH = os.path.join(DATA_PATH, "brief_ia.json")
BRIEF_MD_PATH = os.path.join(DATA_PATH, "brief_ia.md")


# ======================================================================
# Helpers g√©n√©riques
# ======================================================================

def load_json_data(file_path):
    """Charger des donn√©es depuis un fichier JSON avec gestion d'erreurs."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            data = json.load(file)
            if not isinstance(data, (dict, list)):
                logger.warning(f"‚ö†Ô∏è Format de donn√©es non valide dans {file_path}, doit √™tre dict ou list")
                return {}
            logger.info(f"‚úÖ Donn√©es JSON charg√©es avec succ√®s depuis {file_path}")
            return data
    except FileNotFoundError:
        logger.error(f"‚ùå Fichier non trouv√©: {file_path}")
        return {}
    except json.JSONDecodeError:
        logger.error(f"‚ùå Format JSON invalide dans {file_path}")
        return {}
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement de {file_path}: {str(e)}")
        return {}


def extract_timestamp_from_data(data):
    """Tente d'extraire un timestamp ISO 8601 des m√©tadonn√©es."""
    if not isinstance(data, dict):
        return None

    candidates = []

    # Top-level cl√©s possibles
    for key in ("updated_at", "generated_at", "last_updated", "lastUpdated"):
        val = data.get(key)
        if isinstance(val, str):
            candidates.append(val)

    # Dans meta / metadata
    for meta_key in ("meta", "metadata"):
        meta = data.get(meta_key)
        if isinstance(meta, dict):
            for key in ("updated_at", "generated_at", "last_updated", "lastUpdated"):
                val = meta.get(key)
                if isinstance(val, str):
                    candidates.append(val)

    for ts in candidates:
        try:
            # Gestion du 'Z' pour UTC
            if ts.endswith("Z"):
                ts = ts.replace("Z", "+00:00")
            return datetime.datetime.fromisoformat(ts)
        except Exception:
            continue

    return None


def validate_data_freshness(data, label, max_age_hours=48):
    """
    Valide la fra√Æcheur des donn√©es si un timestamp est disponible.
    Retourne True si OK ou si aucun timestamp n'est trouv√©.
    """
    ts = extract_timestamp_from_data(data)
    if ts is None:
        logger.info(f"‚ÑπÔ∏è Aucun timestamp d√©tect√© pour les donn√©es '{label}', pas de contr√¥le de fra√Æcheur strict.")
        return True

    # Gestion timezone
    now = datetime.datetime.now(ts.tzinfo) if ts.tzinfo else datetime.datetime.now()
    age_hours = (now - ts).total_seconds() / 3600.0
    
    if age_hours > max_age_hours:
        logger.error(
            f"‚ùå Donn√©es '{label}' obsol√®tes: {age_hours:.1f}h (> {max_age_hours}h) depuis la derni√®re mise √† jour ({ts.isoformat()})"
        )
        return False

    logger.info(f"‚úÖ Donn√©es '{label}' fra√Æches ({age_hours:.1f}h d'anciennet√©).")
    return True


def truncate_json_data(data, max_chars=15000, label=""):
    """Tronque les donn√©es JSON si elles d√©passent max_chars pour √©viter le d√©passement du context window."""
    if data is None:
        return ""
    if isinstance(data, str):
        json_str = data
    else:
        try:
            json_str = json.dumps(data, indent=2, ensure_ascii=False)
        except Exception:
            json_str = str(data)

    length = len(json_str)
    if length > max_chars:
        logger.warning(
            f"‚ö†Ô∏è Donn√©es '{label}' tronqu√©es ({length} ‚Üí {max_chars} caract√®res) pour respecter la limite de contexte."
        )
        return json_str[:max_chars] + "\n... [TRONQU√â]"
    return json_str


# ======================================================================
# Extraction de scores et r√©gions depuis les news
# ======================================================================

def get_importance_score(item):
    """R√©cup√®re un score d'importance coh√©rent depuis diff√©rentes cl√©s possibles."""
    for key in ("imp", "importance_score", "score"):
        val = item.get(key)
        if val is None:
            continue
        if isinstance(val, (int, float)):
            return float(val)
        try:
            return float(val)
        except (TypeError, ValueError):
            continue
    return 0.0


def get_quality_score(item):
    """R√©cup√®re un score de qualit√© s'il existe."""
    for key in ("quality_score", "quality", "score"):
        val = item.get(key)
        if val is None:
            continue
        if isinstance(val, (int, float)):
            return float(val)
        try:
            return float(val)
        except (TypeError, ValueError):
            continue
    return None


def get_region_label(item):
    """
    Normalise la r√©gion pour limiter le biais US.
    Utilise d'abord 'region', puis 'country' / 'country_code'.
    """
    region = item.get("region") or item.get("zone")
    if isinstance(region, str) and region.strip():
        return region

    country = item.get("country") or item.get("country_code") or item.get("location")
    if not country:
        return "Autres"

    country_upper = str(country).upper()

    if any(tok in country_upper for tok in ("US", "UNITED STATES", "USA", "AMERICA")):
        return "√âtats-Unis"
    if any(tok in country_upper for tok in ("FR", "FRANCE", "DE", "GERMANY", "EU", "EUROPE",
                                            "IT", "ITALY", "ES", "SPAIN", "UK", "UNITED KINGDOM", "GB")):
        return "Europe"
    if any(tok in country_upper for tok in ("CN", "CHINA", "JP", "JAPAN", "HK", "HONG KONG",
                                            "SG", "SINGAPORE", "KR", "KOREA", "INDIA", "IN")):
        return "Asie"
    if any(tok in country_upper for tok in ("BR", "BRAZIL", "MX", "MEXICO", "ZA", "SOUTH AFRICA",
                                            "AFRICA", "LATAM")):
        return "√âmergents"

    return "Autres"


def compute_rank_score(item):
    """Score composite importance + qualit√© pour le tri."""
    imp = get_importance_score(item)
    q = get_quality_score(item)
    if q is None:
        return imp
    return 0.7 * imp + 0.3 * q


# ======================================================================
# S√©lection et synth√®se des news
# ======================================================================

def select_top_news(all_news, max_items=60, min_items=30):
    """
    S√©lectionne les actualit√©s les plus importantes avec :
    - seuil dynamique sur le score d'importance (‚âà percentile 75)
    - r√©√©quilibrage par r√©gion pour limiter le biais US
    """
    if not all_news:
        return []

    # Calcul du seuil dynamique (percentile 75)
    imp_values = [get_importance_score(n) for n in all_news if get_importance_score(n) > 0]
    if imp_values:
        values_sorted = sorted(imp_values)
        idx = int(0.75 * (len(values_sorted) - 1))
        imp_threshold = values_sorted[idx]
        imp_threshold = max(imp_threshold, 40.0)  # seuil plancher
    else:
        imp_threshold = 60.0

    logger.info(f"üìä Seuil dynamique d'importance des news fix√© √† {imp_threshold:.1f}")

    # Filtrage initial
    filtered = [n for n in all_news if get_importance_score(n) >= imp_threshold]

    if len(filtered) > max_items * 3:
        filtered = sorted(filtered, key=compute_rank_score, reverse=True)[: max_items * 3]

    # Fallback si trop peu de news passent le filtre
    if len(filtered) < min_items:
        logger.warning(
            f"‚ö†Ô∏è Seulement {len(filtered)} actualit√©s au-dessus du seuil, fallback sur tri global."
        )
        filtered = sorted(all_news, key=compute_rank_score, reverse=True)[: max(max_items, min_items)]
    else:
        filtered = sorted(filtered, key=compute_rank_score, reverse=True)

    # Regroupement par r√©gion
    news_by_region = defaultdict(list)
    for n in filtered:
        reg = get_region_label(n)
        news_by_region[reg].append(n)

    logger.info("üìå R√©partition par r√©gion avant filtrage:")
    for reg, items in news_by_region.items():
        logger.info(f"   - {reg}: {len(items)} actualit√©s")

    region_count = len(news_by_region)
    if region_count == 0:
        return filtered[:max_items]

    base_per_region = max_items // region_count if region_count > 0 else max_items
    selected = []
    used_ids = set()

    # Limites par r√©gion (anti-biais US: max 30%)
    for reg, items in news_by_region.items():
        items_sorted = sorted(items, key=compute_rank_score, reverse=True)
        if reg in ("√âtats-Unis", "US", "USA"):
            limit = max(base_per_region, int(max_items * 0.3))
        else:
            limit = max(base_per_region, int(max_items * 0.15))
        for n in items_sorted[:limit]:
            if len(selected) >= max_items:
                break
            selected.append(n)
            used_ids.add(id(n))

    # Compl√©ter si pas assez d'items
    if len(selected) < max_items:
        remaining = [
            n for n in sorted(filtered, key=compute_rank_score, reverse=True) if id(n) not in used_ids
        ]
        for n in remaining:
            if len(selected) >= max_items:
                break
            selected.append(n)

    logger.info(f"üîù {len(selected)} actualit√©s s√©lectionn√©es apr√®s filtrage dynamique et r√©√©quilibrage r√©gional")
    return selected[:max_items]


def synthesize_news(news_list):
    """
    Synth√©tise les actualit√©s pour optimiser l'utilisation des tokens.
    Adapt√© √† la structure actuelle de news.json (imp, impact, quality_score, snippet, country, t).
    """
    logger.info("üîÑ Synth√©tisation des actualit√©s pour optimisation des tokens...")

    simplified_news = []
    
    for item in news_list:
        titre = item.get("title") or item.get("headline") or ""
        titre = titre[:150]

        date = item.get("date") or item.get("published_at") or item.get("time") or ""
        category = item.get("category") or item.get("topic") or ""
        region = get_region_label(item)
        importance = get_importance_score(item)
        quality = get_quality_score(item)
        composite = compute_rank_score(item)

        news_item = {
            "titre": titre,
            "date": date,
            "cat√©gorie": category,
            "r√©gion": region,
            "importance": round(importance, 1),
            "score_composite": round(composite, 1),
        }

        if quality is not None:
            news_item["qualit√©"] = round(quality, 1)

        # Sentiment / impact
        sentiment = item.get("impact") or item.get("sentiment")
        if sentiment:
            news_item["sentiment"] = sentiment

        # Source
        source = item.get("source") or item.get("provider")
        if source:
            news_item["source"] = source

        # Tags th√©matiques
        tags = item.get("t", [])
        if tags:
            news_item["tags"] = tags[:5]

        # Snippet / r√©sum√©
        snippet = (
            item.get("snippet")
            or item.get("summary")
            or item.get("content")
            or item.get("text")
        )
        if snippet:
            s = str(snippet).strip().replace("\n", " ")
            if len(s) > 220:
                s = s[:220] + "..."
            news_item["r√©sum√©"] = s

        simplified_news.append(news_item)

    logger.info(f"‚úÖ {len(simplified_news)} actualit√©s synth√©tis√©es pour optimisation des tokens")
    return simplified_news


# ======================================================================
# Validation du brief g√©n√©r√©
# ======================================================================

def validate_brief_structure(brief_text):
    """
    Valide grossi√®rement la structure du brief via des titres Markdown.
    Ne bloque pas, mais log un warning si des sections manquent.
    """
    if not brief_text:
        logger.error("‚ùå Brief vide, impossible de valider la structure.")
        return {"ok": False, "missing": ["TOUT"]}

    section_patterns = {
        "Macro√©conomie": r"(?:^|\n)#{1,3}\s+.*macro",
        "March√©s": r"(?:^|\n)#{1,3}\s+.*march",
        "Secteurs": r"(?:^|\n)#{1,3}\s+.*secteur",
        "R√©gions": r"(?:^|\n)#{1,3}\s+.*r√©gion",
        "Implications pour l'investisseur": r"(?:^|\n)#{1,3}\s+.*implication",
        "Anticipations vs R√©alit√©": r"(?:^|\n)#{1,3}\s+.*anticipation",
        "Risques cl√©s": r"(?:^|\n)#{1,3}\s+.*risque",
        "Facteurs d√©terminants": r"(?:^|\n)#{1,3}\s+.*facteur",
    }

    missing = []
    for name, pattern in section_patterns.items():
        if not re.search(pattern, brief_text, flags=re.IGNORECASE):
            missing.append(name)

    if missing:
        logger.warning(
            f"‚ö†Ô∏è Structure du brief incompl√®te, sections potentielles manquantes: {', '.join(missing)}"
        )
    else:
        logger.info("‚úÖ Structure globale du brief conforme (sections Markdown d√©tect√©es).")

    return {"ok": len(missing) == 0, "missing": missing}


# ======================================================================
# Appel API OpenAI avec retry robuste
# ======================================================================

def call_openai_api(prompt, model=None, temperature=0.2, max_retries=5, timeout=90):
    """
    Appel √† l'API OpenAI avec gestion robuste des erreurs:
    - Retry avec backoff exponentiel jusqu'√† 120s
    - Gestion sp√©cifique du 429 (rate limit) avec Retry-After header
    - Timeout configurable
    """
    if model is None:
        model = MODEL_NAME

    attempt = 1
    while attempt <= max_retries:
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {API_KEY}",
            }

            data = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
            }

            logger.info(f"üß† Appel OpenAI (tentative {attempt}/{max_retries}, mod√®le={model})...")
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=timeout,
            )

            # Succ√®s
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                logger.info(f"‚úÖ R√©ponse OpenAI re√ßue (tentative {attempt}).")
                return content

            # Rate limit sp√©cifique (429)
            if response.status_code == 429:
                retry_after_header = response.headers.get("Retry-After")
                if retry_after_header:
                    try:
                        retry_after = int(retry_after_header)
                    except ValueError:
                        retry_after = 60
                else:
                    retry_after = min(4 * (2 ** (attempt - 1)), 120)

                logger.warning(
                    f"‚è≥ Rate limit OpenAI (429) √† la tentative {attempt}. Pause de {retry_after} secondes..."
                )
                time.sleep(retry_after)

            else:
                # Autres erreurs HTTP
                logger.error(
                    f"‚ùå Erreur API OpenAI (tentative {attempt}): {response.status_code} - {response.text}"
                )
                if attempt == max_retries:
                    raise Exception(
                        f"Erreur API OpenAI apr√®s {max_retries} tentatives: {response.status_code}"
                    )
                backoff = min(4 * (2 ** (attempt - 1)), 120)
                logger.info(
                    f"‚è±Ô∏è Backoff exponentiel: attente de {backoff} secondes avant nouvelle tentative..."
                )
                time.sleep(backoff)

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Exception r√©seau OpenAI (tentative {attempt}): {str(e)}")
            if attempt == max_retries:
                raise
            backoff = min(4 * (2 ** (attempt - 1)), 120)
            logger.info(
                f"‚è±Ô∏è Backoff r√©seau: attente de {backoff} secondes avant nouvelle tentative..."
            )
            time.sleep(backoff)

        except Exception as e:
            logger.error(
                f"‚ùå Exception inattendue lors de l'appel √† l'API OpenAI (tentative {attempt}): {str(e)}"
            )
            if attempt == max_retries:
                raise
            backoff = min(4 * (2 ** (attempt - 1)), 120)
            logger.info(
                f"‚è±Ô∏è Backoff g√©n√©ral: attente de {backoff} secondes avant nouvelle tentative..."
            )
            time.sleep(backoff)

        finally:
            attempt += 1

    raise RuntimeError("√âchec de l'appel OpenAI apr√®s tous les retries.")


# ======================================================================
# Fonction principale
# ======================================================================

def main():
    """Fonction principale pour g√©n√©rer le brief strat√©gique."""
    try:
        logger.info("üîç Chargement des donn√©es financi√®res...")

        # Chargement des fichiers JSON
        themes_data = load_json_data(THEMES_PATH)
        news_data = load_json_data(NEWS_PATH)
        markets_data = load_json_data(MARKET_PATH)
        sectors_data = load_json_data(SECTOR_PATH)

        # Validation fra√Æcheur (si timestamps disponibles)
        freshness_ok = True
        if themes_data:
            freshness_ok &= validate_data_freshness(themes_data, "th√®mes", max_age_hours=72)
        if news_data:
            freshness_ok &= validate_data_freshness(news_data, "actualit√©s", max_age_hours=48)
        if markets_data:
            freshness_ok &= validate_data_freshness(markets_data, "march√©s", max_age_hours=24)
        if sectors_data:
            freshness_ok &= validate_data_freshness(sectors_data, "secteurs", max_age_hours=72)

        if not freshness_ok:
            logger.error("‚ùå Donn√©es obsol√®tes d√©tect√©es, arr√™t de la g√©n√©ration du brief.")
            sys.exit(1)

        # Validation des donn√©es minimales
        if not themes_data or not news_data:
            logger.error("‚ùå Donn√©es incompl√®tes pour g√©n√©rer le brief (th√®mes ou actualit√©s manquants).")
            sys.exit(1)

        # Regrouper toutes les actualit√©s (globales)
        all_news = []
        if isinstance(news_data, dict):
            for category, articles in news_data.items():
                # On ne prend que les listes (us, france, asia, etc.), pas lastUpdated/metadata
                if isinstance(articles, list):
                    all_news.extend(articles)
                    logger.info(f"üì∞ {len(articles)} actualit√©s trouv√©es dans la cat√©gorie {category}")
        elif isinstance(news_data, list):
            all_news = news_data
            logger.info(f"üì∞ {len(all_news)} actualit√©s trouv√©es (format liste).")

        if not all_news:
            logger.error("‚ùå Aucune actualit√© trouv√©e. V√©rifiez le format de news.json")
            sys.exit(1)

        logger.info(f"üìä Total: {len(all_news)} actualit√©s brutes √† analyser")

        # S√©lection dynamique des actualit√©s les plus importantes
        top_news = select_top_news(all_news, max_items=60, min_items=30)

        # Synth√®se des actualit√©s s√©lectionn√©es
        synthesized_news = synthesize_news(top_news)

        # Extraction des th√®mes dominants
        if isinstance(themes_data, dict):
            themes_weekly = themes_data.get("themes", {}).get("weekly", [])
        else:
            themes_weekly = []

        if not themes_weekly:
            logger.warning("‚ö†Ô∏è Aucun th√®me hebdomadaire trouv√©. V√©rifiez le format de themes.json")
            themes_section = "[]"
        else:
            themes_section = json.dumps(themes_weekly, indent=2, ensure_ascii=False)
            logger.info(f"üîç {len(themes_weekly)} th√®mes dominants identifi√©s")

        # Formatage de la date actuelle
        current_date = datetime.datetime.now()
        try:
            date_formatted = current_date.strftime("%d %B %Y")
        except Exception:
            month_names = {
                1: "janvier", 2: "f√©vrier", 3: "mars", 4: "avril", 5: "mai", 6: "juin",
                7: "juillet", 8: "ao√ªt", 9: "septembre", 10: "octobre", 11: "novembre", 12: "d√©cembre",
            }
            date_formatted = f"{current_date.day} {month_names[current_date.month]} {current_date.year}"

        # Pr√©paration des blocs JSON tronqu√©s pour le prompt
        news_block = truncate_json_data(
            synthesized_news, max_chars=20000, label="actualit√©s importantes"
        )
        markets_block = truncate_json_data(
            markets_data, max_chars=15000, label="march√©s"
        )
        sectors_block = truncate_json_data(
            sectors_data, max_chars=12000, label="secteurs"
        )

        # Construction du prompt expert
        prompt = f"""
Tu es un strat√®ge senior en allocation d'actifs au sein d'une soci√©t√© de gestion de renom.

Tu re√ßois plusieurs types de donn√©es financi√®res :
1. **Th√®mes dominants** extraits de plus de 100 articles √©conomiques (structur√©s par th√®me, r√©gion, secteur)
2. **Actualit√©s √† fort impact** (Top {len(synthesized_news)} globales, scor√©es par importance/qualit√©, en format synth√©tis√©)
3. **Donn√©es march√© actuelles** (indices, taux, spreads, etc.)
4. **Performances sectorielles r√©centes**

üéØ **Objectif** : Produire un **brief strat√©gique √† destination d'un comit√© d'investissement**, clair, synth√©tique et orient√© allocation.

---

üóìÔ∏è Nous sommes la semaine du {date_formatted}. Tu peux utiliser cette information temporelle pour contextualiser tes sc√©narios (FOMC, √©ch√©ances, saison des r√©sultats...).

---

üéì **Tes missions** :

- Identifier les grandes **dynamiques macro, g√©opolitiques et sectorielles**
- D√©tailler **2 √† 3 sc√©narios macro probables** √† 3-12 mois, avec **leurs implications concr√®tes sur les classes d'actifs**
- Anticiper les **r√©actions probables des march√©s** (prixant d√©j√† certaines hypoth√®ses)
- D√©tecter des **d√©calages perception / r√©alit√©** : o√π les march√©s ou m√©dias se trompent-ils ?
- G√©n√©rer **des recommandations actionnables** sur l'allocation (secteurs, zones, classes d'actifs)
- Utiliser les donn√©es de march√© et sectorielles comme points de rep√®re factuels dans tes anticipations
- Identifier les risques cl√©s qui pourraient modifier les sc√©narios pr√©sent√©s
- Lister les m√©triques et √©v√©nements importants √† surveiller dans les semaines √† venir
- Int√©grer des chiffres cl√©s des donn√©es de march√© et sectorielles pour renforcer l'analyse
- Formuler une recommandation explicite sur la position en liquidit√©/cash √† maintenir
- Ne pas inventer de chiffres pr√©cis si l'information n'est pas pr√©sente dans les donn√©es. Dans ce cas, reste g√©n√©rique (ex: "les taux longs restent √©lev√©s") plut√¥t que faux-pr√©cis.

---

üìê **Structure du brief attendue** (utilise des **titres Markdown de niveau 2 (`## ...`)** pour chaque grande partie) :

1. **Macro√©conomie** ‚Äì Tendances globales, sc√©narios, causalit√© √©conomique (ex : "Si X ‚áí alors Y ‚áí impact Z")
   - Pour chaque sc√©nario, AJOUTE UN TITRE EXPLICITE, par exemple : 
     * Sc√©nario 1 : "R√©cession mod√©r√©e" (probabilit√© √©lev√©e)
     * Sc√©nario 2 : "Stabilisation progressive" (probabilit√© moyenne)
     * Sc√©nario 3 : "Rebond optimiste" (probabilit√© faible)

2. **March√©s** ‚Äì O√π en est-on dans le cycle ? Que price le march√© ? Quelles rotations sectorielles probables ?
   - INT√àGRE DES CHIFFRES CL√âS, comme "Les indices boursiers ont perdu en moyenne -3% cette semaine" ou "le taux 10 ans US est descendu √† 3,25%"

3. **Secteurs** ‚Äì Surperformance / sous-performance attendue
   - CITE DES DONN√âES CONCR√àTES, par exemple "Le secteur technologique a surperform√© de +5,2% le mois dernier"

4. **R√©gions cl√©s** ‚Äì √âtats-Unis, Europe, Asie, Emergents : quelles zones sur / sous-performent ?

5. **Implications pour l'investisseur** ‚Äì Synth√®se claire avec recommandations (actions value ? mati√®res premi√®res ? obligations longues ?)
   - INCLURE UNE POSITION SUR LA LIQUIDIT√â/CASH, par exemple "Maintenir 15% de liquidit√©s pour saisir les opportunit√©s en cas de correction"

6. üß† **Anticipations vs R√©alit√©** ‚Äì Mets en √©vidence 2 ou 3 endroits o√π la perception du march√© semble erron√©e, et ce que cela implique.

7. üî∫ **Risques cl√©s** ‚Äì Quels sont les 3 √† 5 principaux risques √† surveiller ?

8. üìä **Facteurs d√©terminants du march√©** ‚Äì Quelles seront les m√©triques ou annonces √† suivre dans les semaines √† venir ?

---

‚ö†Ô∏è **Niveau d'exigence** :

- Sois **strat√©gique et synth√©tique** (max ~800 tokens)
- Utilise des **cha√Ænes de raisonnement** (pas seulement des constats)
- Distingue **court terme (1-3 mois)** vs **moyen terme (6-12 mois)**
- Int√®gre la **composante comportementale** : que price d√©j√† le march√© ? quelles attentes sont risqu√©es ?
- IMPORTANT: En conclusion, inclure **3 convictions majeures avec une nuance temporelle pr√©cise**:
  - Utiliser des mois pr√©cis plut√¥t que "3 prochains mois" (ex: "Entre mai et juillet 2025")
  - Ajouter une raison d'action imm√©diate et lier √† des √©v√©nements sp√©cifiques
  - Exemple am√©lior√©: "Entre avril et juin, les obligations longues offrent un couple rendement/risque attractif en anticipation d'une d√©tente mon√©taire d√©but √©t√©."

---

üìÇ **Th√®mes dominants (30 derniers jours)** :
{themes_section}

üìÇ **Actualit√©s importantes (Top {len(synthesized_news)} globales, format synth√©tis√©)** :
{news_block}

üìà **Donn√©es march√© actuelles** (indices, taux, spreads, etc.) :
{markets_block}

üè≠ **Performances sectorielles r√©centes** :
{sectors_block}

---

üß† Fournis maintenant le **brief strat√©gique complet**, directement exploitable par une √©quipe d'asset allocation.
"""

        logger.info("üß† G√©n√©ration du brief strat√©gique via OpenAI...")

        # Appel √† l'API OpenAI
        brief = call_openai_api(prompt, temperature=0.2)

        # Validation basique de la longueur du brief
        brief_len = len(brief.strip()) if brief else 0
        if brief_len < 500:
            raise ValueError(
                f"Brief g√©n√©r√© anormalement court ({brief_len} caract√®res). Probable erreur de g√©n√©ration."
            )

        # Validation de la structure (non bloquant mais loggue)
        validation = validate_brief_structure(brief)

        # Pr√©paration des donn√©es √† sauvegarder
        brief_data = {
            "brief": brief,
            "generated_at": datetime.datetime.now().isoformat(),
            "source": {
                "themes_count": len(themes_weekly),
                "news_count": len(synthesized_news),
                "original_news_count": len(all_news),
                "selected_news_count": len(top_news),
                "markets_data": bool(markets_data),
                "sectors_data": bool(sectors_data),
            },
            "validation": validation,
        }

        # Assurez-vous que le r√©pertoire data existe
        os.makedirs(DATA_PATH, exist_ok=True)

        # Sauvegarde dans brief_ia.json
        with open(BRIEF_PATH, "w", encoding="utf-8") as f:
            json.dump(brief_data, f, ensure_ascii=False, indent=2)

        # Sauvegarde en format Markdown pour lisibilit√© humaine
        with open(BRIEF_MD_PATH, "w", encoding="utf-8") as f:
            f.write("# Brief Strat√©gique TradePulse\n\n")
            f.write(f"*G√©n√©r√© le {datetime.datetime.now().strftime('%d/%m/%Y √† %H:%M')}*\n\n")
            f.write("> **Sources de donn√©es:**\n")
            f.write(f"> - **March√©s:** {'‚úÖ Charg√©s' if markets_data else '‚ùå Non disponibles'}\n")
            f.write(f"> - **Secteurs:** {'‚úÖ Charg√©s' if sectors_data else '‚ùå Non disponibles'}\n")
            f.write(f"> - **Actualit√©s:** {len(synthesized_news)} sources analys√©es (sur {len(all_news)} brutes)\n")
            f.write(f"> - **Th√®mes:** {len(themes_weekly)} th√®mes dominants identifi√©s\n")
            if validation["ok"]:
                f.write("> - **Validation:** ‚úÖ Structure compl√®te\n\n")
            else:
                missing = ", ".join(validation["missing"])
                f.write(f"> - **Validation:** ‚ö†Ô∏è Sections manquantes: {missing}\n\n")
            f.write(brief)
            f.write(
                "\n\n---\n\n*Cette note est g√©n√©r√©e automatiquement par TradePulse AI, "
                "sur la base des actualit√©s et th√®mes d√©tect√©s dans les 7 √† 30 derniers jours.*\n"
            )

        logger.info(f"‚úÖ Brief strat√©gique g√©n√©r√© et sauvegard√©: {BRIEF_PATH} et {BRIEF_MD_PATH}")

        # Version debug
        debug_dir = os.path.join(os.path.dirname(__file__), "debug")
        os.makedirs(debug_dir, exist_ok=True)
        debug_path = os.path.join(
            debug_dir, f"brief_ia_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        )

        with open(debug_path, "w", encoding="utf-8") as f:
            f.write("# Brief Strat√©gique TradePulse - DEBUG\n\n")
            f.write(f"*G√©n√©r√© le {datetime.datetime.now().strftime('%d/%m/%Y √† %H:%M')}*\n\n")
            f.write("> **Sources de donn√©es:**\n")
            f.write(f"> - **March√©s:** {'‚úÖ Charg√©s' if markets_data else '‚ùå Non disponibles'}\n")
            f.write(f"> - **Secteurs:** {'‚úÖ Charg√©s' if sectors_data else '‚ùå Non disponibles'}\n")
            f.write(f"> - **Actualit√©s:** {len(synthesized_news)} sources analys√©es (sur {len(all_news)} brutes)\n")
            f.write(f"> - **Th√®mes:** {len(themes_weekly)} th√®mes dominants identifi√©s\n")
            if validation["ok"]:
                f.write("> - **Validation:** ‚úÖ Structure compl√®te\n\n")
            else:
                missing = ", ".join(validation["missing"])
                f.write(f"> - **Validation:** ‚ö†Ô∏è Sections manquantes: {missing}\n\n")
            f.write("## Prompt envoy√©\n```\n")
            f.write(prompt)
            f.write("\n```\n\n## R√©sultat\n\n")
            f.write(brief)
            f.write(
                "\n\n---\n\n*Cette note est g√©n√©r√©e automatiquement par TradePulse AI, "
                "sur la base des actualit√©s et th√®mes d√©tect√©s dans les 7 √† 30 derniers jours.*\n"
            )

        logger.info(f"üîç Version debug sauvegard√©e: {debug_path}")

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
